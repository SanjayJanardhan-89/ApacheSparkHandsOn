{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SanjayJanardhan-89/ApacheSparkHandsOn/blob/main/Certification/Chapter_5_Advanced_Operations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WuOZ0C0lhCL4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Pyspark\n"
      ],
      "metadata": {
        "id": "vRYFGd5ehHN5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 662
        },
        "id": "QLc7s-Qo1irp",
        "outputId": "02fe8e9e-f352-474d-acc1-9572594b627a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.91.83)] [1 InRelease 20.0 kB/129 k\u001b[0m\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.91.83)] [Connected to cloud.r-proj\u001b[0m\r                                                                               \rHit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.91.83)] [Waiting for headers] [Con\u001b[0m\r                                                                               \rHit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (91.189.91.83)] [Connecting to r2u.stat.il\u001b[0m\r                                                                               \rHit:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:5 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:6 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,848 kB]\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,245 kB]\n",
            "Hit:12 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Fetched 4,222 kB in 2s (2,796 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "43 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (0.10.9.7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fd119cb1810>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://696cfacb5b43:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.5</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>OurSparkApp</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"OurSparkApp\") \\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "salary_data = [(\"John\", \"Field-eng\", 3500),\n",
        "    (\"Michael\", \"Field-eng\", 4500),\n",
        "    (\"Robert\", None, 4000),\n",
        "    (\"Maria\", \"Finance\", 3500),\n",
        "    (\"John\", \"Sales\", 3000),\n",
        "    (\"Kelly\", \"Finance\", 3500),\n",
        "    (\"Kate\", \"Finance\", 3000),\n",
        "    (\"Martin\", None, 3500),\n",
        "    (\"Kiran\", \"Sales\", 2200),\n",
        "    (\"Michael\", \"Field-eng\", 4500)\n",
        "  ]\n",
        "columns= [\"Employee\", \"Department\", \"Salary\"]\n",
        "salary_data = spark.createDataFrame(data = salary_data, schema = columns)\n",
        "salary_data.printSchema()\n",
        "salary_data.show()"
      ],
      "metadata": {
        "id": "0oyJX03Hlu8f",
        "outputId": "9415b8e8-91b2-4f4e-c184-3c8053e28893",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- Employee: string (nullable = true)\n",
            " |-- Department: string (nullable = true)\n",
            " |-- Salary: long (nullable = true)\n",
            "\n",
            "+--------+----------+------+\n",
            "|Employee|Department|Salary|\n",
            "+--------+----------+------+\n",
            "|    John| Field-eng|  3500|\n",
            "| Michael| Field-eng|  4500|\n",
            "|  Robert|      NULL|  4000|\n",
            "|   Maria|   Finance|  3500|\n",
            "|    John|     Sales|  3000|\n",
            "|   Kelly|   Finance|  3500|\n",
            "|    Kate|   Finance|  3000|\n",
            "|  Martin|      NULL|  3500|\n",
            "|   Kiran|     Sales|  2200|\n",
            "| Michael| Field-eng|  4500|\n",
            "+--------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "salary_data.groupBy('Department')"
      ],
      "metadata": {
        "id": "0kBgRSvYlvTM",
        "outputId": "2d98fedd-0b8c-4634-8831-3f126141ac5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GroupedData[grouping expressions: [Department], value: [Employee: string, Department: string ... 1 more field], type: GroupBy]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "salary_data.groupBy().avg().show()"
      ],
      "metadata": {
        "id": "Xvrrxrbrl7BQ",
        "outputId": "e39c8c33-68c1-481f-8057-8d728badaf56",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "|avg(Salary)|\n",
            "+-----------+\n",
            "|     3520.0|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "salary_data.groupBy('Department').avg().show()"
      ],
      "metadata": {
        "id": "xUGgC1iul9yr",
        "outputId": "1da2bd2e-d450-424a-e644-27237d715b74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------------+\n",
            "|Department|       avg(Salary)|\n",
            "+----------+------------------+\n",
            "|     Sales|            2600.0|\n",
            "| Field-eng| 4166.666666666667|\n",
            "|      NULL|            3750.0|\n",
            "|   Finance|3333.3333333333335|\n",
            "+----------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import lit\n",
        "salary_data = salary_data.withColumn('AnotherCol', lit(100))"
      ],
      "metadata": {
        "id": "_ah36xOgmFhD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "salary_data.groupBy('Department').avg().show()"
      ],
      "metadata": {
        "id": "YeruKg7vmKkK",
        "outputId": "ed262738-9ccf-4195-c0f8-91f571777fd2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------------+---------------+\n",
            "|Department|       avg(Salary)|avg(AnotherCol)|\n",
            "+----------+------------------+---------------+\n",
            "|     Sales|            2600.0|          100.0|\n",
            "| Field-eng| 4166.666666666667|          100.0|\n",
            "|      NULL|            3750.0|          100.0|\n",
            "|   Finance|3333.3333333333335|          100.0|\n",
            "+----------+------------------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "salary_data = salary_data.drop('AnotherCol')"
      ],
      "metadata": {
        "id": "Wdin2EabmOl1"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "salary_data.show()"
      ],
      "metadata": {
        "id": "sGqH-FWsmYjG",
        "outputId": "8881130b-6467-4658-ea28-d72ca7d9337b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+------+\n",
            "|Employee|Department|Salary|\n",
            "+--------+----------+------+\n",
            "|    John| Field-eng|  3500|\n",
            "| Michael| Field-eng|  4500|\n",
            "|  Robert|      NULL|  4000|\n",
            "|   Maria|   Finance|  3500|\n",
            "|    John|     Sales|  3000|\n",
            "|   Kelly|   Finance|  3500|\n",
            "|    Kate|   Finance|  3000|\n",
            "|  Martin|      NULL|  3500|\n",
            "|   Kiran|     Sales|  2200|\n",
            "| Michael| Field-eng|  4500|\n",
            "+--------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "\n",
        "(salary_data\n",
        "  .groupBy(\"Department\").sum(\"salary\")\n",
        "  .withColumn(\"sum(salary)\", round(col(\"sum(salary)\"),4))\n",
        "  .withColumnRenamed(\"sum(salary)\", \"sum_salary\")\n",
        "  .orderBy(desc(\"sum_salary\"))\n",
        "  .show()\n",
        ")"
      ],
      "metadata": {
        "id": "KCZYGdElmaGK",
        "outputId": "b586622a-18f6-420f-ec65-63544bbbbe9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+\n",
            "|Department|sum_salary|\n",
            "+----------+----------+\n",
            "| Field-eng|     12500|\n",
            "|   Finance|     10000|\n",
            "|      NULL|      7500|\n",
            "|     Sales|      5200|\n",
            "+----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "salary_data_with_id = [(1, \"John\", \"Field-eng\", 3500), \\\n",
        "    (2, \"Robert\", \"Sales\", 4000), \\\n",
        "    (3, \"Maria\", \"Finance\", 3500), \\\n",
        "    (4, \"Michael\", \"Sales\", 3000), \\\n",
        "    (5, \"Kelly\", \"Finance\", 3500), \\\n",
        "    (6, \"Kate\", \"Finance\", 3000), \\\n",
        "    (7, \"Martin\", \"Finance\", 3500), \\\n",
        "    (8, \"Kiran\", \"Sales\", 2200), \\\n",
        "  ]\n",
        "columns= [\"ID\", \"Employee\", \"Department\", \"Salary\"]\n",
        "salary_data_with_id = spark.createDataFrame(data = salary_data_with_id, schema = columns)\n",
        "salary_data_with_id.show()\n",
        "\n",
        "employee_data = [(1, \"NY\", \"M\"), \\\n",
        "    (2, \"NC\", \"M\"), \\\n",
        "    (3, \"NY\", \"F\"), \\\n",
        "    (4, \"TX\", \"M\"), \\\n",
        "    (5, \"NY\", \"F\"), \\\n",
        "    (6, \"AZ\", \"F\") \\\n",
        "  ]\n",
        "columns= [\"ID\", \"State\", \"Gender\"]\n",
        "employee_data = spark.createDataFrame(data = employee_data, schema = columns)\n",
        "employee_data.show()"
      ],
      "metadata": {
        "id": "x4gburxBmsI7",
        "outputId": "c3521c48-1daa-4047-9a88-7987f670b779",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+----------+------+\n",
            "| ID|Employee|Department|Salary|\n",
            "+---+--------+----------+------+\n",
            "|  1|    John| Field-eng|  3500|\n",
            "|  2|  Robert|     Sales|  4000|\n",
            "|  3|   Maria|   Finance|  3500|\n",
            "|  4| Michael|     Sales|  3000|\n",
            "|  5|   Kelly|   Finance|  3500|\n",
            "|  6|    Kate|   Finance|  3000|\n",
            "|  7|  Martin|   Finance|  3500|\n",
            "|  8|   Kiran|     Sales|  2200|\n",
            "+---+--------+----------+------+\n",
            "\n",
            "+---+-----+------+\n",
            "| ID|State|Gender|\n",
            "+---+-----+------+\n",
            "|  1|   NY|     M|\n",
            "|  2|   NC|     M|\n",
            "|  3|   NY|     F|\n",
            "|  4|   TX|     M|\n",
            "|  5|   NY|     F|\n",
            "|  6|   AZ|     F|\n",
            "+---+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "salary_data_with_id.join(employee_data, salary_data_with_id.ID == employee_data.ID).show()"
      ],
      "metadata": {
        "id": "czdvDiFMnU4G",
        "outputId": "e942318c-ae0a-4aec-c96c-9515a2c69b43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+----------+------+---+-----+------+\n",
            "| ID|Employee|Department|Salary| ID|State|Gender|\n",
            "+---+--------+----------+------+---+-----+------+\n",
            "|  1|    John| Field-eng|  3500|  1|   NY|     M|\n",
            "|  2|  Robert|     Sales|  4000|  2|   NC|     M|\n",
            "|  3|   Maria|   Finance|  3500|  3|   NY|     F|\n",
            "|  4| Michael|     Sales|  3000|  4|   TX|     M|\n",
            "|  5|   Kelly|   Finance|  3500|  5|   NY|     F|\n",
            "|  6|    Kate|   Finance|  3000|  6|   AZ|     F|\n",
            "+---+--------+----------+------+---+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "salary_data_with_id.join(employee_data, salary_data_with_id.ID == employee_data.ID, \"full\").show()"
      ],
      "metadata": {
        "id": "0hkvWiA_nijK",
        "outputId": "297b01e2-4240-41c7-beb8-625138e67003",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+----------+------+----+-----+------+\n",
            "| ID|Employee|Department|Salary|  ID|State|Gender|\n",
            "+---+--------+----------+------+----+-----+------+\n",
            "|  1|    John| Field-eng|  3500|   1|   NY|     M|\n",
            "|  2|  Robert|     Sales|  4000|   2|   NC|     M|\n",
            "|  3|   Maria|   Finance|  3500|   3|   NY|     F|\n",
            "|  4| Michael|     Sales|  3000|   4|   TX|     M|\n",
            "|  5|   Kelly|   Finance|  3500|   5|   NY|     F|\n",
            "|  6|    Kate|   Finance|  3000|   6|   AZ|     F|\n",
            "|  7|  Martin|   Finance|  3500|NULL| NULL|  NULL|\n",
            "|  8|   Kiran|     Sales|  2200|NULL| NULL|  NULL|\n",
            "+---+--------+----------+------+----+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "salary_data_with_id.crossJoin(employee_data).show()"
      ],
      "metadata": {
        "id": "MRsEZBVunmUk",
        "outputId": "e3d33cee-ea47-4a4c-d6dc-01e8212b443d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+----------+------+---+-----+------+\n",
            "| ID|Employee|Department|Salary| ID|State|Gender|\n",
            "+---+--------+----------+------+---+-----+------+\n",
            "|  1|    John| Field-eng|  3500|  1|   NY|     M|\n",
            "|  1|    John| Field-eng|  3500|  2|   NC|     M|\n",
            "|  1|    John| Field-eng|  3500|  3|   NY|     F|\n",
            "|  2|  Robert|     Sales|  4000|  1|   NY|     M|\n",
            "|  2|  Robert|     Sales|  4000|  2|   NC|     M|\n",
            "|  2|  Robert|     Sales|  4000|  3|   NY|     F|\n",
            "|  3|   Maria|   Finance|  3500|  1|   NY|     M|\n",
            "|  3|   Maria|   Finance|  3500|  2|   NC|     M|\n",
            "|  3|   Maria|   Finance|  3500|  3|   NY|     F|\n",
            "|  4| Michael|     Sales|  3000|  1|   NY|     M|\n",
            "|  4| Michael|     Sales|  3000|  2|   NC|     M|\n",
            "|  4| Michael|     Sales|  3000|  3|   NY|     F|\n",
            "|  1|    John| Field-eng|  3500|  4|   TX|     M|\n",
            "|  1|    John| Field-eng|  3500|  5|   NY|     F|\n",
            "|  1|    John| Field-eng|  3500|  6|   AZ|     F|\n",
            "|  2|  Robert|     Sales|  4000|  4|   TX|     M|\n",
            "|  2|  Robert|     Sales|  4000|  5|   NY|     F|\n",
            "|  2|  Robert|     Sales|  4000|  6|   AZ|     F|\n",
            "|  3|   Maria|   Finance|  3500|  4|   TX|     M|\n",
            "|  3|   Maria|   Finance|  3500|  5|   NY|     F|\n",
            "+---+--------+----------+------+---+-----+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "salary_data_with_id_2 = \\\n",
        " [\n",
        "              (1, \"John\", \"Field-eng\", 3500), \\\n",
        "              (2, \"Robert\", \"Sales\", 4000), \\\n",
        "              (3, \"Aliya\", \"Finance\", 3500), \\\n",
        "              (4, \"Nate\", \"Sales\", 3000), \\\n",
        " ]\n",
        "columns2= [\"ID\", \"Employee\", \"Department\", \"Salary\"]\n",
        "salary_data_with_id_2 = spark.createDataFrame(data = salary_data_with_id_2, schema = columns2)\n",
        "salary_data_with_id_2.printSchema()\n",
        "salary_data_with_id_2.show(truncate=False)"
      ],
      "metadata": {
        "id": "T2fAXW0boBow",
        "outputId": "6d58508f-5a67-4673-fc95-af8fdf4d88b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- ID: long (nullable = true)\n",
            " |-- Employee: string (nullable = true)\n",
            " |-- Department: string (nullable = true)\n",
            " |-- Salary: long (nullable = true)\n",
            "\n",
            "+---+--------+----------+------+\n",
            "|ID |Employee|Department|Salary|\n",
            "+---+--------+----------+------+\n",
            "|1  |John    |Field-eng |3500  |\n",
            "|2  |Robert  |Sales     |4000  |\n",
            "|3  |Aliya   |Finance   |3500  |\n",
            "|4  |Nate    |Sales     |3000  |\n",
            "+---+--------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "salary_data_with_id.union(salary_data_with_id_2).show()"
      ],
      "metadata": {
        "id": "7-pTsyLlo3gl",
        "outputId": "59e9fedc-e996-4a67-e0a9-112cbde5f678",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+----------+------+\n",
            "| ID|Employee|Department|Salary|\n",
            "+---+--------+----------+------+\n",
            "|  1|    John| Field-eng|  3500|\n",
            "|  2|  Robert|     Sales|  4000|\n",
            "|  3|   Maria|   Finance|  3500|\n",
            "|  4| Michael|     Sales|  3000|\n",
            "|  5|   Kelly|   Finance|  3500|\n",
            "|  6|    Kate|   Finance|  3000|\n",
            "|  7|  Martin|   Finance|  3500|\n",
            "|  8|   Kiran|     Sales|  2200|\n",
            "|  1|    John| Field-eng|  3500|\n",
            "|  2|  Robert|     Sales|  4000|\n",
            "|  3|   Aliya|   Finance|  3500|\n",
            "|  4|    Nate|     Sales|  3000|\n",
            "+---+--------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "salary_data_with_id.intersect(salary_data_with_id_2).show()"
      ],
      "metadata": {
        "id": "mHDRvrF9py3I",
        "outputId": "f1bf1de3-6500-4fd1-ec37-35c241fb5a97",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+----------+------+\n",
            "| ID|Employee|Department|Salary|\n",
            "+---+--------+----------+------+\n",
            "|  2|  Robert|     Sales|  4000|\n",
            "|  1|    John| Field-eng|  3500|\n",
            "+---+--------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "salary_data_with_id.union(salary_data_with_id_2).distinct().show()"
      ],
      "metadata": {
        "id": "t2rBAiyVp2Bz",
        "outputId": "6c0552bb-b18a-4c7c-a026-e741139df3fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+----------+------+\n",
            "| ID|Employee|Department|Salary|\n",
            "+---+--------+----------+------+\n",
            "|  4| Michael|     Sales|  3000|\n",
            "|  3|   Maria|   Finance|  3500|\n",
            "|  2|  Robert|     Sales|  4000|\n",
            "|  1|    John| Field-eng|  3500|\n",
            "|  8|   Kiran|     Sales|  2200|\n",
            "|  6|    Kate|   Finance|  3000|\n",
            "|  7|  Martin|   Finance|  3500|\n",
            "|  5|   Kelly|   Finance|  3500|\n",
            "|  3|   Aliya|   Finance|  3500|\n",
            "|  4|    Nate|     Sales|  3000|\n",
            "+---+--------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(employee_data\n",
        " .write\n",
        " .format('csv')\n",
        " .mode('overwrite')\n",
        " .option(\"header\", True)\n",
        " .save('salary_data.csv')\n",
        " )\n"
      ],
      "metadata": {
        "id": "-oNoepg_qBEt"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.read.format('csv').load('salary_data.csv').show()"
      ],
      "metadata": {
        "id": "JY9lZ-kjyoXT",
        "outputId": "5634f7e9-27e8-4a9c-88b4-8fbf906fd87a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+------+\n",
            "|_c0|  _c1|   _c2|\n",
            "+---+-----+------+\n",
            "| ID|State|Gender|\n",
            "|  1|   NY|     M|\n",
            "|  2|   NC|     M|\n",
            "|  3|   NY|     F|\n",
            "| ID|State|Gender|\n",
            "|  4|   TX|     M|\n",
            "|  5|   NY|     F|\n",
            "|  6|   AZ|     F|\n",
            "+---+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(spark.read\n",
        " .format('csv')\n",
        " .option(\"header\", True)\n",
        " .load('salary_data.csv').show()\n",
        ")"
      ],
      "metadata": {
        "id": "zGF47sElsqnr",
        "outputId": "401bdbc2-f0f1-41dc-ee97-0884327f5592",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+------+\n",
            "| ID|State|Gender|\n",
            "+---+-----+------+\n",
            "|  1|   NY|     M|\n",
            "|  2|   NC|     M|\n",
            "|  3|   NY|     F|\n",
            "|  4|   TX|     M|\n",
            "|  5|   NY|     F|\n",
            "|  6|   AZ|     F|\n",
            "+---+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "filePath = '\\salary_data.csv'\n",
        "columns= [\"ID\", \"State\", \"Gender\"]\n",
        "schema = StructType([\n",
        "      StructField(\"ID\", IntegerType(),True),\n",
        "  StructField(\"State\",  StringType(),True),\n",
        "  StructField(\"Gender\",  StringType(),True)\n",
        "])\n",
        "\n",
        "\n",
        "read_data = spark.read.format(\"csv\").option(\"header\",\"true\").schema(schema).load(filePath)\n",
        "read_data.show()\n"
      ],
      "metadata": {
        "id": "CCJLEnqetcK0",
        "outputId": "87716c03-6dbe-45d9-8ec6-1e033accc626",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----+------+\n",
            "| ID|State|Gender|\n",
            "+---+-----+------+\n",
            "|  1|   NY|     M|\n",
            "|  2|   NC|     M|\n",
            "|  3|   NY|     F|\n",
            "|  4|   TX|     M|\n",
            "|  5|   NY|     F|\n",
            "|  6|   AZ|     F|\n",
            "+---+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "salary_data_with_id.write.mode(\"append\").format(\"parquet\").save(\"salary_data.parquet\")"
      ],
      "metadata": {
        "id": "QFu0gtmUyQy1"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BWLbQPgwzRnh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark.read.format(\"parquet\").load(\"salary_data.parquet\").show()"
      ],
      "metadata": {
        "id": "kkkZ5VuUzGpi",
        "outputId": "8618bbd1-d546-432d-eea7-73901400a3ac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+----------+------+\n",
            "| ID|Employee|Department|Salary|\n",
            "+---+--------+----------+------+\n",
            "|  1|    John| Field-eng|  3500|\n",
            "|  2|  Robert|     Sales|  4000|\n",
            "|  3|   Maria|   Finance|  3500|\n",
            "|  4| Michael|     Sales|  3000|\n",
            "|  1|    John| Field-eng|  3500|\n",
            "|  2|  Robert|     Sales|  4000|\n",
            "|  3|   Maria|   Finance|  3500|\n",
            "|  4| Michael|     Sales|  3000|\n",
            "|  1|    John| Field-eng|  3500|\n",
            "|  2|  Robert|     Sales|  4000|\n",
            "|  3|   Maria|   Finance|  3500|\n",
            "|  4| Michael|     Sales|  3000|\n",
            "|  5|   Kelly|   Finance|  3500|\n",
            "|  6|    Kate|   Finance|  3000|\n",
            "|  7|  Martin|   Finance|  3500|\n",
            "|  8|   Kiran|     Sales|  2200|\n",
            "|  5|   Kelly|   Finance|  3500|\n",
            "|  6|    Kate|   Finance|  3000|\n",
            "|  7|  Martin|   Finance|  3500|\n",
            "|  8|   Kiran|     Sales|  2200|\n",
            "+---+--------+----------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "f3XlpxFVzaFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "salary_data_with_id.write.mode(\"append\").format(\"orc\").save(\"salary_data.orc\")\n",
        "spark.read.format(\"orc\").load(\"salary_data.orc\").show()"
      ],
      "metadata": {
        "id": "HMLBn9TVzNaH",
        "outputId": "6a5f62a0-a02f-4501-a5ca-96731b6b7c0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------+----------+------+\n",
            "| ID|Employee|Department|Salary|\n",
            "+---+--------+----------+------+\n",
            "|  1|    John| Field-eng|  3500|\n",
            "|  2|  Robert|     Sales|  4000|\n",
            "|  3|   Maria|   Finance|  3500|\n",
            "|  4| Michael|     Sales|  3000|\n",
            "|  1|    John| Field-eng|  3500|\n",
            "|  2|  Robert|     Sales|  4000|\n",
            "|  3|   Maria|   Finance|  3500|\n",
            "|  4| Michael|     Sales|  3000|\n",
            "|  5|   Kelly|   Finance|  3500|\n",
            "|  6|    Kate|   Finance|  3000|\n",
            "|  7|  Martin|   Finance|  3500|\n",
            "|  8|   Kiran|     Sales|  2200|\n",
            "|  5|   Kelly|   Finance|  3500|\n",
            "|  6|    Kate|   Finance|  3000|\n",
            "|  7|  Martin|   Finance|  3500|\n",
            "|  8|   Kiran|     Sales|  2200|\n",
            "+---+--------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Delta File"
      ],
      "metadata": {
        "id": "QyxNA6Vb2hXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "salary_data_with_id.write.mode(\"append\").format(\"delta\").save(\"salary_data.delta\")\n"
      ],
      "metadata": {
        "id": "QMEbQ0k45qsR",
        "outputId": "d62bc742-ff20-4cfa-c739-1885d71053ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Py4JJavaError",
          "evalue": "An error occurred while calling o220.save.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:260)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: delta.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 16 more\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-b80d4cb2dc74>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msalary_data_with_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"append\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"delta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"salary_data.delta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1461\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1463\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1465\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minsertInto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtableName\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
            "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o220.save.\n: org.apache.spark.SparkClassNotFoundException: [DATA_SOURCE_NOT_FOUND] Failed to find the data source: delta. Please find packages at `https://spark.apache.org/third-party-projects.html`.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.dataSourceNotFoundError(QueryExecutionErrors.scala:725)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:647)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:697)\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:873)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:260)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: delta.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:476)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:594)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:527)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:633)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:633)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:633)\n\t... 16 more\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0mC4IkAS59s5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}