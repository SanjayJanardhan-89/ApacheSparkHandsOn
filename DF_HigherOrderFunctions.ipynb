{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SanjayJanardhan-89/ApacheSparkHandsOn/blob/main/DF_HigherOrderFunctions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "8bkC8UaXyb9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WuOZ0C0lhCL4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Pyspark\n"
      ],
      "metadata": {
        "id": "vRYFGd5ehHN5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 760
        },
        "id": "QLc7s-Qo1irp",
        "outputId": "c2f4bce9-fcf8-4a14-e8db-93c4509b4ceb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:4 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,679 kB]\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,767 kB]\n",
            "Get:6 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,692 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,237 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,535 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,003 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [3,962 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [82.7 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [35.2 kB]\n",
            "Hit:17 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:18 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:19 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 24.4 MB in 9s (2,825 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "30 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (0.10.9.7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7b7b34a4d4d0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://5f3722e84646:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.5</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>OurSparkApp</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"OurSparkApp\") \\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In Python\n",
        "# Set file paths\n",
        "from pyspark.sql.functions import expr\n",
        "tripdelaysFilePath = \"departuredelays.csv\"\n",
        "airportsFilePath = \"airport-codes-na.txt\"\n",
        ""
      ],
      "metadata": {
        "id": "TD7KFNq_zmlf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Obtain airports data set\n",
        "airports = (spark.read\n",
        "  .format(\"csv\")\n",
        "  .options(header=\"true\", inferSchema=\"true\", sep=\"\\t\")\n",
        "  .load(airportsFilePath))\n",
        "\n",
        "airports.createOrReplaceTempView(\"airports\")\n",
        "\n",
        "# Obtain departure delays data set\n",
        "departureDelays = (spark.read\n",
        "  .format(\"csv\")\n",
        "  .options(header=\"true\")\n",
        "  .load(tripdelaysFilePath))\n",
        "\n",
        "departureDelays = (departureDelays\n",
        "  .withColumn(\"delay\", expr(\"CAST(delay as INT) as delay\"))\n",
        "  .withColumn(\"distance\", expr(\"CAST(distance as INT) as distance\")))\n",
        "\n",
        "departureDelays.createOrReplaceTempView(\"departureDelays\")\n"
      ],
      "metadata": {
        "id": "zrm-OcxuztXI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create temporary small table\n",
        "foo = (departureDelays\n",
        "        .filter(expr(\"\"\"origin == 'SEA' and destination == 'SFO' and date like '01010%' and delay > 0\"\"\"))\n",
        "        )\n",
        "foo.createOrReplaceTempView(\"foo\")"
      ],
      "metadata": {
        "id": "ZTQeB023zxp_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spark.sql(\"SELECT * FROM airports LIMIT 10\").show()\n",
        "spark.sql(\"SELECT * FROM departureDelays LIMIT 10\").show()\n",
        "spark.sql(\"SELECT * FROM foo\").show()"
      ],
      "metadata": {
        "id": "WyYQ1Kbiz4FO",
        "outputId": "fb12ffdb-c345-48a0-df1c-f324bd47a7b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-----+-------+----+\n",
            "|       City|State|Country|IATA|\n",
            "+-----------+-----+-------+----+\n",
            "| Abbotsford|   BC| Canada| YXX|\n",
            "|   Aberdeen|   SD|    USA| ABR|\n",
            "|    Abilene|   TX|    USA| ABI|\n",
            "|      Akron|   OH|    USA| CAK|\n",
            "|    Alamosa|   CO|    USA| ALS|\n",
            "|     Albany|   GA|    USA| ABY|\n",
            "|     Albany|   NY|    USA| ALB|\n",
            "|Albuquerque|   NM|    USA| ABQ|\n",
            "| Alexandria|   LA|    USA| AEX|\n",
            "|  Allentown|   PA|    USA| ABE|\n",
            "+-----------+-----+-------+----+\n",
            "\n",
            "+--------+-----+--------+------+-----------+\n",
            "|    date|delay|distance|origin|destination|\n",
            "+--------+-----+--------+------+-----------+\n",
            "|01011245|    6|     602|   ABE|        ATL|\n",
            "|01020600|   -8|     369|   ABE|        DTW|\n",
            "|01021245|   -2|     602|   ABE|        ATL|\n",
            "|01020605|   -4|     602|   ABE|        ATL|\n",
            "|01031245|   -4|     602|   ABE|        ATL|\n",
            "|01030605|    0|     602|   ABE|        ATL|\n",
            "|01041243|   10|     602|   ABE|        ATL|\n",
            "|01040605|   28|     602|   ABE|        ATL|\n",
            "|01051245|   88|     602|   ABE|        ATL|\n",
            "|01050605|    9|     602|   ABE|        ATL|\n",
            "+--------+-----+--------+------+-----------+\n",
            "\n",
            "+--------+-----+--------+------+-----------+\n",
            "|    date|delay|distance|origin|destination|\n",
            "+--------+-----+--------+------+-----------+\n",
            "|01010710|   31|     590|   SEA|        SFO|\n",
            "|01010955|  104|     590|   SEA|        SFO|\n",
            "|01010730|    5|     590|   SEA|        SFO|\n",
            "+--------+-----+--------+------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HL6qM4PKz8k2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}