{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPa/kqAkGDkcFpy9tJIuz8Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SanjayJanardhan-89/ApacheSparkHandsOn/blob/main/ComplexDatatypes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WuOZ0C0lhCL4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Pyspark\n"
      ],
      "metadata": {
        "id": "vRYFGd5ehHN5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 812
        },
        "id": "QLc7s-Qo1irp",
        "outputId": "37e473a2-f1a3-4499-8b95-0a39ebc6ce42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "\u001b[33m\r0% [Waiting for headers] [1 InRelease 14.2 kB/129 kB 11%] [Connected to cloud.r\u001b[0m\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\u001b[33m\r0% [Waiting for headers] [1 InRelease 54.7 kB/129 kB 42%] [Connected to cloud.r\u001b[0m\r                                                                               \rGet:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:8 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,262 kB]\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,804 kB]\n",
            "Get:10 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4,763 kB]\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,092 kB]\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [48.5 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,404 kB]\n",
            "Hit:14 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [56.4 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,566 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,917 kB]\n",
            "Get:18 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Get:19 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,081 kB]\n",
            "Hit:20 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:21 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,752 kB]\n",
            "Get:22 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [51.0 kB]\n",
            "Fetched 33.2 MB in 3s (10.6 MB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "36 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (0.10.9.7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x78a7699b5cd0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://1744e7c0620b:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>OurSparkApp</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"OurSparkApp\") \\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Play with Arrays\n"
      ],
      "metadata": {
        "id": "cAbqNvQRhMI2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Length of the array"
      ],
      "metadata": {
        "id": "Yr8cte2KYfUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import size\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "data = [\n",
        "    (1, [1, 2, 3]),\n",
        "    (2, [4, 5]),\n",
        "    (3, [])\n",
        "]\n",
        "df = spark.createDataFrame(data, [\"id\", \"numbers\"])\n",
        "\n",
        "df.show(truncate=False)\n",
        "\n",
        "from pyspark.sql.functions import size, col\n",
        "\n",
        "df_with_size = df.withColumn(\"numbers_size\", size(\"numbers\"))\n",
        "df_with_size = df_with_size.withColumn(\"numbers_size_2\", size(col(\"numbers\")))\n",
        "\n",
        "df_with_size.show()"
      ],
      "metadata": {
        "id": "VrOTfPUeYe29",
        "outputId": "c2fe420e-8fad-4214-8a10-653d8ee678a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------+\n",
            "|id |numbers  |\n",
            "+---+---------+\n",
            "|1  |[1, 2, 3]|\n",
            "|2  |[4, 5]   |\n",
            "|3  |[]       |\n",
            "+---+---------+\n",
            "\n",
            "+---+---------+------------+--------------+\n",
            "| id|  numbers|numbers_size|numbers_size_2|\n",
            "+---+---------+------------+--------------+\n",
            "|  1|[1, 2, 3]|           3|             3|\n",
            "|  2|   [4, 5]|           2|             2|\n",
            "|  3|       []|           0|             0|\n",
            "+---+---------+------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fist item from array(element_at)"
      ],
      "metadata": {
        "id": "fAp8LKThU9q5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import element_at\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Example DataFrame with an array column\n",
        "\n",
        "data = [\n",
        "(1, [10, 20, 30]),\n",
        "(2, [40, 50, 60]),\n",
        "(3, [70, 80, 90])\n",
        "]\n",
        "\n",
        "schema = [\"id\", \"numbers\"]\n",
        "\n",
        "df = spark.createDataFrame(data, schema=schema)\n",
        "df.show()\n",
        "\n",
        "# Select the first element from the array column 'numbers'\n",
        "df.select(element_at(col(\"numbers\"), 1).alias(\"first_element\")).show()"
      ],
      "metadata": {
        "id": "fJf-7FI1Ykr5",
        "outputId": "69ff1380-de30-4da8-a94b-c61cebddd654",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------+\n",
            "| id|     numbers|\n",
            "+---+------------+\n",
            "|  1|[10, 20, 30]|\n",
            "|  2|[40, 50, 60]|\n",
            "|  3|[70, 80, 90]|\n",
            "+---+------------+\n",
            "\n",
            "+-------------+\n",
            "|first_element|\n",
            "+-------------+\n",
            "|           10|\n",
            "|           40|\n",
            "|           70|\n",
            "+-------------+\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'get_item' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-24-3965187877.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement_at\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"numbers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"first_element\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_item\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"numbers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"first_element\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'get_item' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "-U4rtCf3XnIa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create array using Spark SQL"
      ],
      "metadata": {
        "id": "YpGEFG0bYy4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sql_array = spark.sql(\"SELECT array('KGF 1', 'KGF 2', 'Autograph', 'Kicha','Hucha') as movies\")\n",
        "df_sql_array.printSchema()\n",
        "df_sql_array.show(truncate=False)"
      ],
      "metadata": {
        "id": "NDhHyKMo5Q4K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43e9624a-362a-4269-aa40-353740809032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- movies: array (nullable = false)\n",
            " |    |-- element: string (containsNull = false)\n",
            "\n",
            "+---------------------------------------+\n",
            "|movies                                 |\n",
            "+---------------------------------------+\n",
            "|[KGF 1, KGF 2, Autograph, Kicha, Hucha]|\n",
            "+---------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType,StructField, StringType, ArrayType,MapType\n",
        "\n",
        "data = [\n",
        "          [\n",
        "            [\"KGF 1\", \"KGF 2\", \"Autograph\", \"Kicha\",\"Hucha\"],\n",
        "            # [\"Hello\", \"Hi\"]\n",
        "          ]\n",
        "        ]\n",
        "\n",
        "# Schema\n",
        "schema = StructType([\n",
        "             StructField('movies', ArrayType(StringType()), True),\n",
        "     ])\n",
        "\n",
        "# Create DataFrame\n",
        "df_array = spark.createDataFrame(data = data, schema = schema)\n",
        "df_array.printSchema()\n",
        "df_array.show(truncate=False) # shows all columns"
      ],
      "metadata": {
        "id": "KRnw_ETEci45",
        "outputId": "65004037-5205-43a5-ffe6-9b9df4b5a9b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- movies: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n",
            "+---------------------------------------+\n",
            "|movies                                 |\n",
            "+---------------------------------------+\n",
            "|[KGF 1, KGF 2, Autograph, Kicha, Hucha]|\n",
            "+---------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge 2 arrays\n"
      ],
      "metadata": {
        "id": "eKUjWRTJpdqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import concat, array\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "data = [\n",
        "    (1, [1, 2], [1,3, 4]),\n",
        "    (2, [5, 6], [7, 8]),\n",
        "]\n",
        "df = spark.createDataFrame(data, [\"id\", \"arr1\", \"arr2\"])\n",
        "\n",
        "# concat arrays\n",
        "df2 = df.withColumn(\"merged\", concat(\"arr1\", \"arr2\"))\n",
        "\n",
        "df2.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmzRZag1q150",
        "outputId": "4e2a647f-a554-4c8c-9b74-a0405a747b31"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---------+---------------+\n",
            "|id |arr1  |arr2     |merged         |\n",
            "+---+------+---------+---------------+\n",
            "|1  |[1, 2]|[1, 3, 4]|[1, 2, 1, 3, 4]|\n",
            "|2  |[5, 6]|[7, 8]   |[5, 6, 7, 8]   |\n",
            "+---+------+---------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Concat cant not handle nested array\n",
        "\n",
        "from pyspark.sql.functions import flatten, concat\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    (1, [[1, 2], [3, 4]]),\n",
        "    (2, [[5, 6]])\n",
        "], [\"id\", \"nested_array\"])\n",
        "\n",
        "df.select(\"id\",\"nested_array\",concat(\"nested_array\")).show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sK1Ig2YFMYU",
        "outputId": "0d787ea1-4b21-4840-bab5-0ddab309d96b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------------+--------------------+\n",
            "|id |nested_array    |concat(nested_array)|\n",
            "+---+----------------+--------------------+\n",
            "|1  |[[1, 2], [3, 4]]|[[1, 2], [3, 4]]    |\n",
            "|2  |[[5, 6]]        |[[5, 6]]            |\n",
            "+---+----------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flatten Nested Array"
      ],
      "metadata": {
        "id": "jw-4Lda2EYxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import flatten\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    (1, [[1, 2], [3, 4]]),\n",
        "    (2, [[5, 6]])\n",
        "], [\"id\", \"nested_array\"])\n",
        "\n",
        "df.select(flatten(\"nested_array\")).show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRSAVBMzEbZv",
        "outputId": "edbe1f7d-27da-45f4-ad67-d75c40a17903"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------+\n",
            "|flatten(nested_array)|\n",
            "+---------------------+\n",
            "|[1, 2, 3, 4]         |\n",
            "|[5, 6]               |\n",
            "+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import flatten\n",
        "\n",
        "# Sample data: each row includes a product ID and nested arrays of review key phrases\n",
        "data = [\n",
        "    (1, [[\"great battery life\", \"sleek design\"], [\"heavy\", \"expensive\"], [\"sleek design\"]]),\n",
        "    (2, [[\"easy to install\", \"value for money\"], [\"requires maintenance\"]])\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, [\"product_id\", \"reviews\"])\n",
        "\n",
        "# Show the original DataFrame\n",
        "df.show(truncate=False)\n",
        "\n",
        "# Flatten the nested array of reviews into a single array per product\n",
        "flattened_df = df.withColumn(\"flattened_reviews\", flatten(df[\"reviews\"]))\n",
        "\n",
        "# Show the DataFrame with the flattened reviews\n",
        "flattened_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfvxahcBFFac",
        "outputId": "bf8feedf-7f97-4fff-b7fc-5d05421ceada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------------------------------------------------------------------+\n",
            "|product_id|reviews                                                                 |\n",
            "+----------+------------------------------------------------------------------------+\n",
            "|1         |[[great battery life, sleek design], [heavy, expensive], [sleek design]]|\n",
            "|2         |[[easy to install, value for money], [requires maintenance]]            |\n",
            "+----------+------------------------------------------------------------------------+\n",
            "\n",
            "+----------+------------------------------------------------------------------------+------------------------------------------------------------------+\n",
            "|product_id|reviews                                                                 |flattened_reviews                                                 |\n",
            "+----------+------------------------------------------------------------------------+------------------------------------------------------------------+\n",
            "|1         |[[great battery life, sleek design], [heavy, expensive], [sleek design]]|[great battery life, sleek design, heavy, expensive, sleek design]|\n",
            "|2         |[[easy to install, value for money], [requires maintenance]]            |[easy to install, value for money, requires maintenance]          |\n",
            "+----------+------------------------------------------------------------------------+------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import flatten\n",
        "# Sample DataFrame creation\n",
        "recs_df = spark.createDataFrame([\n",
        "    (1, [[\"prodA\", \"prodB\"], [\"prodC\"]]),\n",
        "    (2, [[\"prodD\"], [\"prodE\", \"prodF\"]])\n",
        "], [\"user_id\", \"recommendations\"])\n",
        "\n",
        "# without flattening\n",
        "recs_df.show(truncate=False)\n",
        "\n",
        "flattened_recs = recs_df.select(\"user_id\", flatten(\"recommendations\").alias(\"all_recs\"))\n",
        "flattened_recs.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kU1JpK6oGz4U",
        "outputId": "ac50341b-b9ec-4b13-ea90-9d0c9321c3ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------------+\n",
            "|user_id|recommendations          |\n",
            "+-------+-------------------------+\n",
            "|1      |[[prodA, prodB], [prodC]]|\n",
            "|2      |[[prodD], [prodE, prodF]]|\n",
            "+-------+-------------------------+\n",
            "\n",
            "+-------+---------------------+\n",
            "|user_id|all_recs             |\n",
            "+-------+---------------------+\n",
            "|1      |[prodA, prodB, prodC]|\n",
            "|2      |[prodD, prodE, prodF]|\n",
            "+-------+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explode"
      ],
      "metadata": {
        "id": "BfSfR0RXHM0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode\n",
        "df_exploded = recs_df.withColumn(\"all_recs\", explode(flatten(\"recommendations\")))\n",
        "df_exploded.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiyAZspqHKOJ",
        "outputId": "686f2177-89c5-4c17-b7c2-3d53feb18924"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------------+--------+\n",
            "|user_id|recommendations          |all_recs|\n",
            "+-------+-------------------------+--------+\n",
            "|1      |[[prodA, prodB], [prodC]]|prodA   |\n",
            "|1      |[[prodA, prodB], [prodC]]|prodB   |\n",
            "|1      |[[prodA, prodB], [prodC]]|prodC   |\n",
            "|2      |[[prodD], [prodE, prodF]]|prodD   |\n",
            "|2      |[[prodD], [prodE, prodF]]|prodE   |\n",
            "|2      |[[prodD], [prodE, prodF]]|prodF   |\n",
            "+-------+-------------------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode\n",
        "df_exploded = flattened_recs.withColumn(\"all_recs\", explode(\"all_recs\"))\n",
        "df_exploded.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iql5DT-zHo90",
        "outputId": "d381d439-d1a2-48f5-a5f3-9be762281c1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------+\n",
            "|user_id|all_recs|\n",
            "+-------+--------+\n",
            "|      1|   prodA|\n",
            "|      1|   prodB|\n",
            "|      1|   prodC|\n",
            "|      2|   prodD|\n",
            "|      2|   prodE|\n",
            "|      2|   prodF|\n",
            "+-------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode_outer\n",
        "\n",
        "# Handling nulls and empties\n",
        "nullable_df = spark.createDataFrame([\n",
        "    (1, [\"apple\", \"banana\"]),\n",
        "    (2, []),\n",
        "    (3, None)\n",
        "], [\"id\", \"fruits\"])\n",
        "\n",
        "# Applying explode_outer\n",
        "nullable_exploded = nullable_df.select(\"id\", explode(\"fruits\").alias(\"fruit\"))\n",
        "nullable_exploded.show()\n",
        "\n",
        "\n",
        "nullable_df.select(\"id\", explode_outer(\"fruits\").alias(\"fruit\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWuN5IUHIL6z",
        "outputId": "d2142e26-296b-4fdb-9407-85663ca8505a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+\n",
            "| id| fruit|\n",
            "+---+------+\n",
            "|  1| apple|\n",
            "|  1|banana|\n",
            "+---+------+\n",
            "\n",
            "+---+------+\n",
            "| id| fruit|\n",
            "+---+------+\n",
            "|  1| apple|\n",
            "|  1|banana|\n",
            "|  2|  NULL|\n",
            "|  3|  NULL|\n",
            "+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge arrays - Using Array Union(Variation of Concat)\n"
      ],
      "metadata": {
        "id": "oB2MIulWpdnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import array_union\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "data = [\n",
        "    (1, [1, 2, 3], [3, 4, 5]),\n",
        "    (2, [5, 6], [6, 7, 8])\n",
        "]\n",
        "df = spark.createDataFrame(data, [\"id\", \"arr1\", \"arr2\"])\n",
        "\n",
        "df2 = df.withColumn(\"union_array\", array_union(\"arr1\", \"arr2\"))\n",
        "\n",
        "df2.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTPRhY5-q-Y6",
        "outputId": "207f8cd8-9713-4bbd-abad-d86d81e3d8e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------+---------+---------------+\n",
            "|id |arr1     |arr2     |union_array    |\n",
            "+---+---------+---------+---------------+\n",
            "|1  |[1, 2, 3]|[3, 4, 5]|[1, 2, 3, 4, 5]|\n",
            "|2  |[5, 6]   |[6, 7, 8]|[5, 6, 7, 8]   |\n",
            "+---+---------+---------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Play with Map\n"
      ],
      "metadata": {
        "id": "r8_hBKIdhUev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating Map type**"
      ],
      "metadata": {
        "id": "FtiiHCUffA9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sql_map = spark.sql(\"SELECT map('Building','500 CR', 'Commercal',100) as income\")\n",
        "df_sql_map.printSchema()\n",
        "df_sql_map.show(truncate=False)"
      ],
      "metadata": {
        "id": "PpgJZQiUfJA3",
        "outputId": "aecd7037-6294-4dd5-da2e-c464636bb7fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- income: map (nullable = false)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = false)\n",
            "\n",
            "+--------------------------------------+\n",
            "|income                                |\n",
            "+--------------------------------------+\n",
            "|{Building -> 500 CR, Commercal -> 100}|\n",
            "+--------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "                              Row({\"Bank\":\"100 CR\", \"Business\":\"50 CR\", \"Land\":\"150 CR\"}),\n",
        "                              Row({\"Others\":\"300 CR\"}),\n",
        "                              Row({\"Building\":\"500 CR\", \"Commercal\":100}),\n",
        "                              Row({\"Building\":100 , \"Commercal\":100}),\n",
        "                            ]\n",
        "                            , [\"Assests\"])\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "id": "q74em3H4heaZ",
        "outputId": "639281ec-1b9c-4b08-f678-2429b9480e9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------+\n",
            "|Assests                                            |\n",
            "+---------------------------------------------------+\n",
            "|{Bank -> 100 CR, Land -> 150 CR, Business -> 50 CR}|\n",
            "|{Others -> 300 CR}                                 |\n",
            "|{Building -> 500 CR, Commercal -> 100}             |\n",
            "|{Building -> 100, Commercal -> 100}                |\n",
            "+---------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType,StructField, StringType, ArrayType,MapType\n",
        "\n",
        "\n",
        "data = [\n",
        "\n",
        "          ({\"Bank\":\"100 CR\", \"Business\":\"50 CR\", \"Land\":\"150 CR\"}),\n",
        "          ({\"Others\":\"300 CR\"}),\n",
        "          # ({\"Building\":\"500 CR\", \"Commercal\":100}),\n",
        "          # ({\"Building\":\"500 CR\", \"Commercal\":100}),\n",
        "\n",
        "      ]\n",
        "\n",
        "\n",
        "# Schema\n",
        "schema = StructType([\n",
        "               StructField('properties', MapType(StringType(),StringType()), True)\n",
        "        ])\n",
        "\n",
        "# Create DataFrame\n",
        "df_map = spark.createDataFrame(data = data, schema = schema)\n",
        "df_map.printSchema()\n",
        "df_map.show(truncate=False) # shows all columns"
      ],
      "metadata": {
        "id": "vL_8teOyfpC4",
        "outputId": "44a2954e-8e88-4144-f857-4042f3e8ad92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- properties: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            "\n",
            "+----------+\n",
            "|properties|\n",
            "+----------+\n",
            "|NULL      |\n",
            "|NULL      |\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "|spark.sql(\"SELECT struct(1, 2, 3) as ex_struct\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77On1brROzSI",
        "outputId": "11a8b49c-d290-4289-9972-b8180310fcc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[ex_struct: struct<col1:int,col2:int,col3:int>]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieve all keys from a map column(map_keys)"
      ],
      "metadata": {
        "id": "s6A806YNrHmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, MapType\n",
        "from pyspark.sql.functions import col, map_keys\n",
        "\n",
        "# Define MapType\n",
        "\n",
        "schema = StructType([\n",
        "StructField(\"id\", IntegerType(), True),\n",
        "StructField(\"attributes\", MapType(StringType(), StringType()), True)\n",
        "])\n",
        "\n",
        "# Sample Data\n",
        "\n",
        "data = [(1, {\"Key1\": \"Value1\", \"Key2\": \"Value2\"}), (2, {\"Key3\": \"Value3\", \"Key4\": \"Value4\"})]\n",
        "\n",
        "df = spark.createDataFrame(data, schema)\n",
        "df.show(truncate=False)\n",
        "df.select(map_keys(col(\"attributes\")).alias(\"keys\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVfLEashrHXr",
        "outputId": "5d4aef1d-916c-4f66-e0b2-b20a00c8cd2d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------------------+\n",
            "|id |attributes                      |\n",
            "+---+--------------------------------+\n",
            "|1  |{Key2 -> Value2, Key1 -> Value1}|\n",
            "|2  |{Key4 -> Value4, Key3 -> Value3}|\n",
            "+---+--------------------------------+\n",
            "\n",
            "+------------+\n",
            "|        keys|\n",
            "+------------+\n",
            "|[Key2, Key1]|\n",
            "|[Key4, Key3]|\n",
            "+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge map(map_concat)\n"
      ],
      "metadata": {
        "id": "_TwI5YiCSflB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import map_concat\n",
        "\n",
        "# Sample Data\n",
        "data = [\n",
        "    (1, {\"a\": 1, \"b\": 2}, {\"c\": 3, \"d\": 4}),\n",
        "    (2, {\"x\": 10}, {\"y\": 20, \"z\": 30}),\n",
        "    (3, {}, {\"p\": 100, \"q\": 200}),\n",
        "    (4, {\"k\": 5}, {})\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"id\", \"map1\", \"map2\"])\n",
        "\n",
        "print(\"before the merge\")\n",
        "df.show(truncate=False)\n",
        "\n",
        "# Merge the two map columns\n",
        "df_merged = df.withColumn(\"merged_map\", map_concat(\"map1\", \"map2\"))\n",
        "\n",
        "print(\"after the merge\")\n",
        "df_merged.show(truncate=False)\n"
      ],
      "metadata": {
        "id": "1nFfMq4SShZh",
        "outputId": "002ff1b7-e4d7-44dd-e18f-3bf9215d0848",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before the merge\n",
            "+---+----------------+--------------------+\n",
            "|id |map1            |map2                |\n",
            "+---+----------------+--------------------+\n",
            "|1  |{a -> 1, b -> 2}|{d -> 4, c -> 3}    |\n",
            "|2  |{x -> 10}       |{y -> 20, z -> 30}  |\n",
            "|3  |{}              |{p -> 100, q -> 200}|\n",
            "|4  |{k -> 5}        |{}                  |\n",
            "+---+----------------+--------------------+\n",
            "\n",
            "after the merge\n",
            "+---+----------------+--------------------+--------------------------------+\n",
            "|id |map1            |map2                |merged_map                      |\n",
            "+---+----------------+--------------------+--------------------------------+\n",
            "|1  |{a -> 1, b -> 2}|{d -> 4, c -> 3}    |{a -> 1, b -> 2, d -> 4, c -> 3}|\n",
            "|2  |{x -> 10}       |{y -> 20, z -> 30}  |{x -> 10, y -> 20, z -> 30}     |\n",
            "|3  |{}              |{p -> 100, q -> 200}|{p -> 100, q -> 200}            |\n",
            "|4  |{k -> 5}        |{}                  |{k -> 5}                        |\n",
            "+---+----------------+--------------------+--------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Play with Struct\n"
      ],
      "metadata": {
        "id": "Ky55RcyOiHOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_struct = spark.sql(\"SELECT struct(1, 2, '3') as ex_struct\")\n",
        "df_struct.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACGJLu9-NjKw",
        "outputId": "266cc7c4-a284-47da-8a16-b17910862ffe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|ex_struct|\n",
            "+---------+\n",
            "|{1, 2, 3}|\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_struct.select(\"ex_struct.col3\", \"ex_struct\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B46JwCaePajw",
        "outputId": "82ce3755-9e13-404d-897c-a0c22551a2bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------+\n",
            "|col3|ex_struct|\n",
            "+----+---------+\n",
            "|   3|{1, 2, 3}|\n",
            "+----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_struct.select(\"ex_struct.*\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeMsLV5Qdbp9",
        "outputId": "4a03e2d9-f573-4e65-f05f-57e597d116d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+----+\n",
            "|col1|col2|col3|\n",
            "+----+----+----+\n",
            "|1   |2   |3   |\n",
            "+----+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_map = spark.sql(\"SELECT map(1.0, '2', 3.0, '4') as ex_map\")\n",
        "df_map.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-Dl2Y1DOSd3",
        "outputId": "b1195e4e-834a-40f3-e646-3f8e06500f20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|              ex_map|\n",
            "+--------------------+\n",
            "|{1.0 -> 2, 3.0 -> 4}|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType,StructField, StringType, ArrayType,MapType\n",
        "\n",
        "# https://sparkbyexamples.com/pyspark/pyspark-maptype-dict-examples/\n",
        "\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "\n",
        "# Data\n",
        "data = [\n",
        "        ((\"James\",None,\"Smith\"),\"OH\",\"M\"),\n",
        "        ((\"Anna\",\"Rose\",\"\"),\"NY\",\"F\"),\n",
        "        ((\"Julia\",\"\",\"Williams\"),\"OH\",\"F\"),\n",
        "        ((\"Maria\",\"Anne\",\"Jones\"),\"NY\",\"M\"),\n",
        "        ((\"Jen\",\"Mary\",\"Brown\"),\"NY\",\"M\"),\n",
        "        ((\"Mike\",\"Mary\",\"Williams\"),\"OH\",\"M\")\n",
        "        ]\n",
        "\n",
        "# Schema\n",
        "schema = StructType([\n",
        "    StructField('name', StructType([\n",
        "         StructField('firstname', StringType(), True),\n",
        "         StructField('middlename', StringType(), True),\n",
        "         StructField('lastname', StringType(), True)\n",
        "         ])),\n",
        "     StructField('state', StringType(), True),\n",
        "     StructField('gender', StringType(), True)\n",
        "     ])\n",
        "\n",
        "# Create DataFrame\n",
        "df2 = spark.createDataFrame(data = data, schema = schema)\n",
        "df2.printSchema()\n",
        "df2.show(truncate=False) # shows all columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jye_i4rTOfIk",
        "outputId": "bbe0fdec-6e59-4576-ce1f-863553ac0384"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: struct (nullable = true)\n",
            " |    |-- firstname: string (nullable = true)\n",
            " |    |-- middlename: string (nullable = true)\n",
            " |    |-- lastname: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            "\n",
            "+----------------------+-----+------+\n",
            "|name                  |state|gender|\n",
            "+----------------------+-----+------+\n",
            "|{James, NULL, Smith}  |OH   |M     |\n",
            "|{Anna, Rose, }        |NY   |F     |\n",
            "|{Julia, , Williams}   |OH   |F     |\n",
            "|{Maria, Anne, Jones}  |NY   |M     |\n",
            "|{Jen, Mary, Brown}    |NY   |M     |\n",
            "|{Mike, Mary, Williams}|OH   |M     |\n",
            "+----------------------+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2.select(\"name\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLx1sLW0ctrH",
        "outputId": "6fdd982d-97ec-4db7-c198-c55727ce70ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|                name|\n",
            "+--------------------+\n",
            "|{James, NULL, Smith}|\n",
            "|      {Anna, Rose, }|\n",
            "| {Julia, , Williams}|\n",
            "|{Maria, Anne, Jones}|\n",
            "|  {Jen, Mary, Brown}|\n",
            "|{Mike, Mary, Will...|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType,StructField, StringType, ArrayType,MapType\n",
        "from pyspark.sql.functions import element_at\n",
        "\n",
        "data = [\n",
        "        ([(\"Yash\",\"K\",None),(\"Yash2\",\"K2\",None)],\"BL\",\"M\",[\"KGF 1\", \"KGF 2\"],{\"Bank\":\"100 CR\", \"Business\":\"50 CR\", \"Land\":\"150 CR\"}),\n",
        "        ([(\"Sudeep\",\"Kicha\",\"S\")],\"DL\",\"M\",[\"Autograph\", \"Kicha\",\"Hucha\"],{\"Others\":\"300 CR\"}),\n",
        "        ([(\"Puneeth\",None,\"Raj\")],\"MB\",\"M\",[], {\"Building\":\"500 CR\", \"Commercal\":100}),\n",
        "        ([(\"Darshan\",None,None)],\"MB\",\"M\",[], {\"Building\":\"500 CR\", \"Commercal\":100}),\n",
        "\n",
        "        ]\n",
        "\n",
        "# Schema\n",
        "schema = StructType([\n",
        "    StructField(    'name'\n",
        "                  , ArrayType(\n",
        "                        StructType([\n",
        "                            StructField('firstname', StringType(), True),\n",
        "                            StructField('middlename', StringType(), True),\n",
        "                            StructField('lastname', StringType(), True)\n",
        "                        ])\n",
        "                    ), True),\n",
        "     StructField('state', StringType(), True),\n",
        "     StructField('gender', StringType(), True),\n",
        "     StructField('movies', ArrayType(StringType()), True),\n",
        "     StructField('properties', MapType(StringType(),StringType()), True)\n",
        "     ])\n",
        "\n",
        "# Create DataFrame\n",
        "df2 = spark.createDataFrame(data = data, schema = schema)\n",
        "df2.printSchema()\n",
        "df2.show(truncate=False) # shows all columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJwWwq4Xc-0Z",
        "outputId": "468e11f5-95f8-4a03-80a4-102964f61988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- firstname: string (nullable = true)\n",
            " |    |    |-- middlename: string (nullable = true)\n",
            " |    |    |-- lastname: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- movies: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- properties: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            "\n",
            "+------------------------------------+-----+------+-------------------------+---------------------------------------------------+\n",
            "|name                                |state|gender|movies                   |properties                                         |\n",
            "+------------------------------------+-----+------+-------------------------+---------------------------------------------------+\n",
            "|[{Yash, K, NULL}, {Yash2, K2, NULL}]|BL   |M     |[KGF 1, KGF 2]           |{Bank -> 100 CR, Land -> 150 CR, Business -> 50 CR}|\n",
            "|[{Sudeep, Kicha, S}]                |DL   |M     |[Autograph, Kicha, Hucha]|{Others -> 300 CR}                                 |\n",
            "|[{Puneeth, NULL, Raj}]              |MB   |M     |[]                       |{Building -> 500 CR, Commercal -> 100}             |\n",
            "|[{Darshan, NULL, NULL}]             |MB   |M     |[]                       |{Building -> 500 CR, Commercal -> 100}             |\n",
            "+------------------------------------+-----+------+-------------------------+---------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " (df2\n",
        "  .select(\"properties\", \"name\", \"name.firstname\")\n",
        "  .withColumn(\"M-Building\", col(\"properties\").getItem(\"Building\"))\n",
        "  .withColumn(\"M-Commercal \", col(\"properties\").getItem(\"Commercal\"))\n",
        "  .withColumn(\"M-Others \", col(\"properties\").getItem(\"Others\"))\n",
        "  .withColumn(\"S-FName\", col(\"name.firstname\"))\n",
        "  ).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0-Z3EjP0njz",
        "outputId": "a6e04d9a-3bb8-48f6-9dcd-8a7d9bd219a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+---------+----------+------------+---------+-------+\n",
            "|          properties|                name|firstname|M-Building|M-Commercal |M-Others |S-FName|\n",
            "+--------------------+--------------------+---------+----------+------------+---------+-------+\n",
            "|{Bank -> 100 CR, ...|     {Yash, K, NULL}|     Yash|      NULL|        NULL|     NULL|   Yash|\n",
            "|  {Others -> 300 CR}|  {Sudeep, Kicha, S}|   Sudeep|      NULL|        NULL|   300 CR| Sudeep|\n",
            "|{Building -> 500 ...|{Puneeth, NULL, Raj}|  Puneeth|    500 CR|         100|     NULL|Puneeth|\n",
            "|{Building -> 500 ...|{Darshan, NULL, N...|  Darshan|    500 CR|         100|     NULL|Darshan|\n",
            "+--------------------+--------------------+---------+----------+------------+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType,StructField, StringType, ArrayType,MapType\n",
        "from pyspark.sql.functions import element_at\n",
        "\n",
        "data = [\n",
        "        ((\"Yash\",\"K\",None),\"BL\",\"M\",[\"KGF 1\", \"KGF 2\"],{\"Bank\":\"100 CR\", \"Business\":\"50 CR\", \"Land\":\"150 CR\"}),\n",
        "        ((\"Sudeep\",\"Kicha\",\"S\"),\"DL\",\"M\",[\"Autograph\", \"Kicha\",\"Hucha\"],{\"Others\":\"300 CR\"}),\n",
        "        ((\"Puneeth\",None,\"Raj\"),\"MB\",\"M\",[], {\"Building\":\"500 CR\", \"Commercal\":100}),\n",
        "        ((\"Darshan\",None,None),\"MB\",\"M\",[], {\"Building\":\"500 CR\", \"Commercal\":100}),\n",
        "\n",
        "        ]\n",
        "\n",
        "# Schema\n",
        "schema = StructType([\n",
        "    StructField('name', StructType([\n",
        "         StructField('firstname', StringType(), True),\n",
        "         StructField('middlename', StringType(), True),\n",
        "         StructField('lastname', StringType(), True)\n",
        "         ])),\n",
        "     StructField('state', StringType(), True),\n",
        "     StructField('gender', StringType(), True),\n",
        "     StructField('movies', ArrayType(StringType()), True),\n",
        "     StructField('properties', MapType(StringType(),StringType()), True)\n",
        "     ])\n",
        "\n",
        "# Create DataFrame\n",
        "df2 = spark.createDataFrame(data = data, schema = schema)\n",
        "df2.printSchema()\n",
        "df2.show(truncate=False) # shows all columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QiMme632DET",
        "outputId": "22e1ec1c-594b-473b-b950-2f87e1b7e29c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: struct (nullable = true)\n",
            " |    |-- firstname: string (nullable = true)\n",
            " |    |-- middlename: string (nullable = true)\n",
            " |    |-- lastname: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- movies: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- properties: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            "\n",
            "+---------------------+-----+------+-------------------------+---------------------------------------------------+\n",
            "|name                 |state|gender|movies                   |properties                                         |\n",
            "+---------------------+-----+------+-------------------------+---------------------------------------------------+\n",
            "|{Yash, K, NULL}      |BL   |M     |[KGF 1, KGF 2]           |{Bank -> 100 CR, Land -> 150 CR, Business -> 50 CR}|\n",
            "|{Sudeep, Kicha, S}   |DL   |M     |[Autograph, Kicha, Hucha]|{Others -> 300 CR}                                 |\n",
            "|{Puneeth, NULL, Raj} |MB   |M     |[]                       |{Building -> 500 CR, Commercal -> 100}             |\n",
            "|{Darshan, NULL, NULL}|MB   |M     |[]                       |{Building -> 500 CR, Commercal -> 100}             |\n",
            "+---------------------+-----+------+-------------------------+---------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
        "\n",
        "myManualSchema = StructType([\n",
        "  StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
        "  StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
        "  StructField(\"count\", LongType(), False, metadata={\"hello\":\"world\"})\n",
        "])\n",
        "df = spark.read.format(\"json\").schema(myManualSchema)\\\n",
        "  .load(\"sample_data/2015-summary.json\")\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "id": "21WEGU2v8_tA",
        "outputId": "50a1e446-52a5-4bf8-c206-028cf5b7f366",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/content/sample_data/2015-summary.json.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-50b2766d5f6a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m ])\n\u001b[1;32m      8\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyManualSchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sample_data/2015-summary.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/content/sample_data/2015-summary.json."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "id": "mcGnKSvKbQSR",
        "outputId": "42ba3405-038e-4bcd-fb0b-92de2fa18373",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
            " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
            " |-- count: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.read.format(\"json\").load(\"sample_data/2015-summary.json\").schema"
      ],
      "metadata": {
        "id": "vC1GYIlfbT9m",
        "outputId": "0e7f09ed-fa95-454d-f2dc-168a6a8eaac2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructType([StructField('DEST_COUNTRY_NAME', StringType(), True), StructField('ORIGIN_COUNTRY_NAME', StringType(), True), StructField('count', LongType(), True)])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge struct"
      ],
      "metadata": {
        "id": "Dnh1YCKdN4_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "data = [\n",
        "    Row(id=1, s1=Row(a=10, b=20), s2=Row(c=30, d=40)),\n",
        "    Row(id=2, s1=Row(a=100, b=200), s2=Row(c=300, d=400))\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data)\n",
        "\n",
        "df.show(truncate=False)\n",
        "df.printSchema()\n",
        "\n",
        "\n",
        "from pyspark.sql.functions import col, struct\n",
        "\n",
        "merged_df = df.withColumn(\n",
        "    \"merged_struct\",\n",
        "    struct(\n",
        "        col(\"s1.a\").alias(\"a\"),\n",
        "        col(\"s1.b\").alias(\"b\"),\n",
        "        col(\"s2.c\").alias(\"c\"),\n",
        "        col(\"s2.d\").alias(\"d\")\n",
        "    )\n",
        ")\n",
        "\n",
        "merged_df.select(\"id\", \"merged_struct\").show(truncate=False)\n",
        "merged_df.printSchema()\n"
      ],
      "metadata": {
        "id": "T49WlOf2N4zK",
        "outputId": "9dff8c08-6ea5-42eb-c25a-44b2f6e0768c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+----------+\n",
            "|id |s1        |s2        |\n",
            "+---+----------+----------+\n",
            "|1  |{10, 20}  |{30, 40}  |\n",
            "|2  |{100, 200}|{300, 400}|\n",
            "+---+----------+----------+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- s1: struct (nullable = true)\n",
            " |    |-- a: long (nullable = true)\n",
            " |    |-- b: long (nullable = true)\n",
            " |-- s2: struct (nullable = true)\n",
            " |    |-- c: long (nullable = true)\n",
            " |    |-- d: long (nullable = true)\n",
            "\n",
            "+---+--------------------+\n",
            "|id |merged_struct       |\n",
            "+---+--------------------+\n",
            "|1  |{10, 20, 30, 40}    |\n",
            "|2  |{100, 200, 300, 400}|\n",
            "+---+--------------------+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- s1: struct (nullable = true)\n",
            " |    |-- a: long (nullable = true)\n",
            " |    |-- b: long (nullable = true)\n",
            " |-- s2: struct (nullable = true)\n",
            " |    |-- c: long (nullable = true)\n",
            " |    |-- d: long (nullable = true)\n",
            " |-- merged_struct: struct (nullable = false)\n",
            " |    |-- a: long (nullable = true)\n",
            " |    |-- b: long (nullable = true)\n",
            " |    |-- c: long (nullable = true)\n",
            " |    |-- d: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Other Examples\n",
        "\n"
      ],
      "metadata": {
        "id": "Q2lTqwHdg9V4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using expressions"
      ],
      "metadata": {
        "id": "IJPbdlX1cAQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import col, expr\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "  Row(name=\"san\", salary=1500),\n",
        "  Row(name=\"ana\", salary=2000),\n",
        "  Row(name=\"shu\", salary=1000)\n",
        "])\n",
        "\n",
        "print(\"approach 1:\")\n",
        "df.withColumn(\"bonus\", df.salary * 0.1).show()\n",
        "\n",
        "print(\"approach 2:\")\n",
        "df.withColumn(\"bonus\", expr(\"salary * 0.1\")).show()\n",
        "\n",
        "print(\"approach 3:\")\n",
        "df.select(col(\"name\"), col(\"salary\"), (col(\"salary\") * 0.1).alias(\"bonus\")).show()"
      ],
      "metadata": {
        "id": "2DZCMcH5DkWZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c01f4e9-a34c-4dea-fe21-6fc6abe01c32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "approach 1:\n",
            "+----+------+-----+\n",
            "|name|salary|bonus|\n",
            "+----+------+-----+\n",
            "| san|  1500|150.0|\n",
            "| ana|  2000|200.0|\n",
            "| shu|  1000|100.0|\n",
            "+----+------+-----+\n",
            "\n",
            "approach 2:\n",
            "+----+------+-----+\n",
            "|name|salary|bonus|\n",
            "+----+------+-----+\n",
            "| san|  1500|150.0|\n",
            "| ana|  2000|200.0|\n",
            "| shu|  1000|100.0|\n",
            "+----+------+-----+\n",
            "\n",
            "approach 3:\n",
            "+----+------+-----+\n",
            "|name|salary|bonus|\n",
            "+----+------+-----+\n",
            "| san|  1500|150.0|\n",
            "| ana|  2000|200.0|\n",
            "| shu|  1000|100.0|\n",
            "+----+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nested fields fetching(struct)"
      ],
      "metadata": {
        "id": "LdZGUXj5T3py"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "(1, {\"city\": \"New York\", \"state\": \"NY\"}),\n",
        "(2, {\"city\": \"San Francisco\", \"state\": \"CA\"})\n",
        "]\n",
        "\n",
        "schema = [\"id\", \"address\"]\n",
        "\n",
        "df = spark.createDataFrame(data, schema=schema)\n",
        "\n",
        "print(\"approach 1\")\n",
        "df.select(col(\"address.city\")).show() #correct syntax\n",
        "\n",
        "print(\"approach 2\")\n",
        "df.select(\"address.city\").show() #correct syntax\n",
        "\n",
        "print(\"approach 3\")\n",
        "df.select(col(\"address\")[\"city\"]).show() #correct syntax\n",
        "\n",
        "print(\"approach 4\")\n",
        "(df.select(\"address\")\n",
        "   .select(col(\"address.city\")).show()) #correct syntax"
      ],
      "metadata": {
        "id": "WhEcKtt3bvJp",
        "outputId": "79dc4b1d-c712-4bc3-d636-15be2dc7ab50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "approach 1\n",
            "+-------------+\n",
            "|         city|\n",
            "+-------------+\n",
            "|     New York|\n",
            "|San Francisco|\n",
            "+-------------+\n",
            "\n",
            "approach 2\n",
            "+-------------+\n",
            "|         city|\n",
            "+-------------+\n",
            "|     New York|\n",
            "|San Francisco|\n",
            "+-------------+\n",
            "\n",
            "approach 3\n",
            "+-------------+\n",
            "|address[city]|\n",
            "+-------------+\n",
            "|     New York|\n",
            "|San Francisco|\n",
            "+-------------+\n",
            "\n",
            "approach 4\n",
            "+-------------+\n",
            "|         city|\n",
            "+-------------+\n",
            "|     New York|\n",
            "|San Francisco|\n",
            "+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SELECT mappings"
      ],
      "metadata": {
        "id": "RPrQSmMFUZSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import Row\n",
        "from pyspark.sql.types import StructType, StructField, LongType, StringType\n",
        "\n",
        "# Define Schema\n",
        "schema = StructType([\n",
        "    StructField(\"id\", LongType(), True),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"age\", LongType(), True),\n",
        "    StructField(\"salary\", LongType(), True)\n",
        "])\n",
        "\n",
        "# Create Dummy Data\n",
        "data = [\n",
        "    Row(id=1, name=\"San\", age=30, salary=70000),\n",
        "    Row(id=2, name=\"Ana\", age=35, salary=80000),\n",
        "    Row(id=3, name=\"Shuchi\", age=25, salary=50000),\n",
        "    Row(id=4, name=\"Krishna\", age=40, salary=90000)\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, schema)\n",
        "df.show()"
      ],
      "metadata": {
        "id": "hTHdYCwQT8rs",
        "outputId": "e42185e4-90c0-4444-885e-f7a261f0f9f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+---+------+\n",
            "| id|   name|age|salary|\n",
            "+---+-------+---+------+\n",
            "|  1|    San| 30| 70000|\n",
            "|  2|    Ana| 35| 80000|\n",
            "|  3| Shuchi| 25| 50000|\n",
            "|  4|Krishna| 40| 90000|\n",
            "+---+-------+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import expr\n",
        "\n",
        "# Select 'name' and 'age', and create 'age_category' based on 'age'\n",
        "df_expr = df.select(\n",
        "    \"name\",\n",
        "    \"age\",\n",
        "    expr(\"CASE WHEN age >= 35 THEN 'Senior' \\\n",
        "ELSE 'Junior' END as age_category\")\n",
        ")\n",
        "df_expr.show()\n",
        "\n",
        "\n",
        "from pyspark.sql.functions import expr\n",
        "\n",
        "# Select 'name' and 'age', and create 'age_category' based on 'age'\n",
        "df_select_expr = df.selectExpr(\n",
        "    \"name\",\n",
        "    \"age\",\n",
        "    \"CASE WHEN age >= 35 THEN 'Senior' ELSE 'Junior' END as age_category\"\n",
        ")\n",
        "df_select_expr.show()"
      ],
      "metadata": {
        "id": "ZuXs2kcuUfFR",
        "outputId": "392b7e37-f147-41be-ea40-d2c1797d64bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+------------+\n",
            "|   name|age|age_category|\n",
            "+-------+---+------------+\n",
            "|    San| 30|      Junior|\n",
            "|    Ana| 35|      Senior|\n",
            "| Shuchi| 25|      Junior|\n",
            "|Krishna| 40|      Senior|\n",
            "+-------+---+------------+\n",
            "\n",
            "+-------+---+------------+\n",
            "|   name|age|age_category|\n",
            "+-------+---+------------+\n",
            "|    San| 30|      Junior|\n",
            "|    Ana| 35|      Senior|\n",
            "| Shuchi| 25|      Junior|\n",
            "|Krishna| 40|      Senior|\n",
            "+-------+---+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Add a new column 'age_plus_5'\n",
        "df_with_age = df.withColumn(\"age_plus_5\", col(\"age\") + 5)\n",
        "df_with_age.show()"
      ],
      "metadata": {
        "id": "JqWBxOnrUjlW",
        "outputId": "d2ca1302-ca79-4131-a629-c95891e2bc19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+---+------+----------+\n",
            "| id|   name|age|salary|age_plus_5|\n",
            "+---+-------+---+------+----------+\n",
            "|  1|    San| 30| 70000|        35|\n",
            "|  2|    Ana| 35| 80000|        40|\n",
            "|  3| Shuchi| 25| 50000|        30|\n",
            "|  4|Krishna| 40| 90000|        45|\n",
            "+---+-------+---+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cast 'salary' from LongType to DoubleType\n",
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_casted = df.withColumn(\"salary_double\", col(\"salary\").cast(DoubleType()))\n",
        "df_casted.show()"
      ],
      "metadata": {
        "id": "Luebgve5Uq2R",
        "outputId": "53837180-090b-4c8d-ae63-47923c1a2130",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+---+------+-------------+\n",
            "| id|   name|age|salary|salary_double|\n",
            "+---+-------+---+------+-------------+\n",
            "|  1|    San| 30| 70000|      70000.0|\n",
            "|  2|    Ana| 35| 80000|      80000.0|\n",
            "|  3| Shuchi| 25| 50000|      50000.0|\n",
            "|  4|Krishna| 40| 90000|      90000.0|\n",
            "+---+-------+---+------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UDF"
      ],
      "metadata": {
        "id": "GvUJ1n6_XpuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql.functions import pandas_udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "@pandas_udf(IntegerType())\n",
        "def string_length(s: pd.Series) -> pd.Series:\n",
        "    return s.str.len()"
      ],
      "metadata": {
        "id": "7MzeRaxxUuaK"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r4VQR80ZXraB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}