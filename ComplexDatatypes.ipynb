{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOLDyoCIHdp6EdTzj3TLoTa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SanjayJanardhan-89/ApacheSparkHandsOn/blob/main/ComplexDatatypes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WuOZ0C0lhCL4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Pyspark\n"
      ],
      "metadata": {
        "id": "vRYFGd5ehHN5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 812
        },
        "id": "QLc7s-Qo1irp",
        "outputId": "1a025a06-6c34-4eb1-af77-62df9bca95a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "\r            \rGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.81)] [Waiting for headers] [1\u001b[0m\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.81)] [Waiting for headers] [W\u001b[0m\u001b[33m\r0% [Connecting to archive.ubuntu.com (185.125.190.81)] [Waiting for headers] [W\u001b[0m\r                                                                               \rGet:3 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,804 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,750 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,077 kB]\n",
            "Get:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [4,763 kB]\n",
            "Get:15 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [51.0 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [56.4 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,566 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,262 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,092 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,404 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [48.5 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,917 kB]\n",
            "Fetched 33.2 MB in 2s (13.8 MB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "38 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (0.10.9.7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x794ecae83590>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://da101886e934:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>OurSparkApp</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"OurSparkApp\") \\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Play with Arrays\n"
      ],
      "metadata": {
        "id": "cAbqNvQRhMI2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Append an item to array(array_append)"
      ],
      "metadata": {
        "id": "lxUvBW5t82tO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import array_append\n",
        "\n",
        "data = [(1, [\"apple\", \"banana\"]), (2, [\"orange\", \"grape\"])]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"id\", \"fruits\"])\n",
        "\n",
        "print(\"Original DataFrame:\")\n",
        "df.show(truncate=False)\n",
        "\n",
        "df_with_new_fruit = df.withColumn(\"fruits\", array_append(df[\"fruits\"], \"kiwi\"))\n",
        "\n",
        "print(\"Updated DataFrame with new fruit:\")\n",
        "df_with_new_fruit.show(truncate=False)\n"
      ],
      "metadata": {
        "id": "AjgKK6QC81c0",
        "outputId": "204fbefd-c0f2-457a-b4ec-91035828c407",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+---+---------------+\n",
            "|id |fruits         |\n",
            "+---+---------------+\n",
            "|1  |[apple, banana]|\n",
            "|2  |[orange, grape]|\n",
            "+---+---------------+\n",
            "\n",
            "Updated DataFrame with new fruit:\n",
            "+---+---------------------+\n",
            "|id |fruits               |\n",
            "+---+---------------------+\n",
            "|1  |[apple, banana, kiwi]|\n",
            "|2  |[orange, grape, kiwi]|\n",
            "+---+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Length of the array(size)"
      ],
      "metadata": {
        "id": "Yr8cte2KYfUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import size\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "data = [\n",
        "    (1, [1, 2, 3]),\n",
        "    (2, [4, 5]),\n",
        "    (3, [])\n",
        "]\n",
        "df = spark.createDataFrame(data, [\"id\", \"numbers\"])\n",
        "\n",
        "df.show(truncate=False)\n",
        "\n",
        "from pyspark.sql.functions import size, col\n",
        "\n",
        "df_with_size = df.withColumn(\"numbers_size\", size(\"numbers\"))\n",
        "df_with_size = df_with_size.withColumn(\"numbers_size_2\", size(col(\"numbers\")))\n",
        "\n",
        "df_with_size.show()"
      ],
      "metadata": {
        "id": "VrOTfPUeYe29",
        "outputId": "c2fe420e-8fad-4214-8a10-653d8ee678a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------+\n",
            "|id |numbers  |\n",
            "+---+---------+\n",
            "|1  |[1, 2, 3]|\n",
            "|2  |[4, 5]   |\n",
            "|3  |[]       |\n",
            "+---+---------+\n",
            "\n",
            "+---+---------+------------+--------------+\n",
            "| id|  numbers|numbers_size|numbers_size_2|\n",
            "+---+---------+------------+--------------+\n",
            "|  1|[1, 2, 3]|           3|             3|\n",
            "|  2|   [4, 5]|           2|             2|\n",
            "|  3|       []|           0|             0|\n",
            "+---+---------+------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fist item from array(element_at)"
      ],
      "metadata": {
        "id": "fAp8LKThU9q5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import element_at\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Example DataFrame with an array column\n",
        "\n",
        "data = [\n",
        "(1, [10, 20, 30]),\n",
        "(2, [40, 50, 60]),\n",
        "(3, [70, 80, 90])\n",
        "]\n",
        "\n",
        "schema = [\"id\", \"numbers\"]\n",
        "\n",
        "df = spark.createDataFrame(data, schema=schema)\n",
        "df.show()\n",
        "\n",
        "# Select the first element from the array column 'numbers'\n",
        "df.select(element_at(col(\"numbers\"), 2).alias(\"first_element\")).show()"
      ],
      "metadata": {
        "id": "fJf-7FI1Ykr5",
        "outputId": "09945ba8-f61a-439b-e8d2-12e6b33f455d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------+\n",
            "| id|     numbers|\n",
            "+---+------------+\n",
            "|  1|[10, 20, 30]|\n",
            "|  2|[40, 50, 60]|\n",
            "|  3|[70, 80, 90]|\n",
            "+---+------------+\n",
            "\n",
            "+-------------+\n",
            "|first_element|\n",
            "+-------------+\n",
            "|           20|\n",
            "|           50|\n",
            "|           80|\n",
            "+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create array using Spark SQL"
      ],
      "metadata": {
        "id": "YpGEFG0bYy4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sql_array = spark.sql(\"SELECT array('KGF 1', 'KGF 2', 'Autograph', 'Kicha','Hucha') as movies\")\n",
        "df_sql_array.printSchema()\n",
        "df_sql_array.show(truncate=False)"
      ],
      "metadata": {
        "id": "NDhHyKMo5Q4K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43e9624a-362a-4269-aa40-353740809032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- movies: array (nullable = false)\n",
            " |    |-- element: string (containsNull = false)\n",
            "\n",
            "+---------------------------------------+\n",
            "|movies                                 |\n",
            "+---------------------------------------+\n",
            "|[KGF 1, KGF 2, Autograph, Kicha, Hucha]|\n",
            "+---------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType,StructField, StringType, ArrayType,MapType\n",
        "\n",
        "data = [\n",
        "          [\n",
        "            [\"KGF 1\", \"KGF 2\", \"Autograph\", \"Kicha\",\"Hucha\"],\n",
        "            # [\"Hello\", \"Hi\"]\n",
        "          ]\n",
        "        ]\n",
        "\n",
        "# Schema\n",
        "schema = StructType([\n",
        "             StructField('movies', ArrayType(StringType()), True),\n",
        "     ])\n",
        "\n",
        "# Create DataFrame\n",
        "df_array = spark.createDataFrame(data = data, schema = schema)\n",
        "df_array.printSchema()\n",
        "df_array.show(truncate=False) # shows all columns"
      ],
      "metadata": {
        "id": "KRnw_ETEci45",
        "outputId": "65004037-5205-43a5-ffe6-9b9df4b5a9b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- movies: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n",
            "+---------------------------------------+\n",
            "|movies                                 |\n",
            "+---------------------------------------+\n",
            "|[KGF 1, KGF 2, Autograph, Kicha, Hucha]|\n",
            "+---------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge 2 arrays(concat)\n"
      ],
      "metadata": {
        "id": "eKUjWRTJpdqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import concat, array\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "data = [\n",
        "    (1, [1, 2], [1,3, 4]),\n",
        "    (2, [5, 6], [7, 8]),\n",
        "]\n",
        "df = spark.createDataFrame(data, [\"id\", \"arr1\", \"arr2\"])\n",
        "\n",
        "# concat arrays\n",
        "df2 = df.withColumn(\"merged\", concat(\"arr1\", \"arr2\"))\n",
        "\n",
        "df2.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmzRZag1q150",
        "outputId": "4e2a647f-a554-4c8c-9b74-a0405a747b31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---------+---------------+\n",
            "|id |arr1  |arr2     |merged         |\n",
            "+---+------+---------+---------------+\n",
            "|1  |[1, 2]|[1, 3, 4]|[1, 2, 1, 3, 4]|\n",
            "|2  |[5, 6]|[7, 8]   |[5, 6, 7, 8]   |\n",
            "+---+------+---------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Concat cant not handle nested array\n",
        "\n",
        "from pyspark.sql.functions import flatten, concat\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    (1, [[1, 2], [3, 4]]),\n",
        "    (2, [[5, 6]])\n",
        "], [\"id\", \"nested_array\"])\n",
        "\n",
        "df.select(\"id\",\"nested_array\",concat(\"nested_array\")).show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sK1Ig2YFMYU",
        "outputId": "0d787ea1-4b21-4840-bab5-0ddab309d96b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------------+--------------------+\n",
            "|id |nested_array    |concat(nested_array)|\n",
            "+---+----------------+--------------------+\n",
            "|1  |[[1, 2], [3, 4]]|[[1, 2], [3, 4]]    |\n",
            "|2  |[[5, 6]]        |[[5, 6]]            |\n",
            "+---+----------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flatten Nested Array(flatten)\n"
      ],
      "metadata": {
        "id": "jw-4Lda2EYxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import flatten\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    (1, [[1, 2], [3, 4]]),\n",
        "    (2, [[5, 6]])\n",
        "], [\"id\", \"nested_array\"])\n",
        "\n",
        "df.select(flatten(\"nested_array\")).show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRSAVBMzEbZv",
        "outputId": "edbe1f7d-27da-45f4-ad67-d75c40a17903"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------+\n",
            "|flatten(nested_array)|\n",
            "+---------------------+\n",
            "|[1, 2, 3, 4]         |\n",
            "|[5, 6]               |\n",
            "+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import flatten\n",
        "\n",
        "# Sample data: each row includes a product ID and nested arrays of review key phrases\n",
        "data = [\n",
        "    (1, [[\"great battery life\", \"sleek design\"], [\"heavy\", \"expensive\"], [\"sleek design\"]]),\n",
        "    (2, [[\"easy to install\", \"value for money\"], [\"requires maintenance\"]])\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, [\"product_id\", \"reviews\"])\n",
        "\n",
        "# Show the original DataFrame\n",
        "df.show(truncate=False)\n",
        "\n",
        "# Flatten the nested array of reviews into a single array per product\n",
        "flattened_df = df.withColumn(\"flattened_reviews\", flatten(df[\"reviews\"]))\n",
        "\n",
        "# Show the DataFrame with the flattened reviews\n",
        "flattened_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfvxahcBFFac",
        "outputId": "bf8feedf-7f97-4fff-b7fc-5d05421ceada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------------------------------------------------------------------+\n",
            "|product_id|reviews                                                                 |\n",
            "+----------+------------------------------------------------------------------------+\n",
            "|1         |[[great battery life, sleek design], [heavy, expensive], [sleek design]]|\n",
            "|2         |[[easy to install, value for money], [requires maintenance]]            |\n",
            "+----------+------------------------------------------------------------------------+\n",
            "\n",
            "+----------+------------------------------------------------------------------------+------------------------------------------------------------------+\n",
            "|product_id|reviews                                                                 |flattened_reviews                                                 |\n",
            "+----------+------------------------------------------------------------------------+------------------------------------------------------------------+\n",
            "|1         |[[great battery life, sleek design], [heavy, expensive], [sleek design]]|[great battery life, sleek design, heavy, expensive, sleek design]|\n",
            "|2         |[[easy to install, value for money], [requires maintenance]]            |[easy to install, value for money, requires maintenance]          |\n",
            "+----------+------------------------------------------------------------------------+------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import flatten\n",
        "# Sample DataFrame creation\n",
        "recs_df = spark.createDataFrame([\n",
        "    (1, [[\"prodA\", \"prodB\"], [\"prodC\"]]),\n",
        "    (2, [[\"prodD\"], [\"prodE\", \"prodF\"]])\n",
        "], [\"user_id\", \"recommendations\"])\n",
        "\n",
        "# without flattening\n",
        "recs_df.show(truncate=False)\n",
        "\n",
        "flattened_recs = recs_df.select(\"user_id\", flatten(\"recommendations\").alias(\"all_recs\"))\n",
        "flattened_recs.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kU1JpK6oGz4U",
        "outputId": "ac50341b-b9ec-4b13-ea90-9d0c9321c3ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------------+\n",
            "|user_id|recommendations          |\n",
            "+-------+-------------------------+\n",
            "|1      |[[prodA, prodB], [prodC]]|\n",
            "|2      |[[prodD], [prodE, prodF]]|\n",
            "+-------+-------------------------+\n",
            "\n",
            "+-------+---------------------+\n",
            "|user_id|all_recs             |\n",
            "+-------+---------------------+\n",
            "|1      |[prodA, prodB, prodC]|\n",
            "|2      |[prodD, prodE, prodF]|\n",
            "+-------+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explode-values in array to rows(explode)\n"
      ],
      "metadata": {
        "id": "BfSfR0RXHM0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode\n",
        "df_exploded = recs_df.withColumn(\"all_recs\", explode(flatten(\"recommendations\")))\n",
        "df_exploded.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiyAZspqHKOJ",
        "outputId": "686f2177-89c5-4c17-b7c2-3d53feb18924"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------------+--------+\n",
            "|user_id|recommendations          |all_recs|\n",
            "+-------+-------------------------+--------+\n",
            "|1      |[[prodA, prodB], [prodC]]|prodA   |\n",
            "|1      |[[prodA, prodB], [prodC]]|prodB   |\n",
            "|1      |[[prodA, prodB], [prodC]]|prodC   |\n",
            "|2      |[[prodD], [prodE, prodF]]|prodD   |\n",
            "|2      |[[prodD], [prodE, prodF]]|prodE   |\n",
            "|2      |[[prodD], [prodE, prodF]]|prodF   |\n",
            "+-------+-------------------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode\n",
        "df_exploded = flattened_recs.withColumn(\"all_recs\", explode(\"all_recs\"))\n",
        "df_exploded.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iql5DT-zHo90",
        "outputId": "d381d439-d1a2-48f5-a5f3-9be762281c1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------+\n",
            "|user_id|all_recs|\n",
            "+-------+--------+\n",
            "|      1|   prodA|\n",
            "|      1|   prodB|\n",
            "|      1|   prodC|\n",
            "|      2|   prodD|\n",
            "|      2|   prodE|\n",
            "|      2|   prodF|\n",
            "+-------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode_outer\n",
        "\n",
        "# Handling nulls and empties\n",
        "nullable_df = spark.createDataFrame([\n",
        "    (1, [\"apple\", \"banana\"]),\n",
        "    (2, []),\n",
        "    (3, None)\n",
        "], [\"id\", \"fruits\"])\n",
        "\n",
        "# Applying explode_outer\n",
        "nullable_exploded = nullable_df.select(\"id\", explode(\"fruits\").alias(\"fruit\"))\n",
        "nullable_exploded.show()\n",
        "\n",
        "\n",
        "nullable_df.select(\"id\", explode_outer(\"fruits\").alias(\"fruit\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWuN5IUHIL6z",
        "outputId": "d2142e26-296b-4fdb-9407-85663ca8505a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+\n",
            "| id| fruit|\n",
            "+---+------+\n",
            "|  1| apple|\n",
            "|  1|banana|\n",
            "+---+------+\n",
            "\n",
            "+---+------+\n",
            "| id| fruit|\n",
            "+---+------+\n",
            "|  1| apple|\n",
            "|  1|banana|\n",
            "|  2|  NULL|\n",
            "|  3|  NULL|\n",
            "+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge arrays - Using array_union(Variation of Concat)\n"
      ],
      "metadata": {
        "id": "oB2MIulWpdnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import array_union\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "data = [\n",
        "    (1, [1, 2, 3], [3, 4, 5]),\n",
        "    (2, [5, 6], [6, 7, 8])\n",
        "]\n",
        "df = spark.createDataFrame(data, [\"id\", \"arr1\", \"arr2\"])\n",
        "\n",
        "df2 = df.withColumn(\"union_array\", array_union(\"arr1\", \"arr2\"))\n",
        "\n",
        "df2.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTPRhY5-q-Y6",
        "outputId": "207f8cd8-9713-4bbd-abad-d86d81e3d8e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------+---------+---------------+\n",
            "|id |arr1     |arr2     |union_array    |\n",
            "+---+---------+---------+---------------+\n",
            "|1  |[1, 2, 3]|[3, 4, 5]|[1, 2, 3, 4, 5]|\n",
            "|2  |[5, 6]   |[6, 7, 8]|[5, 6, 7, 8]   |\n",
            "+---+---------+---------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## From Certificaiton"
      ],
      "metadata": {
        "id": "04o0Q1ke-SlN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sample Data"
      ],
      "metadata": {
        "id": "XVPmImlJ-VuN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample DataFrame\n",
        "data = [\n",
        "    (1, \"apple banana cherry\"),\n",
        "    (2, \"grape orange mango\"),\n",
        "    (3, \"kiwi\"),\n",
        "    (4, None)\n",
        "]\n",
        "columns = [\"id\", \"desc\"]\n",
        "\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Show original DataFrame\n",
        "print(\"Original DataFrame:\")\n",
        "df.show()"
      ],
      "metadata": {
        "id": "VKJLjyqq-Uvu",
        "outputId": "d05253c7-565d-4cf0-e795-aa9150a2dbcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original DataFrame:\n",
            "+---+-------------------+\n",
            "| id|               desc|\n",
            "+---+-------------------+\n",
            "|  1|apple banana cherry|\n",
            "|  2| grape orange mango|\n",
            "|  3|               kiwi|\n",
            "|  4|               NULL|\n",
            "+---+-------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import split, col\n",
        "\n",
        "# Split 'desc' column into an array of words\n",
        "df_with_array = df.withColumn(\"array_col\", split(col(\"desc\"), \" \"))\n",
        "\n",
        "# Show DataFrame with the new array column\n",
        "print(\"DataFrame with 'array_col' (split into an array):\")\n",
        "df_with_array.show(truncate=False)"
      ],
      "metadata": {
        "id": "oH4ZGeFk-Y-7",
        "outputId": "6465ab09-b078-4cd3-f18e-37ee8ee6e39e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame with 'array_col' (split into an array):\n",
            "+---+-------------------+-----------------------+\n",
            "|id |desc               |array_col              |\n",
            "+---+-------------------+-----------------------+\n",
            "|1  |apple banana cherry|[apple, banana, cherry]|\n",
            "|2  |grape orange mango |[grape, orange, mango] |\n",
            "|3  |kiwi               |[kiwi]                 |\n",
            "|4  |NULL               |NULL                   |\n",
            "+---+-------------------+-----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Access the first element of the array column\n",
        "df_first_element = df_with_array.select(col(\"id\"), col(\"array_col\")[1].alias(\"first_element\"))\n",
        "\n",
        "# Show DataFrame with the first element of the array\n",
        "print(\"DataFrame with first element of 'array_col':\")\n",
        "df_first_element.show()"
      ],
      "metadata": {
        "id": "6KU0mStS-fPS",
        "outputId": "66c8fd6f-0e7c-4f28-e71d-1ccc4c7f6e65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame with first element of 'array_col':\n",
            "+---+-------------+\n",
            "| id|first_element|\n",
            "+---+-------------+\n",
            "|  1|       banana|\n",
            "|  2|       orange|\n",
            "|  3|         NULL|\n",
            "|  4|         NULL|\n",
            "+---+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import size\n",
        "\n",
        "# Calculate the length of the array column\n",
        "df_array_size = df_with_array.select(col(\"id\"), \\\n",
        "    size(col(\"array_col\")).alias(\"array_length\"))\n",
        "\n",
        "# Show DataFrame with the array length\n",
        "print(\"DataFrame with array length:\")\n",
        "df_array_size.show()"
      ],
      "metadata": {
        "id": "LAeTvgwb-otj",
        "outputId": "7c73f409-8230-48a5-8397-15a4cf6e5f52",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame with array length:\n",
            "+---+------------+\n",
            "| id|array_length|\n",
            "+---+------------+\n",
            "|  1|           3|\n",
            "|  2|           3|\n",
            "|  3|           1|\n",
            "|  4|          -1|\n",
            "+---+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### array_contains"
      ],
      "metadata": {
        "id": "AWE_5P9M-wRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import array_contains\n",
        "\n",
        "# Check if the array contains the word 'banana'\n",
        "df_array_contains = df_with_array.select(\n",
        "    col(\"id\"),\n",
        "    array_contains(col(\"array_col\"), \"banana\").alias(\"contains_banana\")\n",
        ")\n",
        "\n",
        "# Show DataFrame with the result of array_contains\n",
        "print(\"DataFrame with array contains 'banana':\")\n",
        "df_array_contains.show()"
      ],
      "metadata": {
        "id": "QA_zpmwt-r5p",
        "outputId": "d8ee203d-2bad-4f01-b0d6-00cc6c729a50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame with array contains 'banana':\n",
            "+---+---------------+\n",
            "| id|contains_banana|\n",
            "+---+---------------+\n",
            "|  1|           true|\n",
            "|  2|          false|\n",
            "|  3|          false|\n",
            "|  4|           NULL|\n",
            "+---+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Play with Map\n"
      ],
      "metadata": {
        "id": "r8_hBKIdhUev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating Map type**"
      ],
      "metadata": {
        "id": "FtiiHCUffA9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sql_map = spark.sql(\"SELECT map('Building','500 CR', 'Commercal',100) as income\")\n",
        "df_sql_map.printSchema()\n",
        "df_sql_map.show(truncate=False)"
      ],
      "metadata": {
        "id": "PpgJZQiUfJA3",
        "outputId": "aecd7037-6294-4dd5-da2e-c464636bb7fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- income: map (nullable = false)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = false)\n",
            "\n",
            "+--------------------------------------+\n",
            "|income                                |\n",
            "+--------------------------------------+\n",
            "|{Building -> 500 CR, Commercal -> 100}|\n",
            "+--------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "                              Row({\"Bank\":\"100 CR\", \"Business\":\"50 CR\", \"Land\":\"150 CR\"}),\n",
        "                              Row({\"Others\":\"300 CR\"}),\n",
        "                              Row({\"Building\":\"500 CR\", \"Commercal\":100}),\n",
        "                              Row({\"Building\":100 , \"Commercal\":100}),\n",
        "                            ]\n",
        "                            , [\"Assests\"])\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "id": "q74em3H4heaZ",
        "outputId": "639281ec-1b9c-4b08-f678-2429b9480e9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------+\n",
            "|Assests                                            |\n",
            "+---------------------------------------------------+\n",
            "|{Bank -> 100 CR, Land -> 150 CR, Business -> 50 CR}|\n",
            "|{Others -> 300 CR}                                 |\n",
            "|{Building -> 500 CR, Commercal -> 100}             |\n",
            "|{Building -> 100, Commercal -> 100}                |\n",
            "+---------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType,StructField, StringType, ArrayType,MapType\n",
        "\n",
        "\n",
        "data = [\n",
        "\n",
        "          ({\"Bank\":\"100 CR\", \"Business\":\"50 CR\", \"Land\":\"150 CR\"}),\n",
        "          ({\"Others\":\"300 CR\"}),\n",
        "          # ({\"Building\":\"500 CR\", \"Commercal\":100}),\n",
        "          # ({\"Building\":\"500 CR\", \"Commercal\":100}),\n",
        "\n",
        "      ]\n",
        "\n",
        "\n",
        "# Schema\n",
        "schema = StructType([\n",
        "               StructField('properties', MapType(StringType(),StringType()), True)\n",
        "        ])\n",
        "\n",
        "# Create DataFrame\n",
        "df_map = spark.createDataFrame(data = data, schema = schema)\n",
        "df_map.printSchema()\n",
        "df_map.show(truncate=False) # shows all columns"
      ],
      "metadata": {
        "id": "vL_8teOyfpC4",
        "outputId": "44a2954e-8e88-4144-f857-4042f3e8ad92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- properties: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            "\n",
            "+----------+\n",
            "|properties|\n",
            "+----------+\n",
            "|NULL      |\n",
            "|NULL      |\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "|spark.sql(\"SELECT struct(1, 2, 3) as ex_struct\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77On1brROzSI",
        "outputId": "11a8b49c-d290-4289-9972-b8180310fcc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[ex_struct: struct<col1:int,col2:int,col3:int>]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieve all keys from a map column(map_keys)"
      ],
      "metadata": {
        "id": "s6A806YNrHmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, MapType\n",
        "from pyspark.sql.functions import col, map_keys\n",
        "\n",
        "# Define MapType\n",
        "\n",
        "schema = StructType([\n",
        "StructField(\"id\", IntegerType(), True),\n",
        "StructField(\"attributes\", MapType(StringType(), StringType()), True)\n",
        "])\n",
        "\n",
        "# Sample Data\n",
        "\n",
        "data = [(1, {\"Key1\": \"Value1\", \"Key2\": \"Value2\"}), (2, {\"Key3\": \"Value3\", \"Key4\": \"Value4\"})]\n",
        "\n",
        "df = spark.createDataFrame(data, schema)\n",
        "df.show(truncate=False)\n",
        "df.select(map_keys(col(\"attributes\")).alias(\"keys\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVfLEashrHXr",
        "outputId": "5d4aef1d-916c-4f66-e0b2-b20a00c8cd2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------------------+\n",
            "|id |attributes                      |\n",
            "+---+--------------------------------+\n",
            "|1  |{Key2 -> Value2, Key1 -> Value1}|\n",
            "|2  |{Key4 -> Value4, Key3 -> Value3}|\n",
            "+---+--------------------------------+\n",
            "\n",
            "+------------+\n",
            "|        keys|\n",
            "+------------+\n",
            "|[Key2, Key1]|\n",
            "|[Key4, Key3]|\n",
            "+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create map(create_map)"
      ],
      "metadata": {
        "id": "H22pyZ5k8nQl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "\n",
        "df.withColumn(\n",
        "    \"my_map\",\n",
        "    F.create_map(\n",
        "        F.lit(\"A\"), F.lit(1),\n",
        "        F.lit(\"B\"), F.lit(2)\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "YNJFx9Sl8lh-",
        "outputId": "73a7be25-9c73-43c8-9d4d-cb890d632e37",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[product: string, sales: double, my_map: map<string,int>]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge map(map_concat)\n"
      ],
      "metadata": {
        "id": "_TwI5YiCSflB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import map_concat\n",
        "\n",
        "# Sample Data\n",
        "data = [\n",
        "    (1, {\"a\": 1, \"b\": 2}, {\"c\": 3, \"d\": 4}),\n",
        "    (2, {\"x\": 10}, {\"y\": 20, \"z\": 30}),\n",
        "    (3, {}, {\"p\": 100, \"q\": 200}),\n",
        "    (4, {\"k\": 5}, {})\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"id\", \"map1\", \"map2\"])\n",
        "\n",
        "print(\"before the merge\")\n",
        "df.show(truncate=False)\n",
        "\n",
        "# Merge the two map columns\n",
        "df_merged = df.withColumn(\"merged_map\", map_concat(\"map1\", \"map2\"))\n",
        "\n",
        "print(\"after the merge\")\n",
        "df_merged.show(truncate=False)\n"
      ],
      "metadata": {
        "id": "1nFfMq4SShZh",
        "outputId": "002ff1b7-e4d7-44dd-e18f-3bf9215d0848",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before the merge\n",
            "+---+----------------+--------------------+\n",
            "|id |map1            |map2                |\n",
            "+---+----------------+--------------------+\n",
            "|1  |{a -> 1, b -> 2}|{d -> 4, c -> 3}    |\n",
            "|2  |{x -> 10}       |{y -> 20, z -> 30}  |\n",
            "|3  |{}              |{p -> 100, q -> 200}|\n",
            "|4  |{k -> 5}        |{}                  |\n",
            "+---+----------------+--------------------+\n",
            "\n",
            "after the merge\n",
            "+---+----------------+--------------------+--------------------------------+\n",
            "|id |map1            |map2                |merged_map                      |\n",
            "+---+----------------+--------------------+--------------------------------+\n",
            "|1  |{a -> 1, b -> 2}|{d -> 4, c -> 3}    |{a -> 1, b -> 2, d -> 4, c -> 3}|\n",
            "|2  |{x -> 10}       |{y -> 20, z -> 30}  |{x -> 10, y -> 20, z -> 30}     |\n",
            "|3  |{}              |{p -> 100, q -> 200}|{p -> 100, q -> 200}            |\n",
            "|4  |{k -> 5}        |{}                  |{k -> 5}                        |\n",
            "+---+----------------+--------------------+--------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, \\\n",
        "IntegerType\n",
        "from pyspark.sql.types import MapType\n",
        "\n",
        "# Define MapType\n",
        "schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"metadata\", MapType(StringType(), StringType()), True)\n",
        "])\n",
        "\n",
        "# Sample Data\n",
        "data = [(1, {\"Key1\": \"Value1\", \"Key2\": \"Value2\"}),\n",
        "        (2, {\"Key1\": \"Value3\", \"Key2\": \"Value4\"})]\n",
        "\n",
        "df = spark.createDataFrame(data, schema)\n",
        "df.show(truncate=False)\n",
        "\n",
        "df.select(df.metadata.getItem(\"Key1\").alias(\"value_of_key1\")).show()\n",
        "\n",
        "\n",
        "# Explode the metadata column while retaining the original metadata column\n",
        "df_exploded = df.selectExpr(\"id\", \"metadata\", \"explode(metadata) as \\\n",
        "(key, value)\")\n",
        "\n",
        "# Show the resulting DataFrame\n",
        "df_exploded.show(truncate=False)"
      ],
      "metadata": {
        "id": "X_Cguo-A-9ON",
        "outputId": "c104a9e5-6de0-464e-f17b-fd27b4b86144",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------------------+\n",
            "|id |metadata                        |\n",
            "+---+--------------------------------+\n",
            "|1  |{Key2 -> Value2, Key1 -> Value1}|\n",
            "|2  |{Key2 -> Value4, Key1 -> Value3}|\n",
            "+---+--------------------------------+\n",
            "\n",
            "+-------------+\n",
            "|value_of_key1|\n",
            "+-------------+\n",
            "|       Value1|\n",
            "|       Value3|\n",
            "+-------------+\n",
            "\n",
            "+---+--------------------------------+----+------+\n",
            "|id |metadata                        |key |value |\n",
            "+---+--------------------------------+----+------+\n",
            "|1  |{Key2 -> Value2, Key1 -> Value1}|Key2|Value2|\n",
            "|1  |{Key2 -> Value2, Key1 -> Value1}|Key1|Value1|\n",
            "|2  |{Key2 -> Value4, Key1 -> Value3}|Key2|Value4|\n",
            "|2  |{Key2 -> Value4, Key1 -> Value3}|Key1|Value3|\n",
            "+---+--------------------------------+----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Play with Struct\n"
      ],
      "metadata": {
        "id": "Ky55RcyOiHOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_struct = spark.sql(\"SELECT struct(1, 2, '3') as ex_struct\")\n",
        "df_struct.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACGJLu9-NjKw",
        "outputId": "266cc7c4-a284-47da-8a16-b17910862ffe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|ex_struct|\n",
            "+---------+\n",
            "|{1, 2, 3}|\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_struct.select(\"ex_struct.col3\", \"ex_struct\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B46JwCaePajw",
        "outputId": "82ce3755-9e13-404d-897c-a0c22551a2bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------+\n",
            "|col3|ex_struct|\n",
            "+----+---------+\n",
            "|   3|{1, 2, 3}|\n",
            "+----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_struct.select(\"ex_struct.*\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeMsLV5Qdbp9",
        "outputId": "4a03e2d9-f573-4e65-f05f-57e597d116d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+----+\n",
            "|col1|col2|col3|\n",
            "+----+----+----+\n",
            "|1   |2   |3   |\n",
            "+----+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_map = spark.sql(\"SELECT map(1.0, '2', 3.0, '4') as ex_map\")\n",
        "df_map.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-Dl2Y1DOSd3",
        "outputId": "b1195e4e-834a-40f3-e646-3f8e06500f20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|              ex_map|\n",
            "+--------------------+\n",
            "|{1.0 -> 2, 3.0 -> 4}|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType,StructField, StringType, ArrayType,MapType\n",
        "\n",
        "# https://sparkbyexamples.com/pyspark/pyspark-maptype-dict-examples/\n",
        "\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "\n",
        "# Data\n",
        "data = [\n",
        "        ((\"James\",None,\"Smith\"),\"OH\",\"M\"),\n",
        "        ((\"Anna\",\"Rose\",\"\"),\"NY\",\"F\"),\n",
        "        ((\"Julia\",\"\",\"Williams\"),\"OH\",\"F\"),\n",
        "        ((\"Maria\",\"Anne\",\"Jones\"),\"NY\",\"M\"),\n",
        "        ((\"Jen\",\"Mary\",\"Brown\"),\"NY\",\"M\"),\n",
        "        ((\"Mike\",\"Mary\",\"Williams\"),\"OH\",\"M\")\n",
        "        ]\n",
        "\n",
        "# Schema\n",
        "schema = StructType([\n",
        "    StructField('name', StructType([\n",
        "         StructField('firstname', StringType(), True),\n",
        "         StructField('middlename', StringType(), True),\n",
        "         StructField('lastname', StringType(), True)\n",
        "         ])),\n",
        "     StructField('state', StringType(), True),\n",
        "     StructField('gender', StringType(), True)\n",
        "     ])\n",
        "\n",
        "# Create DataFrame\n",
        "df2 = spark.createDataFrame(data = data, schema = schema)\n",
        "df2.printSchema()\n",
        "df2.show(truncate=False) # shows all columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jye_i4rTOfIk",
        "outputId": "bbe0fdec-6e59-4576-ce1f-863553ac0384"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: struct (nullable = true)\n",
            " |    |-- firstname: string (nullable = true)\n",
            " |    |-- middlename: string (nullable = true)\n",
            " |    |-- lastname: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            "\n",
            "+----------------------+-----+------+\n",
            "|name                  |state|gender|\n",
            "+----------------------+-----+------+\n",
            "|{James, NULL, Smith}  |OH   |M     |\n",
            "|{Anna, Rose, }        |NY   |F     |\n",
            "|{Julia, , Williams}   |OH   |F     |\n",
            "|{Maria, Anne, Jones}  |NY   |M     |\n",
            "|{Jen, Mary, Brown}    |NY   |M     |\n",
            "|{Mike, Mary, Williams}|OH   |M     |\n",
            "+----------------------+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2.select(\"name\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLx1sLW0ctrH",
        "outputId": "6fdd982d-97ec-4db7-c198-c55727ce70ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|                name|\n",
            "+--------------------+\n",
            "|{James, NULL, Smith}|\n",
            "|      {Anna, Rose, }|\n",
            "| {Julia, , Williams}|\n",
            "|{Maria, Anne, Jones}|\n",
            "|  {Jen, Mary, Brown}|\n",
            "|{Mike, Mary, Will...|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType,StructField, StringType, ArrayType,MapType\n",
        "from pyspark.sql.functions import element_at\n",
        "\n",
        "data = [\n",
        "        ([(\"Yash\",\"K\",None),(\"Yash2\",\"K2\",None)],\"BL\",\"M\",[\"KGF 1\", \"KGF 2\"],{\"Bank\":\"100 CR\", \"Business\":\"50 CR\", \"Land\":\"150 CR\"}),\n",
        "        ([(\"Sudeep\",\"Kicha\",\"S\")],\"DL\",\"M\",[\"Autograph\", \"Kicha\",\"Hucha\"],{\"Others\":\"300 CR\"}),\n",
        "        ([(\"Puneeth\",None,\"Raj\")],\"MB\",\"M\",[], {\"Building\":\"500 CR\", \"Commercal\":100}),\n",
        "        ([(\"Darshan\",None,None)],\"MB\",\"M\",[], {\"Building\":\"500 CR\", \"Commercal\":100}),\n",
        "\n",
        "        ]\n",
        "\n",
        "# Schema\n",
        "schema = StructType([\n",
        "    StructField(    'name'\n",
        "                  , ArrayType(\n",
        "                        StructType([\n",
        "                            StructField('firstname', StringType(), True),\n",
        "                            StructField('middlename', StringType(), True),\n",
        "                            StructField('lastname', StringType(), True)\n",
        "                        ])\n",
        "                    ), True),\n",
        "     StructField('state', StringType(), True),\n",
        "     StructField('gender', StringType(), True),\n",
        "     StructField('movies', ArrayType(StringType()), True),\n",
        "     StructField('properties', MapType(StringType(),StringType()), True)\n",
        "     ])\n",
        "\n",
        "# Create DataFrame\n",
        "df2 = spark.createDataFrame(data = data, schema = schema)\n",
        "df2.printSchema()\n",
        "df2.show(truncate=False) # shows all columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJwWwq4Xc-0Z",
        "outputId": "468e11f5-95f8-4a03-80a4-102964f61988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- firstname: string (nullable = true)\n",
            " |    |    |-- middlename: string (nullable = true)\n",
            " |    |    |-- lastname: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- movies: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- properties: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            "\n",
            "+------------------------------------+-----+------+-------------------------+---------------------------------------------------+\n",
            "|name                                |state|gender|movies                   |properties                                         |\n",
            "+------------------------------------+-----+------+-------------------------+---------------------------------------------------+\n",
            "|[{Yash, K, NULL}, {Yash2, K2, NULL}]|BL   |M     |[KGF 1, KGF 2]           |{Bank -> 100 CR, Land -> 150 CR, Business -> 50 CR}|\n",
            "|[{Sudeep, Kicha, S}]                |DL   |M     |[Autograph, Kicha, Hucha]|{Others -> 300 CR}                                 |\n",
            "|[{Puneeth, NULL, Raj}]              |MB   |M     |[]                       |{Building -> 500 CR, Commercal -> 100}             |\n",
            "|[{Darshan, NULL, NULL}]             |MB   |M     |[]                       |{Building -> 500 CR, Commercal -> 100}             |\n",
            "+------------------------------------+-----+------+-------------------------+---------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " (df2\n",
        "  .select(\"properties\", \"name\", \"name.firstname\")\n",
        "  .withColumn(\"M-Building\", col(\"properties\").getItem(\"Building\"))\n",
        "  .withColumn(\"M-Commercal \", col(\"properties\").getItem(\"Commercal\"))\n",
        "  .withColumn(\"M-Others \", col(\"properties\").getItem(\"Others\"))\n",
        "  .withColumn(\"S-FName\", col(\"name.firstname\"))\n",
        "  ).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0-Z3EjP0njz",
        "outputId": "a6e04d9a-3bb8-48f6-9dcd-8a7d9bd219a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+---------+----------+------------+---------+-------+\n",
            "|          properties|                name|firstname|M-Building|M-Commercal |M-Others |S-FName|\n",
            "+--------------------+--------------------+---------+----------+------------+---------+-------+\n",
            "|{Bank -> 100 CR, ...|     {Yash, K, NULL}|     Yash|      NULL|        NULL|     NULL|   Yash|\n",
            "|  {Others -> 300 CR}|  {Sudeep, Kicha, S}|   Sudeep|      NULL|        NULL|   300 CR| Sudeep|\n",
            "|{Building -> 500 ...|{Puneeth, NULL, Raj}|  Puneeth|    500 CR|         100|     NULL|Puneeth|\n",
            "|{Building -> 500 ...|{Darshan, NULL, N...|  Darshan|    500 CR|         100|     NULL|Darshan|\n",
            "+--------------------+--------------------+---------+----------+------------+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType,StructField, StringType, ArrayType,MapType\n",
        "from pyspark.sql.functions import element_at\n",
        "\n",
        "data = [\n",
        "        ((\"Yash\",\"K\",None),\"BL\",\"M\",[\"KGF 1\", \"KGF 2\"],{\"Bank\":\"100 CR\", \"Business\":\"50 CR\", \"Land\":\"150 CR\"}),\n",
        "        ((\"Sudeep\",\"Kicha\",\"S\"),\"DL\",\"M\",[\"Autograph\", \"Kicha\",\"Hucha\"],{\"Others\":\"300 CR\"}),\n",
        "        ((\"Puneeth\",None,\"Raj\"),\"MB\",\"M\",[], {\"Building\":\"500 CR\", \"Commercal\":100}),\n",
        "        ((\"Darshan\",None,None),\"MB\",\"M\",[], {\"Building\":\"500 CR\", \"Commercal\":100}),\n",
        "\n",
        "        ]\n",
        "\n",
        "# Schema\n",
        "schema = StructType([\n",
        "    StructField('name', StructType([\n",
        "         StructField('firstname', StringType(), True),\n",
        "         StructField('middlename', StringType(), True),\n",
        "         StructField('lastname', StringType(), True)\n",
        "         ])),\n",
        "     StructField('state', StringType(), True),\n",
        "     StructField('gender', StringType(), True),\n",
        "     StructField('movies', ArrayType(StringType()), True),\n",
        "     StructField('properties', MapType(StringType(),StringType()), True)\n",
        "     ])\n",
        "\n",
        "# Create DataFrame\n",
        "df2 = spark.createDataFrame(data = data, schema = schema)\n",
        "df2.printSchema()\n",
        "df2.show(truncate=False) # shows all columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QiMme632DET",
        "outputId": "22e1ec1c-594b-473b-b950-2f87e1b7e29c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: struct (nullable = true)\n",
            " |    |-- firstname: string (nullable = true)\n",
            " |    |-- middlename: string (nullable = true)\n",
            " |    |-- lastname: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- movies: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- properties: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            "\n",
            "+---------------------+-----+------+-------------------------+---------------------------------------------------+\n",
            "|name                 |state|gender|movies                   |properties                                         |\n",
            "+---------------------+-----+------+-------------------------+---------------------------------------------------+\n",
            "|{Yash, K, NULL}      |BL   |M     |[KGF 1, KGF 2]           |{Bank -> 100 CR, Land -> 150 CR, Business -> 50 CR}|\n",
            "|{Sudeep, Kicha, S}   |DL   |M     |[Autograph, Kicha, Hucha]|{Others -> 300 CR}                                 |\n",
            "|{Puneeth, NULL, Raj} |MB   |M     |[]                       |{Building -> 500 CR, Commercal -> 100}             |\n",
            "|{Darshan, NULL, NULL}|MB   |M     |[]                       |{Building -> 500 CR, Commercal -> 100}             |\n",
            "+---------------------+-----+------+-------------------------+---------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
        "\n",
        "myManualSchema = StructType([\n",
        "  StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
        "  StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
        "  StructField(\"count\", LongType(), False, metadata={\"hello\":\"world\"})\n",
        "])\n",
        "df = spark.read.format(\"json\").schema(myManualSchema)\\\n",
        "  .load(\"sample_data/2015-summary.json\")\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "id": "21WEGU2v8_tA",
        "outputId": "50a1e446-52a5-4bf8-c206-028cf5b7f366",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/content/sample_data/2015-summary.json.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-50b2766d5f6a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m ])\n\u001b[1;32m      8\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyManualSchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sample_data/2015-summary.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/content/sample_data/2015-summary.json."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "id": "mcGnKSvKbQSR",
        "outputId": "42ba3405-038e-4bcd-fb0b-92de2fa18373",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
            " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
            " |-- count: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.read.format(\"json\").load(\"sample_data/2015-summary.json\").schema"
      ],
      "metadata": {
        "id": "vC1GYIlfbT9m",
        "outputId": "0e7f09ed-fa95-454d-f2dc-168a6a8eaac2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructType([StructField('DEST_COUNTRY_NAME', StringType(), True), StructField('ORIGIN_COUNTRY_NAME', StringType(), True), StructField('count', LongType(), True)])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge struct(no built in method)\n"
      ],
      "metadata": {
        "id": "Dnh1YCKdN4_K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "data = [\n",
        "    Row(id=1, s1=Row(a=10, b=20), s2=Row(c=30, d=40)),\n",
        "    Row(id=2, s1=Row(a=100, b=200), s2=Row(c=300, d=400))\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data)\n",
        "\n",
        "df.show(truncate=False)\n",
        "df.printSchema()\n",
        "\n",
        "\n",
        "from pyspark.sql.functions import col, struct\n",
        "\n",
        "merged_df = df.withColumn(\n",
        "    \"merged_struct\",\n",
        "    struct(\n",
        "        col(\"s1.a\").alias(\"a\"),\n",
        "        col(\"s1.b\").alias(\"b\"),\n",
        "        col(\"s2.c\").alias(\"c\"),\n",
        "        col(\"s2.d\").alias(\"d\")\n",
        "    )\n",
        ")\n",
        "\n",
        "merged_df.select(\"id\", \"merged_struct\").show(truncate=False)\n",
        "merged_df.printSchema()\n"
      ],
      "metadata": {
        "id": "T49WlOf2N4zK",
        "outputId": "9dff8c08-6ea5-42eb-c25a-44b2f6e0768c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------+----------+\n",
            "|id |s1        |s2        |\n",
            "+---+----------+----------+\n",
            "|1  |{10, 20}  |{30, 40}  |\n",
            "|2  |{100, 200}|{300, 400}|\n",
            "+---+----------+----------+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- s1: struct (nullable = true)\n",
            " |    |-- a: long (nullable = true)\n",
            " |    |-- b: long (nullable = true)\n",
            " |-- s2: struct (nullable = true)\n",
            " |    |-- c: long (nullable = true)\n",
            " |    |-- d: long (nullable = true)\n",
            "\n",
            "+---+--------------------+\n",
            "|id |merged_struct       |\n",
            "+---+--------------------+\n",
            "|1  |{10, 20, 30, 40}    |\n",
            "|2  |{100, 200, 300, 400}|\n",
            "+---+--------------------+\n",
            "\n",
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- s1: struct (nullable = true)\n",
            " |    |-- a: long (nullable = true)\n",
            " |    |-- b: long (nullable = true)\n",
            " |-- s2: struct (nullable = true)\n",
            " |    |-- c: long (nullable = true)\n",
            " |    |-- d: long (nullable = true)\n",
            " |-- merged_struct: struct (nullable = false)\n",
            " |    |-- a: long (nullable = true)\n",
            " |    |-- b: long (nullable = true)\n",
            " |    |-- c: long (nullable = true)\n",
            " |    |-- d: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, struct\n",
        "\n",
        "# Sample DataFrame with struct column \"address\" (city, zip)\n",
        "data = [\n",
        "    (1, (\"New York\", \"10001\")),\n",
        "    (2, (\"San Francisco\", \"94105\")),\n",
        "    (3, (\"Chicago\", \"60601\"))\n",
        "]\n",
        "\n",
        "# Define schema explicitly\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"id\", IntegerType(), True),\n",
        "    StructField(\"address\", StructType([\n",
        "        StructField(\"city\", StringType(), True),\n",
        "        StructField(\"zip\", StringType(), True)\n",
        "    ]), True)\n",
        "])\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, schema)\n",
        "print(\"Original Schema:\")\n",
        "df.printSchema()\n",
        "\n",
        "# Rename the \"zip\" field to \"zipcode\"\n",
        "df_transformed = df.withColumn(\"address\", struct(\n",
        "    col(\"address.city\"),\n",
        "    col(\"address.zip\").alias(\"zipcode\")  # Renaming zip to zipcode\n",
        "))\n",
        "print(\"Updated Schema:\")\n",
        "df_transformed.printSchema()"
      ],
      "metadata": {
        "id": "0SYf8Q5I0qnM",
        "outputId": "b7e81ea3-088b-40a9-80c4-02655e5c61ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Schema:\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- address: struct (nullable = true)\n",
            " |    |-- city: string (nullable = true)\n",
            " |    |-- zip: string (nullable = true)\n",
            "\n",
            "Updated Schema:\n",
            "root\n",
            " |-- id: integer (nullable = true)\n",
            " |-- address: struct (nullable = false)\n",
            " |    |-- city: string (nullable = true)\n",
            " |    |-- zipcode: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Other Examples\n",
        "\n"
      ],
      "metadata": {
        "id": "Q2lTqwHdg9V4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using expressions"
      ],
      "metadata": {
        "id": "IJPbdlX1cAQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import col, expr\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "  Row(name=\"san\", salary=1500),\n",
        "  Row(name=\"ana\", salary=2000),\n",
        "  Row(name=\"shu\", salary=1000)\n",
        "])\n",
        "\n",
        "print(\"approach 1:\")\n",
        "df.withColumn(\"bonus\", df.salary * 0.1).show()\n",
        "\n",
        "print(\"approach 2:\")\n",
        "df.withColumn(\"bonus\", expr(\"salary * 0.1\")).show()\n",
        "\n",
        "print(\"approach 3:\")\n",
        "df.select(col(\"name\"), col(\"salary\"), (col(\"salary\") * 0.1).alias(\"bonus\")).show()"
      ],
      "metadata": {
        "id": "2DZCMcH5DkWZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c01f4e9-a34c-4dea-fe21-6fc6abe01c32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "approach 1:\n",
            "+----+------+-----+\n",
            "|name|salary|bonus|\n",
            "+----+------+-----+\n",
            "| san|  1500|150.0|\n",
            "| ana|  2000|200.0|\n",
            "| shu|  1000|100.0|\n",
            "+----+------+-----+\n",
            "\n",
            "approach 2:\n",
            "+----+------+-----+\n",
            "|name|salary|bonus|\n",
            "+----+------+-----+\n",
            "| san|  1500|150.0|\n",
            "| ana|  2000|200.0|\n",
            "| shu|  1000|100.0|\n",
            "+----+------+-----+\n",
            "\n",
            "approach 3:\n",
            "+----+------+-----+\n",
            "|name|salary|bonus|\n",
            "+----+------+-----+\n",
            "| san|  1500|150.0|\n",
            "| ana|  2000|200.0|\n",
            "| shu|  1000|100.0|\n",
            "+----+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Nested fields fetching(struct)"
      ],
      "metadata": {
        "id": "LdZGUXj5T3py"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = [\n",
        "(1, {\"city\": \"New York\", \"state\": \"NY\"}),\n",
        "(2, {\"city\": \"San Francisco\", \"state\": \"CA\"})\n",
        "]\n",
        "\n",
        "schema = [\"id\", \"address\"]\n",
        "\n",
        "df = spark.createDataFrame(data, schema=schema)\n",
        "\n",
        "print(\"approach 1\")\n",
        "df.select(col(\"address.city\")).show() #correct syntax\n",
        "\n",
        "print(\"approach 2\")\n",
        "df.select(\"address.city\").show() #correct syntax\n",
        "\n",
        "print(\"approach 3\")\n",
        "df.select(col(\"address\")[\"city\"]).show() #correct syntax\n",
        "\n",
        "print(\"approach 4\")\n",
        "(df.select(\"address\")\n",
        "   .select(col(\"address.city\")).show()) #correct syntax"
      ],
      "metadata": {
        "id": "WhEcKtt3bvJp",
        "outputId": "79dc4b1d-c712-4bc3-d636-15be2dc7ab50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "approach 1\n",
            "+-------------+\n",
            "|         city|\n",
            "+-------------+\n",
            "|     New York|\n",
            "|San Francisco|\n",
            "+-------------+\n",
            "\n",
            "approach 2\n",
            "+-------------+\n",
            "|         city|\n",
            "+-------------+\n",
            "|     New York|\n",
            "|San Francisco|\n",
            "+-------------+\n",
            "\n",
            "approach 3\n",
            "+-------------+\n",
            "|address[city]|\n",
            "+-------------+\n",
            "|     New York|\n",
            "|San Francisco|\n",
            "+-------------+\n",
            "\n",
            "approach 4\n",
            "+-------------+\n",
            "|         city|\n",
            "+-------------+\n",
            "|     New York|\n",
            "|San Francisco|\n",
            "+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SELECT mappings"
      ],
      "metadata": {
        "id": "RPrQSmMFUZSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import Row\n",
        "from pyspark.sql.types import StructType, StructField, LongType, StringType\n",
        "\n",
        "# Define Schema\n",
        "schema = StructType([\n",
        "    StructField(\"id\", LongType(), True),\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"age\", LongType(), True),\n",
        "    StructField(\"salary\", LongType(), True)\n",
        "])\n",
        "\n",
        "# Create Dummy Data\n",
        "data = [\n",
        "    Row(id=1, name=\"San\", age=30, salary=70000),\n",
        "    Row(id=2, name=\"Ana\", age=35, salary=80000),\n",
        "    Row(id=3, name=\"Shuchi\", age=25, salary=50000),\n",
        "    Row(id=4, name=\"Krishna\", age=40, salary=90000)\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, schema)\n",
        "df.show()"
      ],
      "metadata": {
        "id": "hTHdYCwQT8rs",
        "outputId": "e42185e4-90c0-4444-885e-f7a261f0f9f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+---+------+\n",
            "| id|   name|age|salary|\n",
            "+---+-------+---+------+\n",
            "|  1|    San| 30| 70000|\n",
            "|  2|    Ana| 35| 80000|\n",
            "|  3| Shuchi| 25| 50000|\n",
            "|  4|Krishna| 40| 90000|\n",
            "+---+-------+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import expr\n",
        "\n",
        "# Select 'name' and 'age', and create 'age_category' based on 'age'\n",
        "df_expr = df.select(\n",
        "    \"name\",\n",
        "    \"age\",\n",
        "    expr(\"CASE WHEN age >= 35 THEN 'Senior' \\\n",
        "ELSE 'Junior' END as age_category\")\n",
        ")\n",
        "df_expr.show()\n",
        "\n",
        "\n",
        "from pyspark.sql.functions import expr\n",
        "\n",
        "# Select 'name' and 'age', and create 'age_category' based on 'age'\n",
        "df_select_expr = df.selectExpr(\n",
        "    \"name\",\n",
        "    \"age\",\n",
        "    \"CASE WHEN age >= 35 THEN 'Senior' ELSE 'Junior' END as age_category\"\n",
        ")\n",
        "df_select_expr.show()"
      ],
      "metadata": {
        "id": "ZuXs2kcuUfFR",
        "outputId": "392b7e37-f147-41be-ea40-d2c1797d64bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+------------+\n",
            "|   name|age|age_category|\n",
            "+-------+---+------------+\n",
            "|    San| 30|      Junior|\n",
            "|    Ana| 35|      Senior|\n",
            "| Shuchi| 25|      Junior|\n",
            "|Krishna| 40|      Senior|\n",
            "+-------+---+------------+\n",
            "\n",
            "+-------+---+------------+\n",
            "|   name|age|age_category|\n",
            "+-------+---+------------+\n",
            "|    San| 30|      Junior|\n",
            "|    Ana| 35|      Senior|\n",
            "| Shuchi| 25|      Junior|\n",
            "|Krishna| 40|      Senior|\n",
            "+-------+---+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Add a new column 'age_plus_5'\n",
        "df_with_age = df.withColumn(\"age_plus_5\", col(\"age\") + 5)\n",
        "df_with_age.show()"
      ],
      "metadata": {
        "id": "JqWBxOnrUjlW",
        "outputId": "d2ca1302-ca79-4131-a629-c95891e2bc19",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+---+------+----------+\n",
            "| id|   name|age|salary|age_plus_5|\n",
            "+---+-------+---+------+----------+\n",
            "|  1|    San| 30| 70000|        35|\n",
            "|  2|    Ana| 35| 80000|        40|\n",
            "|  3| Shuchi| 25| 50000|        30|\n",
            "|  4|Krishna| 40| 90000|        45|\n",
            "+---+-------+---+------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Cast 'salary' from LongType to DoubleType\n",
        "from pyspark.sql.types import DoubleType\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "df_casted = df.withColumn(\"salary_double\", col(\"salary\").cast(DoubleType()))\n",
        "df_casted.show()"
      ],
      "metadata": {
        "id": "Luebgve5Uq2R",
        "outputId": "53837180-090b-4c8d-ae63-47923c1a2130",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+---+------+-------------+\n",
            "| id|   name|age|salary|salary_double|\n",
            "+---+-------+---+------+-------------+\n",
            "|  1|    San| 30| 70000|      70000.0|\n",
            "|  2|    Ana| 35| 80000|      80000.0|\n",
            "|  3| Shuchi| 25| 50000|      50000.0|\n",
            "|  4|Krishna| 40| 90000|      90000.0|\n",
            "+---+-------+---+------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## UDF"
      ],
      "metadata": {
        "id": "GvUJ1n6_XpuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql.functions import pandas_udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "@pandas_udf(IntegerType())\n",
        "def string_length(s: pd.Series) -> pd.Series:\n",
        "    return s.str.len()"
      ],
      "metadata": {
        "id": "7MzeRaxxUuaK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import lit\n",
        "\n",
        "df = spark.createDataFrame([(1, \"Ram\"), (2, \"Santosh\")], [\"id\", \"name\"])\n",
        "\n",
        "df = df.withColumns({\"age1\": lit(10), \"age2\": lit(20)})\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "id": "r4VQR80ZXraB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49e5a830-1216-4c8f-eee2-97a5656407bd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+----+----+\n",
            "| id|   name|age1|age2|\n",
            "+---+-------+----+----+\n",
            "|  1|    Ram|  10|  20|\n",
            "|  2|Santosh|  10|  20|\n",
            "+---+-------+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "from pyspark.sql import Row\n",
        "\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    Row(id=1, name=\"santosh\", age=30),\n",
        "    Row(id=1, name=\"santosh\", age=30),\n",
        "    Row(id=2, name=\"ram\", age=7),\n",
        "    Row(id=3, name=\"abdul\", age=5),\n",
        "    Row(id=4, name=\"john\", age=5),\n",
        "    Row(id=5, name=\"john\", age=5),\n",
        "    Row(id=6, name=\"john\", age=5),\n",
        "    Row(id=7, name=\"john\", age=5)\n",
        "])\n",
        "\n",
        "df.distinct().show()\n",
        "\n",
        "df.dropDuplicates().show()\n",
        "\n",
        "df.sort(col(\"id\").desc()).dropDuplicates(subset=[\"name\", \"age\"]).show()\n",
        "\n",
        "df.sort(col(\"id\").desc()).dropDuplicates(subset=[\"all\"]).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        },
        "id": "9rTOxSwGY4uY",
        "outputId": "e4c280ff-555c-45d1-ec53-1f3e7cf2b01b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-------+---+\n",
            "| id|   name|age|\n",
            "+---+-------+---+\n",
            "|  2|    ram|  7|\n",
            "|  3|  abdul|  5|\n",
            "|  1|santosh| 30|\n",
            "|  5|   john|  5|\n",
            "|  4|   john|  5|\n",
            "|  7|   john|  5|\n",
            "|  6|   john|  5|\n",
            "+---+-------+---+\n",
            "\n",
            "+---+-------+---+\n",
            "| id|   name|age|\n",
            "+---+-------+---+\n",
            "|  2|    ram|  7|\n",
            "|  3|  abdul|  5|\n",
            "|  1|santosh| 30|\n",
            "|  5|   john|  5|\n",
            "|  4|   john|  5|\n",
            "|  7|   john|  5|\n",
            "|  6|   john|  5|\n",
            "+---+-------+---+\n",
            "\n",
            "+---+-------+---+\n",
            "| id|   name|age|\n",
            "+---+-------+---+\n",
            "|  2|    ram|  7|\n",
            "|  1|santosh| 30|\n",
            "|  7|   john|  5|\n",
            "|  3|  abdul|  5|\n",
            "+---+-------+---+\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "Cannot resolve column name \"all\" among (id, name, age).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-7-718395629.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropDuplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"age\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropDuplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"all\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mdropDuplicates\u001b[0;34m(self, subset)\u001b[0m\n\u001b[1;32m   4217\u001b[0m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropDuplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4218\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4219\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropDuplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4220\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: Cannot resolve column name \"all\" among (id, name, age)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "\n",
        "# Define the UDF using a lambda function\n",
        "split_comma_udf = udf(lambda x: x.split(\",\"), ArrayType(StringType()))\n",
        "\n",
        "# Create some sample data\n",
        "data = [(\"san,ana,roger,ram\",), (\"apple,nvidia,tesla\",)]\n",
        "df = spark.createDataFrame(data, [\"names\"])\n",
        "\n",
        "# Apply the UDF to the DataFrame\n",
        "df_with_split_names = df.withColumn(\"split_names\", split_comma_udf(df[\"names\"]))\n",
        "\n",
        "# Show the results\n",
        "df_with_split_names.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DI_HHj6ZPiu",
        "outputId": "7d4205f4-bf14-49e7-b336-5d495290e636"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------------------+----------------------+\n",
            "|names             |split_names           |\n",
            "+------------------+----------------------+\n",
            "|san,ana,roger,ram |[san, ana, roger, ram]|\n",
            "|apple,nvidia,tesla|[apple, nvidia, tesla]|\n",
            "+------------------+----------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    (1, None, \"B\"),\n",
        "    (2, \"A\", \"A\"),\n",
        "    (3, \"C\", \"D\"),\n",
        "]\n",
        "\n",
        "# Creating DataFrame\n",
        "df = spark.createDataFrame(data, [\"ID\", \"col1\", \"col2\"])\n",
        "\n",
        "# Applying NULL-handling functions\n",
        "df.select(\n",
        "    \"ID\",\n",
        "    \"col1\",\n",
        "    \"col2\",\n",
        "    F.expr(\"ifnull(col1, 'default')\").alias(\"IFNULL\"),\n",
        "    F.expr(\"nvl(col1, 'default')\").alias(\"NVL\"),\n",
        "    F.expr(\"nullif(col1, col2)\").alias(\"NULLIF\"),\n",
        "    F.expr(\"nvl2(col1, 'Not Null', 'Null')\").alias(\"NVL2\")\n",
        ").show()"
      ],
      "metadata": {
        "id": "SB6_1jFar4lu",
        "outputId": "74ab891e-c275-40e3-99da-401f0c3a9159",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----+----+-------+-------+------+--------+\n",
            "| ID|col1|col2| IFNULL|    NVL|NULLIF|    NVL2|\n",
            "+---+----+----+-------+-------+------+--------+\n",
            "|  1|NULL|   B|default|default|  NULL|    Null|\n",
            "|  2|   A|   A|      A|      A|  NULL|Not Null|\n",
            "|  3|   C|   D|      C|      C|     C|Not Null|\n",
            "+---+----+----+-------+-------+------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Sample data for stores DataFrame\n",
        "stores_data = [\n",
        "    (1, \"Store A\", \"NY\"),\n",
        "    (2, \"Store B\", \"CA\"),\n",
        "    (3, \"Store C\", \"NY\"),\n",
        "    (4, \"Store D\", \"TX\")\n",
        "]\n",
        "# Sample data for sales DataFrame\n",
        "sales_data = [\n",
        "    (1, 1500, \"2024-01-01\"),\n",
        "    (2, 800, \"2024-01-02\"),\n",
        "    (3, 1200, \"2024-01-03\"),\n",
        "    (4, 500, \"2024-01-04\"),\n",
        "    (1, 950, \"2024-01-05\"),\n",
        "    (3, 1100, \"2024-01-06\")\n",
        "]\n",
        "# Define schema\n",
        "stores_columns = [\"store_id\", \"store_name\", \"location\"]\n",
        "sales_columns = [\"store_id\", \"sales_amount\", \"sales_date\"]\n",
        "\n",
        "# Create DataFrames\n",
        "stores = spark.createDataFrame(stores_data, stores_columns)\n",
        "stores.show()\n",
        "sales = spark.createDataFrame(sales_data, sales_columns)\n",
        "sales.show()\n",
        "\n",
        "# Perform the join and filter conditions\n",
        "# stores.join(sales, \"store_id\").filter((stores.location == \"NY\") & (sales.sales_amount > 1000)).show()\n",
        "# stores.join(sales, stores.store_id == sales.store_id).filter((stores.location == \"NY\") & (sales.sales_amount > 1000)).show()\n",
        "stores.join(sales, \"store_id\").filter((stores.location == \"NY\") & (sales.sales_amount > 1000)).show()"
      ],
      "metadata": {
        "id": "eRazvRpWstLH",
        "outputId": "6a81673e-7f43-4a95-da70-cff388b7032b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------+--------+\n",
            "|store_id|store_name|location|\n",
            "+--------+----------+--------+\n",
            "|       1|   Store A|      NY|\n",
            "|       2|   Store B|      CA|\n",
            "|       3|   Store C|      NY|\n",
            "|       4|   Store D|      TX|\n",
            "+--------+----------+--------+\n",
            "\n",
            "+--------+------------+----------+\n",
            "|store_id|sales_amount|sales_date|\n",
            "+--------+------------+----------+\n",
            "|       1|        1500|2024-01-01|\n",
            "|       2|         800|2024-01-02|\n",
            "|       3|        1200|2024-01-03|\n",
            "|       4|         500|2024-01-04|\n",
            "|       1|         950|2024-01-05|\n",
            "|       3|        1100|2024-01-06|\n",
            "+--------+------------+----------+\n",
            "\n",
            "+--------+----------+--------+------------+----------+\n",
            "|store_id|store_name|location|sales_amount|sales_date|\n",
            "+--------+----------+--------+------------+----------+\n",
            "|       1|   Store A|      NY|        1500|2024-01-01|\n",
            "|       3|   Store C|      NY|        1200|2024-01-03|\n",
            "|       3|   Store C|      NY|        1100|2024-01-06|\n",
            "+--------+----------+--------+------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Data\n",
        "data = [\n",
        "    (1, \"A\", 50),\n",
        "    (2, \"B\", 120),\n",
        "    (3, \"C\", 200),\n",
        "    (4, \"D\", 90),\n",
        "    (5, \"E\", 150),\n",
        "    (6, \"F\", 300),\n",
        "    (7, \"G\", 400),\n",
        "    (8, \"H\", 75),\n",
        "    (9, \"I\", 110),\n",
        "    (10, \"J\", 500)\n",
        "]\n",
        "\n",
        "# Define schema\n",
        "columns = [\"id\", \"category\", \"value\"]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Split DataFrame into training (70%) and testing (30%)\n",
        "train_df, test_df = df.randomSplit([0.7, 0.3], seed=42)\n",
        "\n",
        "# Show results\n",
        "print(\"Training Data:\")\n",
        "train_df.show()\n",
        "\n",
        "print(\"Testing Data:\")\n",
        "test_df.show()"
      ],
      "metadata": {
        "id": "5X2c5skct-P0",
        "outputId": "a3b1ba9e-693d-4795-989e-8d751540a9b9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data:\n",
            "+---+--------+-----+\n",
            "| id|category|value|\n",
            "+---+--------+-----+\n",
            "|  1|       A|   50|\n",
            "|  2|       B|  120|\n",
            "|  4|       D|   90|\n",
            "|  5|       E|  150|\n",
            "|  7|       G|  400|\n",
            "|  8|       H|   75|\n",
            "|  9|       I|  110|\n",
            "| 10|       J|  500|\n",
            "+---+--------+-----+\n",
            "\n",
            "Testing Data:\n",
            "+---+--------+-----+\n",
            "| id|category|value|\n",
            "+---+--------+-----+\n",
            "|  3|       C|  200|\n",
            "|  6|       F|  300|\n",
            "+---+--------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import col, avg, when\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "  Row(name=\"san\", age = 30),\n",
        "  Row(name=\"bob\", age = 20),\n",
        "  Row(name=\"alice\", age = 10),\n",
        "  Row(name=\"charlie\", age = None)\n",
        "])\n",
        "\n",
        "df = df.na.fill({\"age\": df.agg({\"age\": \"avg\"}).collect()[0][0]})\n",
        "\n",
        "df.show()\n"
      ],
      "metadata": {
        "id": "03XKm4wAu4Oa",
        "outputId": "c0673a50-223c-4874-d0e6-dc1fc899cbd4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+---+\n",
            "|   name|age|\n",
            "+-------+---+\n",
            "|    san| 30|\n",
            "|    bob| 20|\n",
            "|  alice| 10|\n",
            "|charlie| 20|\n",
            "+-------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Create Left DataFrame\n",
        "data1 = [(1, \"Alice\"), (2, \"Bob\"), (3, \"Charlie\"), (4, \"David\")]\n",
        "df1 = spark.createDataFrame(data1, [\"id\", \"name\"])\n",
        "\n",
        "# Create Right DataFrame\n",
        "data2 = [(1, \"Alice\"), (3, \"Charlie\")]\n",
        "df2 = spark.createDataFrame(data2, [\"id\", \"name\"])\n",
        "\n",
        "print(\"data type of the id column:\")\n",
        "print(df2.schema[\"id\"].dataType)\n",
        "\n",
        "print(\"\\ndata type of the name column:\")\n",
        "print(df2.schema[\"name\"].dataType)"
      ],
      "metadata": {
        "id": "O-6p5NiF47uL",
        "outputId": "909c4103-99be-4610-9ca8-a0cd7e13a2ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data type of the id column:\n",
            "LongType()\n",
            "\n",
            "data type of the name column:\n",
            "StringType()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Partition"
      ],
      "metadata": {
        "id": "X9VuhhJh0sL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Create a DataFrame with 10 million records\n",
        "df = spark.range(10_000_000).select(\n",
        "       F.concat(F.lit(\"Product_\"), F.floor(F.rand() * 4)\\\n",
        ".cast(\"int\")).alias(\"product\"),\n",
        "       F.floor(F.rand() * 1000).cast(\"double\").alias(\"sales\")\n",
        ")"
      ],
      "metadata": {
        "id": "BelyeBuk0uCW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import spark_partition_id\n",
        "\n",
        "# Assume 'df' is your DataFrame\n",
        "num_partitions = df.select(spark_partition_id().alias(\"partition_id\")) \\\n",
        "                   .distinct() \\\n",
        "                   .count()\n",
        "\n",
        "print(f\"Number of partitions: {num_partitions}\")"
      ],
      "metadata": {
        "id": "D2w2Sd0t0yjC",
        "outputId": "b325274c-84a4-4fb3-a891-e241ee52af29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of partitions: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions  import spark_partition_id\n",
        "df.withColumn(\"partitionId\", spark_partition_id()).\\\n",
        "groupBy(\"partitionId\").count().show()"
      ],
      "metadata": {
        "id": "wmrsLdJo03Ih",
        "outputId": "55716160-88c7-45dd-f06c-efad2b23d2b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------+\n",
            "|partitionId|  count|\n",
            "+-----------+-------+\n",
            "|          0|5000000|\n",
            "|          1|5000000|\n",
            "+-----------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_re_partition = df.repartition(4)"
      ],
      "metadata": {
        "id": "JwtV5WNv08HS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions  import spark_partition_id\n",
        "df_re_partition.withColumn(\"partitionId\", spark_partition_id()).\\\n",
        "groupBy(\"partitionId\").count().show()"
      ],
      "metadata": {
        "id": "nCxNFeVx0-xB",
        "outputId": "ef3429f6-2365-431a-e746-a1c66904d54e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------+\n",
            "|partitionId|  count|\n",
            "+-----------+-------+\n",
            "|          0|2500000|\n",
            "|          1|2500000|\n",
            "|          2|2500000|\n",
            "|          3|2500000|\n",
            "+-----------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_coalesce = df.coalesce(1)"
      ],
      "metadata": {
        "id": "rFhsIY2s1EM3"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions  import spark_partition_id\n",
        "df_coalesce.withColumn(\"partitionId\", spark_partition_id()).\\\n",
        "groupBy(\"partitionId\").count().show()"
      ],
      "metadata": {
        "id": "DDGTfkOr1NK_",
        "outputId": "7d9959e2-3709-450d-baa2-5e420c518ea2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------+\n",
            "|partitionId|   count|\n",
            "+-----------+--------+\n",
            "|          0|10000000|\n",
            "+-----------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dbvIKEKK1QQu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}