{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO2j//6TuTaoBE7C6dk5pYc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SanjayJanardhan-89/ApacheSparkHandsOn/blob/main/ComplexDatatypes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WuOZ0C0lhCL4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Pyspark\n"
      ],
      "metadata": {
        "id": "vRYFGd5ehHN5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 777
        },
        "id": "QLc7s-Qo1irp",
        "outputId": "e608fcd3-7ca8-4423-eef1-3b4896c62456"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [1,804 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,065 kB]\n",
            "Get:11 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,566 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [56.4 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,401 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,749 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,089 kB]\n",
            "Get:18 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [51.0 kB]\n",
            "Get:19 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,262 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [48.5 kB]\n",
            "Fetched 23.5 MB in 3s (7,918 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "35 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (0.10.9.7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7d8eaf9aaed0>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://33f676134653:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>OurSparkApp</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"OurSparkApp\") \\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Play with Arrays\n"
      ],
      "metadata": {
        "id": "cAbqNvQRhMI2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Length of the array"
      ],
      "metadata": {
        "id": "Yr8cte2KYfUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import size\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "data = [\n",
        "    (1, [1, 2, 3]),\n",
        "    (2, [4, 5]),\n",
        "    (3, [])\n",
        "]\n",
        "df = spark.createDataFrame(data, [\"id\", \"numbers\"])\n",
        "\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "id": "VrOTfPUeYe29",
        "outputId": "b4ee2652-29c3-4f3f-ec10-c1b4c789d01b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------+\n",
            "|id |numbers  |\n",
            "+---+---------+\n",
            "|1  |[1, 2, 3]|\n",
            "|2  |[4, 5]   |\n",
            "|3  |[]       |\n",
            "+---+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import size, col\n",
        "\n",
        "df_with_size = df.withColumn(\"numbers_size\", size(\"numbers\"))\n",
        "df_with_size = df_with_size.withColumn(\"numbers_size_2\", size(col(\"numbers\")))\n",
        "\n",
        "df_with_size.show()"
      ],
      "metadata": {
        "id": "fJf-7FI1Ykr5",
        "outputId": "94f99398-98f4-4c64-fa25-95ac352fbacc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------+------------+--------------+\n",
            "| id|  numbers|numbers_size|numbers_size_2|\n",
            "+---+---------+------------+--------------+\n",
            "|  1|[1, 2, 3]|           3|             3|\n",
            "|  2|   [4, 5]|           2|             2|\n",
            "|  3|       []|           0|             0|\n",
            "+---+---------+------------+--------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create array using Spark SQL"
      ],
      "metadata": {
        "id": "YpGEFG0bYy4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sql_array = spark.sql(\"SELECT array('KGF 1', 'KGF 2', 'Autograph', 'Kicha','Hucha') as movies\")\n",
        "df_sql_array.printSchema()\n",
        "df_sql_array.show(truncate=False)"
      ],
      "metadata": {
        "id": "NDhHyKMo5Q4K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43e9624a-362a-4269-aa40-353740809032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- movies: array (nullable = false)\n",
            " |    |-- element: string (containsNull = false)\n",
            "\n",
            "+---------------------------------------+\n",
            "|movies                                 |\n",
            "+---------------------------------------+\n",
            "|[KGF 1, KGF 2, Autograph, Kicha, Hucha]|\n",
            "+---------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType,StructField, StringType, ArrayType,MapType\n",
        "\n",
        "data = [\n",
        "          [\n",
        "            [\"KGF 1\", \"KGF 2\", \"Autograph\", \"Kicha\",\"Hucha\"],\n",
        "            # [\"Hello\", \"Hi\"]\n",
        "          ]\n",
        "        ]\n",
        "\n",
        "# Schema\n",
        "schema = StructType([\n",
        "             StructField('movies', ArrayType(StringType()), True),\n",
        "     ])\n",
        "\n",
        "# Create DataFrame\n",
        "df_array = spark.createDataFrame(data = data, schema = schema)\n",
        "df_array.printSchema()\n",
        "df_array.show(truncate=False) # shows all columns"
      ],
      "metadata": {
        "id": "KRnw_ETEci45",
        "outputId": "65004037-5205-43a5-ffe6-9b9df4b5a9b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- movies: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n",
            "+---------------------------------------+\n",
            "|movies                                 |\n",
            "+---------------------------------------+\n",
            "|[KGF 1, KGF 2, Autograph, Kicha, Hucha]|\n",
            "+---------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge simple arrays\n"
      ],
      "metadata": {
        "id": "eKUjWRTJpdqc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import concat, array\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "data = [\n",
        "    (1, [1, 2], [1,3, 4]),\n",
        "    (2, [5, 6], [7, 8]),\n",
        "]\n",
        "df = spark.createDataFrame(data, [\"id\", \"arr1\", \"arr2\"])\n",
        "\n",
        "# concat arrays\n",
        "df2 = df.withColumn(\"merged\", concat(\"arr1\", \"arr2\"))\n",
        "\n",
        "df2.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmzRZag1q150",
        "outputId": "debfd8b1-ef69-481e-a173-32a94f45c750"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+---------+---------------+\n",
            "|id |arr1  |arr2     |merged         |\n",
            "+---+------+---------+---------------+\n",
            "|1  |[1, 2]|[1, 3, 4]|[1, 2, 1, 3, 4]|\n",
            "|2  |[5, 6]|[7, 8]   |[5, 6, 7, 8]   |\n",
            "+---+------+---------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Concat cant not handle nested array\n",
        "\n",
        "from pyspark.sql.functions import flatten, concat\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    (1, [[1, 2], [3, 4]]),\n",
        "    (2, [[5, 6]])\n",
        "], [\"id\", \"nested_array\"])\n",
        "\n",
        "df.select(\"id\",\"nested_array\",concat(\"nested_array\")).show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sK1Ig2YFMYU",
        "outputId": "0d787ea1-4b21-4840-bab5-0ddab309d96b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+----------------+--------------------+\n",
            "|id |nested_array    |concat(nested_array)|\n",
            "+---+----------------+--------------------+\n",
            "|1  |[[1, 2], [3, 4]]|[[1, 2], [3, 4]]    |\n",
            "|2  |[[5, 6]]        |[[5, 6]]            |\n",
            "+---+----------------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flatten Nested Array"
      ],
      "metadata": {
        "id": "jw-4Lda2EYxp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import flatten\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "    (1, [[1, 2], [3, 4]]),\n",
        "    (2, [[5, 6]])\n",
        "], [\"id\", \"nested_array\"])\n",
        "\n",
        "df.select(flatten(\"nested_array\")).show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRSAVBMzEbZv",
        "outputId": "edbe1f7d-27da-45f4-ad67-d75c40a17903"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------+\n",
            "|flatten(nested_array)|\n",
            "+---------------------+\n",
            "|[1, 2, 3, 4]         |\n",
            "|[5, 6]               |\n",
            "+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import flatten\n",
        "\n",
        "# Sample data: each row includes a product ID and nested arrays of review key phrases\n",
        "data = [\n",
        "    (1, [[\"great battery life\", \"sleek design\"], [\"heavy\", \"expensive\"], [\"sleek design\"]]),\n",
        "    (2, [[\"easy to install\", \"value for money\"], [\"requires maintenance\"]])\n",
        "]\n",
        "\n",
        "# Create DataFrame\n",
        "df = spark.createDataFrame(data, [\"product_id\", \"reviews\"])\n",
        "\n",
        "# Show the original DataFrame\n",
        "df.show(truncate=False)\n",
        "\n",
        "# Flatten the nested array of reviews into a single array per product\n",
        "flattened_df = df.withColumn(\"flattened_reviews\", flatten(df[\"reviews\"]))\n",
        "\n",
        "# Show the DataFrame with the flattened reviews\n",
        "flattened_df.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfvxahcBFFac",
        "outputId": "bf8feedf-7f97-4fff-b7fc-5d05421ceada"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------------------------------------------------------------------+\n",
            "|product_id|reviews                                                                 |\n",
            "+----------+------------------------------------------------------------------------+\n",
            "|1         |[[great battery life, sleek design], [heavy, expensive], [sleek design]]|\n",
            "|2         |[[easy to install, value for money], [requires maintenance]]            |\n",
            "+----------+------------------------------------------------------------------------+\n",
            "\n",
            "+----------+------------------------------------------------------------------------+------------------------------------------------------------------+\n",
            "|product_id|reviews                                                                 |flattened_reviews                                                 |\n",
            "+----------+------------------------------------------------------------------------+------------------------------------------------------------------+\n",
            "|1         |[[great battery life, sleek design], [heavy, expensive], [sleek design]]|[great battery life, sleek design, heavy, expensive, sleek design]|\n",
            "|2         |[[easy to install, value for money], [requires maintenance]]            |[easy to install, value for money, requires maintenance]          |\n",
            "+----------+------------------------------------------------------------------------+------------------------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import flatten\n",
        "# Sample DataFrame creation\n",
        "recs_df = spark.createDataFrame([\n",
        "    (1, [[\"prodA\", \"prodB\"], [\"prodC\"]]),\n",
        "    (2, [[\"prodD\"], [\"prodE\", \"prodF\"]])\n",
        "], [\"user_id\", \"recommendations\"])\n",
        "\n",
        "# without flattening\n",
        "recs_df.show(truncate=False)\n",
        "\n",
        "flattened_recs = recs_df.select(\"user_id\", flatten(\"recommendations\").alias(\"all_recs\"))\n",
        "flattened_recs.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kU1JpK6oGz4U",
        "outputId": "ac50341b-b9ec-4b13-ea90-9d0c9321c3ec"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------------+\n",
            "|user_id|recommendations          |\n",
            "+-------+-------------------------+\n",
            "|1      |[[prodA, prodB], [prodC]]|\n",
            "|2      |[[prodD], [prodE, prodF]]|\n",
            "+-------+-------------------------+\n",
            "\n",
            "+-------+---------------------+\n",
            "|user_id|all_recs             |\n",
            "+-------+---------------------+\n",
            "|1      |[prodA, prodB, prodC]|\n",
            "|2      |[prodD, prodE, prodF]|\n",
            "+-------+---------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explode"
      ],
      "metadata": {
        "id": "BfSfR0RXHM0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode\n",
        "df_exploded = recs_df.withColumn(\"all_recs\", explode(flatten(\"recommendations\")))\n",
        "df_exploded.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiyAZspqHKOJ",
        "outputId": "686f2177-89c5-4c17-b7c2-3d53feb18924"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+-------------------------+--------+\n",
            "|user_id|recommendations          |all_recs|\n",
            "+-------+-------------------------+--------+\n",
            "|1      |[[prodA, prodB], [prodC]]|prodA   |\n",
            "|1      |[[prodA, prodB], [prodC]]|prodB   |\n",
            "|1      |[[prodA, prodB], [prodC]]|prodC   |\n",
            "|2      |[[prodD], [prodE, prodF]]|prodD   |\n",
            "|2      |[[prodD], [prodE, prodF]]|prodE   |\n",
            "|2      |[[prodD], [prodE, prodF]]|prodF   |\n",
            "+-------+-------------------------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode\n",
        "df_exploded = flattened_recs.withColumn(\"all_recs\", explode(\"all_recs\"))\n",
        "df_exploded.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iql5DT-zHo90",
        "outputId": "d381d439-d1a2-48f5-a5f3-9be762281c1d"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------+\n",
            "|user_id|all_recs|\n",
            "+-------+--------+\n",
            "|      1|   prodA|\n",
            "|      1|   prodB|\n",
            "|      1|   prodC|\n",
            "|      2|   prodD|\n",
            "|      2|   prodE|\n",
            "|      2|   prodF|\n",
            "+-------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import explode_outer\n",
        "\n",
        "# Handling nulls and empties\n",
        "nullable_df = spark.createDataFrame([\n",
        "    (1, [\"apple\", \"banana\"]),\n",
        "    (2, []),\n",
        "    (3, None)\n",
        "], [\"id\", \"fruits\"])\n",
        "\n",
        "# Applying explode_outer\n",
        "nullable_exploded = nullable_df.select(\"id\", explode(\"fruits\").alias(\"fruit\"))\n",
        "nullable_exploded.show()\n",
        "\n",
        "\n",
        "nullable_df.select(\"id\", explode_outer(\"fruits\").alias(\"fruit\")).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWuN5IUHIL6z",
        "outputId": "d2142e26-296b-4fdb-9407-85663ca8505a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------+\n",
            "| id| fruit|\n",
            "+---+------+\n",
            "|  1| apple|\n",
            "|  1|banana|\n",
            "+---+------+\n",
            "\n",
            "+---+------+\n",
            "| id| fruit|\n",
            "+---+------+\n",
            "|  1| apple|\n",
            "|  1|banana|\n",
            "|  2|  NULL|\n",
            "|  3|  NULL|\n",
            "+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge arrays - Using Array Union\n"
      ],
      "metadata": {
        "id": "oB2MIulWpdnq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import array_union\n",
        "\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "data = [\n",
        "    (1, [1, 2, 3], [3, 4, 5]),\n",
        "    (2, [5, 6], [6, 7, 8])\n",
        "]\n",
        "df = spark.createDataFrame(data, [\"id\", \"arr1\", \"arr2\"])\n",
        "\n",
        "df2 = df.withColumn(\"union_array\", array_union(\"arr1\", \"arr2\"))\n",
        "\n",
        "df2.show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTPRhY5-q-Y6",
        "outputId": "207f8cd8-9713-4bbd-abad-d86d81e3d8e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------+---------+---------------+\n",
            "|id |arr1     |arr2     |union_array    |\n",
            "+---+---------+---------+---------------+\n",
            "|1  |[1, 2, 3]|[3, 4, 5]|[1, 2, 3, 4, 5]|\n",
            "|2  |[5, 6]   |[6, 7, 8]|[5, 6, 7, 8]   |\n",
            "+---+---------+---------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Play with Map\n"
      ],
      "metadata": {
        "id": "r8_hBKIdhUev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating Map type**"
      ],
      "metadata": {
        "id": "FtiiHCUffA9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sql_map = spark.sql(\"SELECT map('Building','500 CR', 'Commercal',100) as income\")\n",
        "df_sql_map.printSchema()\n",
        "df_sql_map.show(truncate=False)"
      ],
      "metadata": {
        "id": "PpgJZQiUfJA3",
        "outputId": "aecd7037-6294-4dd5-da2e-c464636bb7fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- income: map (nullable = false)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = false)\n",
            "\n",
            "+--------------------------------------+\n",
            "|income                                |\n",
            "+--------------------------------------+\n",
            "|{Building -> 500 CR, Commercal -> 100}|\n",
            "+--------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "                              Row({\"Bank\":\"100 CR\", \"Business\":\"50 CR\", \"Land\":\"150 CR\"}),\n",
        "                              Row({\"Others\":\"300 CR\"}),\n",
        "                              Row({\"Building\":\"500 CR\", \"Commercal\":100}),\n",
        "                              Row({\"Building\":100 , \"Commercal\":100}),\n",
        "                            ]\n",
        "                            , [\"Assests\"])\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "id": "q74em3H4heaZ",
        "outputId": "639281ec-1b9c-4b08-f678-2429b9480e9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------+\n",
            "|Assests                                            |\n",
            "+---------------------------------------------------+\n",
            "|{Bank -> 100 CR, Land -> 150 CR, Business -> 50 CR}|\n",
            "|{Others -> 300 CR}                                 |\n",
            "|{Building -> 500 CR, Commercal -> 100}             |\n",
            "|{Building -> 100, Commercal -> 100}                |\n",
            "+---------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType,StructField, StringType, ArrayType,MapType\n",
        "\n",
        "\n",
        "data = [\n",
        "\n",
        "          ({\"Bank\":\"100 CR\", \"Business\":\"50 CR\", \"Land\":\"150 CR\"}),\n",
        "          ({\"Others\":\"300 CR\"}),\n",
        "          # ({\"Building\":\"500 CR\", \"Commercal\":100}),\n",
        "          # ({\"Building\":\"500 CR\", \"Commercal\":100}),\n",
        "\n",
        "      ]\n",
        "\n",
        "\n",
        "# Schema\n",
        "schema = StructType([\n",
        "               StructField('properties', MapType(StringType(),StringType()), True)\n",
        "        ])\n",
        "\n",
        "# Create DataFrame\n",
        "df_map = spark.createDataFrame(data = data, schema = schema)\n",
        "df_map.printSchema()\n",
        "df_map.show(truncate=False) # shows all columns"
      ],
      "metadata": {
        "id": "vL_8teOyfpC4",
        "outputId": "44a2954e-8e88-4144-f857-4042f3e8ad92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- properties: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            "\n",
            "+----------+\n",
            "|properties|\n",
            "+----------+\n",
            "|NULL      |\n",
            "|NULL      |\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "|spark.sql(\"SELECT struct(1, 2, 3) as ex_struct\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77On1brROzSI",
        "outputId": "11a8b49c-d290-4289-9972-b8180310fcc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[ex_struct: struct<col1:int,col2:int,col3:int>]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge struct"
      ],
      "metadata": {
        "id": "s6A806YNrHmT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import map_concat\n",
        "\n",
        "# Sample Data\n",
        "data = [\n",
        "    (1, {\"a\": 1, \"b\": 2}, {\"c\": 3, \"d\": 4}),\n",
        "    (2, {\"x\": 10}, {\"y\": 20, \"z\": 30}),\n",
        "    (3, {}, {\"p\": 100, \"q\": 200}),\n",
        "    (4, {\"k\": 5}, {})\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"id\", \"map1\", \"map2\"])\n",
        "\n",
        "print(\"before the merge\")\n",
        "df.show(truncate=False)\n",
        "\n",
        "# Merge the two map columns\n",
        "df_merged = df.withColumn(\"merged_map\", map_concat(\"map1\", \"map2\"))\n",
        "\n",
        "print(\"after the merge\")\n",
        "df_merged.show(truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FVfLEashrHXr",
        "outputId": "498cb39e-7462-4dc0-fd0a-3be79ce3d3a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "before the merge\n",
            "+---+----------------+--------------------+\n",
            "|id |map1            |map2                |\n",
            "+---+----------------+--------------------+\n",
            "|1  |{a -> 1, b -> 2}|{d -> 4, c -> 3}    |\n",
            "|2  |{x -> 10}       |{y -> 20, z -> 30}  |\n",
            "|3  |{}              |{p -> 100, q -> 200}|\n",
            "|4  |{k -> 5}        |{}                  |\n",
            "+---+----------------+--------------------+\n",
            "\n",
            "after the merge\n",
            "+---+----------------+--------------------+--------------------------------+\n",
            "|id |map1            |map2                |merged_map                      |\n",
            "+---+----------------+--------------------+--------------------------------+\n",
            "|1  |{a -> 1, b -> 2}|{d -> 4, c -> 3}    |{a -> 1, b -> 2, d -> 4, c -> 3}|\n",
            "|2  |{x -> 10}       |{y -> 20, z -> 30}  |{x -> 10, y -> 20, z -> 30}     |\n",
            "|3  |{}              |{p -> 100, q -> 200}|{p -> 100, q -> 200}            |\n",
            "|4  |{k -> 5}        |{}                  |{k -> 5}                        |\n",
            "+---+----------------+--------------------+--------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Play with Struct\n"
      ],
      "metadata": {
        "id": "Ky55RcyOiHOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_struct = spark.sql(\"SELECT struct(1, 2, '3') as ex_struct\")\n",
        "df_struct.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACGJLu9-NjKw",
        "outputId": "266cc7c4-a284-47da-8a16-b17910862ffe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|ex_struct|\n",
            "+---------+\n",
            "|{1, 2, 3}|\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_struct.select(\"ex_struct.col3\", \"ex_struct\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B46JwCaePajw",
        "outputId": "82ce3755-9e13-404d-897c-a0c22551a2bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------+\n",
            "|col3|ex_struct|\n",
            "+----+---------+\n",
            "|   3|{1, 2, 3}|\n",
            "+----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_struct.select(\"ex_struct.*\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeMsLV5Qdbp9",
        "outputId": "4a03e2d9-f573-4e65-f05f-57e597d116d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+----+\n",
            "|col1|col2|col3|\n",
            "+----+----+----+\n",
            "|1   |2   |3   |\n",
            "+----+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_map = spark.sql(\"SELECT map(1.0, '2', 3.0, '4') as ex_map\")\n",
        "df_map.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-Dl2Y1DOSd3",
        "outputId": "b1195e4e-834a-40f3-e646-3f8e06500f20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|              ex_map|\n",
            "+--------------------+\n",
            "|{1.0 -> 2, 3.0 -> 4}|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType,StructField, StringType, ArrayType,MapType\n",
        "\n",
        "# https://sparkbyexamples.com/pyspark/pyspark-maptype-dict-examples/\n",
        "\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "\n",
        "# Data\n",
        "data = [\n",
        "        ((\"James\",None,\"Smith\"),\"OH\",\"M\"),\n",
        "        ((\"Anna\",\"Rose\",\"\"),\"NY\",\"F\"),\n",
        "        ((\"Julia\",\"\",\"Williams\"),\"OH\",\"F\"),\n",
        "        ((\"Maria\",\"Anne\",\"Jones\"),\"NY\",\"M\"),\n",
        "        ((\"Jen\",\"Mary\",\"Brown\"),\"NY\",\"M\"),\n",
        "        ((\"Mike\",\"Mary\",\"Williams\"),\"OH\",\"M\")\n",
        "        ]\n",
        "\n",
        "# Schema\n",
        "schema = StructType([\n",
        "    StructField('name', StructType([\n",
        "         StructField('firstname', StringType(), True),\n",
        "         StructField('middlename', StringType(), True),\n",
        "         StructField('lastname', StringType(), True)\n",
        "         ])),\n",
        "     StructField('state', StringType(), True),\n",
        "     StructField('gender', StringType(), True)\n",
        "     ])\n",
        "\n",
        "# Create DataFrame\n",
        "df2 = spark.createDataFrame(data = data, schema = schema)\n",
        "df2.printSchema()\n",
        "df2.show(truncate=False) # shows all columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jye_i4rTOfIk",
        "outputId": "bbe0fdec-6e59-4576-ce1f-863553ac0384"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: struct (nullable = true)\n",
            " |    |-- firstname: string (nullable = true)\n",
            " |    |-- middlename: string (nullable = true)\n",
            " |    |-- lastname: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            "\n",
            "+----------------------+-----+------+\n",
            "|name                  |state|gender|\n",
            "+----------------------+-----+------+\n",
            "|{James, NULL, Smith}  |OH   |M     |\n",
            "|{Anna, Rose, }        |NY   |F     |\n",
            "|{Julia, , Williams}   |OH   |F     |\n",
            "|{Maria, Anne, Jones}  |NY   |M     |\n",
            "|{Jen, Mary, Brown}    |NY   |M     |\n",
            "|{Mike, Mary, Williams}|OH   |M     |\n",
            "+----------------------+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2.select(\"name\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLx1sLW0ctrH",
        "outputId": "6fdd982d-97ec-4db7-c198-c55727ce70ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|                name|\n",
            "+--------------------+\n",
            "|{James, NULL, Smith}|\n",
            "|      {Anna, Rose, }|\n",
            "| {Julia, , Williams}|\n",
            "|{Maria, Anne, Jones}|\n",
            "|  {Jen, Mary, Brown}|\n",
            "|{Mike, Mary, Will...|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType,StructField, StringType, ArrayType,MapType\n",
        "from pyspark.sql.functions import element_at\n",
        "\n",
        "data = [\n",
        "        ([(\"Yash\",\"K\",None),(\"Yash2\",\"K2\",None)],\"BL\",\"M\",[\"KGF 1\", \"KGF 2\"],{\"Bank\":\"100 CR\", \"Business\":\"50 CR\", \"Land\":\"150 CR\"}),\n",
        "        ([(\"Sudeep\",\"Kicha\",\"S\")],\"DL\",\"M\",[\"Autograph\", \"Kicha\",\"Hucha\"],{\"Others\":\"300 CR\"}),\n",
        "        ([(\"Puneeth\",None,\"Raj\")],\"MB\",\"M\",[], {\"Building\":\"500 CR\", \"Commercal\":100}),\n",
        "        ([(\"Darshan\",None,None)],\"MB\",\"M\",[], {\"Building\":\"500 CR\", \"Commercal\":100}),\n",
        "\n",
        "        ]\n",
        "\n",
        "# Schema\n",
        "schema = StructType([\n",
        "    StructField(    'name'\n",
        "                  , ArrayType(\n",
        "                        StructType([\n",
        "                            StructField('firstname', StringType(), True),\n",
        "                            StructField('middlename', StringType(), True),\n",
        "                            StructField('lastname', StringType(), True)\n",
        "                        ])\n",
        "                    ), True),\n",
        "     StructField('state', StringType(), True),\n",
        "     StructField('gender', StringType(), True),\n",
        "     StructField('movies', ArrayType(StringType()), True),\n",
        "     StructField('properties', MapType(StringType(),StringType()), True)\n",
        "     ])\n",
        "\n",
        "# Create DataFrame\n",
        "df2 = spark.createDataFrame(data = data, schema = schema)\n",
        "df2.printSchema()\n",
        "df2.show(truncate=False) # shows all columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJwWwq4Xc-0Z",
        "outputId": "468e11f5-95f8-4a03-80a4-102964f61988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- firstname: string (nullable = true)\n",
            " |    |    |-- middlename: string (nullable = true)\n",
            " |    |    |-- lastname: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- movies: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- properties: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            "\n",
            "+------------------------------------+-----+------+-------------------------+---------------------------------------------------+\n",
            "|name                                |state|gender|movies                   |properties                                         |\n",
            "+------------------------------------+-----+------+-------------------------+---------------------------------------------------+\n",
            "|[{Yash, K, NULL}, {Yash2, K2, NULL}]|BL   |M     |[KGF 1, KGF 2]           |{Bank -> 100 CR, Land -> 150 CR, Business -> 50 CR}|\n",
            "|[{Sudeep, Kicha, S}]                |DL   |M     |[Autograph, Kicha, Hucha]|{Others -> 300 CR}                                 |\n",
            "|[{Puneeth, NULL, Raj}]              |MB   |M     |[]                       |{Building -> 500 CR, Commercal -> 100}             |\n",
            "|[{Darshan, NULL, NULL}]             |MB   |M     |[]                       |{Building -> 500 CR, Commercal -> 100}             |\n",
            "+------------------------------------+-----+------+-------------------------+---------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " (df2\n",
        "  .select(\"properties\", \"name\", \"name.firstname\")\n",
        "  .withColumn(\"M-Building\", col(\"properties\").getItem(\"Building\"))\n",
        "  .withColumn(\"M-Commercal \", col(\"properties\").getItem(\"Commercal\"))\n",
        "  .withColumn(\"M-Others \", col(\"properties\").getItem(\"Others\"))\n",
        "  .withColumn(\"S-FName\", col(\"name.firstname\"))\n",
        "  ).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0-Z3EjP0njz",
        "outputId": "a6e04d9a-3bb8-48f6-9dcd-8a7d9bd219a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+---------+----------+------------+---------+-------+\n",
            "|          properties|                name|firstname|M-Building|M-Commercal |M-Others |S-FName|\n",
            "+--------------------+--------------------+---------+----------+------------+---------+-------+\n",
            "|{Bank -> 100 CR, ...|     {Yash, K, NULL}|     Yash|      NULL|        NULL|     NULL|   Yash|\n",
            "|  {Others -> 300 CR}|  {Sudeep, Kicha, S}|   Sudeep|      NULL|        NULL|   300 CR| Sudeep|\n",
            "|{Building -> 500 ...|{Puneeth, NULL, Raj}|  Puneeth|    500 CR|         100|     NULL|Puneeth|\n",
            "|{Building -> 500 ...|{Darshan, NULL, N...|  Darshan|    500 CR|         100|     NULL|Darshan|\n",
            "+--------------------+--------------------+---------+----------+------------+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType,StructField, StringType, ArrayType,MapType\n",
        "from pyspark.sql.functions import element_at\n",
        "\n",
        "data = [\n",
        "        ((\"Yash\",\"K\",None),\"BL\",\"M\",[\"KGF 1\", \"KGF 2\"],{\"Bank\":\"100 CR\", \"Business\":\"50 CR\", \"Land\":\"150 CR\"}),\n",
        "        ((\"Sudeep\",\"Kicha\",\"S\"),\"DL\",\"M\",[\"Autograph\", \"Kicha\",\"Hucha\"],{\"Others\":\"300 CR\"}),\n",
        "        ((\"Puneeth\",None,\"Raj\"),\"MB\",\"M\",[], {\"Building\":\"500 CR\", \"Commercal\":100}),\n",
        "        ((\"Darshan\",None,None),\"MB\",\"M\",[], {\"Building\":\"500 CR\", \"Commercal\":100}),\n",
        "\n",
        "        ]\n",
        "\n",
        "# Schema\n",
        "schema = StructType([\n",
        "    StructField('name', StructType([\n",
        "         StructField('firstname', StringType(), True),\n",
        "         StructField('middlename', StringType(), True),\n",
        "         StructField('lastname', StringType(), True)\n",
        "         ])),\n",
        "     StructField('state', StringType(), True),\n",
        "     StructField('gender', StringType(), True),\n",
        "     StructField('movies', ArrayType(StringType()), True),\n",
        "     StructField('properties', MapType(StringType(),StringType()), True)\n",
        "     ])\n",
        "\n",
        "# Create DataFrame\n",
        "df2 = spark.createDataFrame(data = data, schema = schema)\n",
        "df2.printSchema()\n",
        "df2.show(truncate=False) # shows all columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QiMme632DET",
        "outputId": "22e1ec1c-594b-473b-b950-2f87e1b7e29c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: struct (nullable = true)\n",
            " |    |-- firstname: string (nullable = true)\n",
            " |    |-- middlename: string (nullable = true)\n",
            " |    |-- lastname: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- movies: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- properties: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            "\n",
            "+---------------------+-----+------+-------------------------+---------------------------------------------------+\n",
            "|name                 |state|gender|movies                   |properties                                         |\n",
            "+---------------------+-----+------+-------------------------+---------------------------------------------------+\n",
            "|{Yash, K, NULL}      |BL   |M     |[KGF 1, KGF 2]           |{Bank -> 100 CR, Land -> 150 CR, Business -> 50 CR}|\n",
            "|{Sudeep, Kicha, S}   |DL   |M     |[Autograph, Kicha, Hucha]|{Others -> 300 CR}                                 |\n",
            "|{Puneeth, NULL, Raj} |MB   |M     |[]                       |{Building -> 500 CR, Commercal -> 100}             |\n",
            "|{Darshan, NULL, NULL}|MB   |M     |[]                       |{Building -> 500 CR, Commercal -> 100}             |\n",
            "+---------------------+-----+------+-------------------------+---------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
        "\n",
        "myManualSchema = StructType([\n",
        "  StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
        "  StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
        "  StructField(\"count\", LongType(), False, metadata={\"hello\":\"world\"})\n",
        "])\n",
        "df = spark.read.format(\"json\").schema(myManualSchema)\\\n",
        "  .load(\"sample_data/2015-summary.json\")\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "id": "21WEGU2v8_tA",
        "outputId": "50a1e446-52a5-4bf8-c206-028cf5b7f366",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/content/sample_data/2015-summary.json.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-50b2766d5f6a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m ])\n\u001b[1;32m      8\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyManualSchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sample_data/2015-summary.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/content/sample_data/2015-summary.json."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "id": "mcGnKSvKbQSR",
        "outputId": "42ba3405-038e-4bcd-fb0b-92de2fa18373",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
            " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
            " |-- count: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.read.format(\"json\").load(\"sample_data/2015-summary.json\").schema"
      ],
      "metadata": {
        "id": "vC1GYIlfbT9m",
        "outputId": "0e7f09ed-fa95-454d-f2dc-168a6a8eaac2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructType([StructField('DEST_COUNTRY_NAME', StringType(), True), StructField('ORIGIN_COUNTRY_NAME', StringType(), True), StructField('count', LongType(), True)])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Other Examples\n",
        "\n"
      ],
      "metadata": {
        "id": "Q2lTqwHdg9V4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using expressions"
      ],
      "metadata": {
        "id": "IJPbdlX1cAQw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "from pyspark.sql.functions import col, expr\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "  Row(name=\"san\", salary=1500),\n",
        "  Row(name=\"ana\", salary=2000),\n",
        "  Row(name=\"shu\", salary=1000)\n",
        "])\n",
        "\n",
        "print(\"approach 1:\")\n",
        "df.withColumn(\"bonus\", df.salary * 0.1).show()\n",
        "\n",
        "print(\"approach 2:\")\n",
        "df.withColumn(\"bonus\", expr(\"salary * 0.1\")).show()\n",
        "\n",
        "print(\"approach 3:\")\n",
        "df.select(col(\"name\"), col(\"salary\"), (col(\"salary\") * 0.1).alias(\"bonus\")).show()"
      ],
      "metadata": {
        "id": "2DZCMcH5DkWZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c01f4e9-a34c-4dea-fe21-6fc6abe01c32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "approach 1:\n",
            "+----+------+-----+\n",
            "|name|salary|bonus|\n",
            "+----+------+-----+\n",
            "| san|  1500|150.0|\n",
            "| ana|  2000|200.0|\n",
            "| shu|  1000|100.0|\n",
            "+----+------+-----+\n",
            "\n",
            "approach 2:\n",
            "+----+------+-----+\n",
            "|name|salary|bonus|\n",
            "+----+------+-----+\n",
            "| san|  1500|150.0|\n",
            "| ana|  2000|200.0|\n",
            "| shu|  1000|100.0|\n",
            "+----+------+-----+\n",
            "\n",
            "approach 3:\n",
            "+----+------+-----+\n",
            "|name|salary|bonus|\n",
            "+----+------+-----+\n",
            "| san|  1500|150.0|\n",
            "| ana|  2000|200.0|\n",
            "| shu|  1000|100.0|\n",
            "+----+------+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WhEcKtt3bvJp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}