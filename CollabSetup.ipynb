{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SanjayJanardhan-89/ApacheSparkHandsOn/blob/main/CollabSetup.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WuOZ0C0lhCL4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Pyspark\n"
      ],
      "metadata": {
        "id": "vRYFGd5ehHN5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 743
        },
        "id": "QLc7s-Qo1irp",
        "outputId": "44fc1a02-9512-40ea-e1db-0dd84047de14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:10 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [72.6 kB]\n",
            "Hit:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Get:12 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,688 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,788 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,824 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,243 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,542 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,100 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,161 kB]\n",
            "Fetched 24.7 MB in 8s (3,114 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "38 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (0.10.9.7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7c2dae441e50>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://07f5ee144c12:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.5</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>OurSparkApp</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"OurSparkApp\") \\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Import\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bmV_LdqNnVAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import col, count, rand, collect_list, explode, struct, count, lit\n",
        "from pyspark.sql.functions import pandas_udf, PandasUDFType, udf"
      ],
      "metadata": {
        "id": "fUk6ZxxJnOjc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.range(0, 10 * 1000 * 1000).withColumn('id', (col('id') / 10000).cast('integer')).withColumn('v', rand())\n",
        "df.cache()\n",
        "df.count()\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x3RQbS0XncGw",
        "outputId": "e203e50e-5367-43f9-d99f-d18d170e8337"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------+\n",
            "| id|                   v|\n",
            "+---+--------------------+\n",
            "|  0| 0.48723839870162955|\n",
            "|  0| 0.15992292148320963|\n",
            "|  0|  0.7117016419527978|\n",
            "|  0| 0.32731501110858585|\n",
            "|  0| 0.06733168598051786|\n",
            "|  0| 0.44530526631032974|\n",
            "|  0|  0.2877573197320078|\n",
            "|  0|  0.2061868747013622|\n",
            "|  0|  0.5611732422236512|\n",
            "|  0|  0.5621137614531215|\n",
            "|  0|  0.4260670334473634|\n",
            "|  0| 0.14480539521192126|\n",
            "|  0| 0.04314252400358565|\n",
            "|  0|0.042197541645179526|\n",
            "|  0| 0.13284635172585146|\n",
            "|  0|   0.859677668094348|\n",
            "|  0|  0.4115068723620233|\n",
            "|  0|   0.677421189108456|\n",
            "|  0|  0.6925199283671691|\n",
            "|  0|  0.6527299352576307|\n",
            "+---+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@udf('double')\n",
        "def plus_one(v):\n",
        "    return v + 1\n",
        "\n",
        "%timeit df.withColumn('v', plus_one(df.v)).agg(count(col('v'))).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ez-72Xu-nf5K",
        "outputId": "2ade635c-6941-4bb1-c0af-e3bbddd73218"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|count(v)|\n",
            "+--------+\n",
            "|10000000|\n",
            "+--------+\n",
            "\n",
            "+--------+\n",
            "|count(v)|\n",
            "+--------+\n",
            "|10000000|\n",
            "+--------+\n",
            "\n",
            "+--------+\n",
            "|count(v)|\n",
            "+--------+\n",
            "|10000000|\n",
            "+--------+\n",
            "\n",
            "+--------+\n",
            "|count(v)|\n",
            "+--------+\n",
            "|10000000|\n",
            "+--------+\n",
            "\n",
            "+--------+\n",
            "|count(v)|\n",
            "+--------+\n",
            "|10000000|\n",
            "+--------+\n",
            "\n",
            "+--------+\n",
            "|count(v)|\n",
            "+--------+\n",
            "|10000000|\n",
            "+--------+\n",
            "\n",
            "+--------+\n",
            "|count(v)|\n",
            "+--------+\n",
            "|10000000|\n",
            "+--------+\n",
            "\n",
            "+--------+\n",
            "|count(v)|\n",
            "+--------+\n",
            "|10000000|\n",
            "+--------+\n",
            "\n",
            "25.6 s ± 1.68 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@pandas_udf(\"double\", PandasUDFType.SCALAR)\n",
        "def pandas_plus_one(v):\n",
        "    return v + 1\n",
        "\n",
        "%timeit df.withColumn('v', pandas_plus_one(df.v)).agg(count(col('v'))).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcCBEIq4nm_M",
        "outputId": "23de8cf6-7125-4694-821a-2eed9516f0bf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pyspark/sql/pandas/functions.py:407: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+\n",
            "|count(v)|\n",
            "+--------+\n",
            "|10000000|\n",
            "+--------+\n",
            "\n",
            "+--------+\n",
            "|count(v)|\n",
            "+--------+\n",
            "|10000000|\n",
            "+--------+\n",
            "\n",
            "+--------+\n",
            "|count(v)|\n",
            "+--------+\n",
            "|10000000|\n",
            "+--------+\n",
            "\n",
            "+--------+\n",
            "|count(v)|\n",
            "+--------+\n",
            "|10000000|\n",
            "+--------+\n",
            "\n",
            "+--------+\n",
            "|count(v)|\n",
            "+--------+\n",
            "|10000000|\n",
            "+--------+\n",
            "\n",
            "+--------+\n",
            "|count(v)|\n",
            "+--------+\n",
            "|10000000|\n",
            "+--------+\n",
            "\n",
            "+--------+\n",
            "|count(v)|\n",
            "+--------+\n",
            "|10000000|\n",
            "+--------+\n",
            "\n",
            "+--------+\n",
            "|count(v)|\n",
            "+--------+\n",
            "|10000000|\n",
            "+--------+\n",
            "\n",
            "6.12 s ± 684 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pandas UDF\n",
        "import pandas as pd\n",
        "from pyspark.sql.functions import pandas_udf, log2, col\n",
        "\n",
        "@pandas_udf('long')\n",
        "def pandas_plus_one(s: pd.Series) -> pd.Series:\n",
        "    return s + 1\n",
        "\n",
        "# pandas_plus_one(\"id\") is identically treated as _a SQL expression_ internally.\n",
        "# Namely, you can combine with other columns, functions and expressions.\n",
        "spark.range(10).select(\n",
        "    pandas_plus_one(col(\"id\") - 1) + log2(\"id\") + 1).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OuUjNdGLn6gH",
        "outputId": "1c76364e-50f6-4ac4-9ea2-68d452352aea"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------------------------------+\n",
            "|((pandas_plus_one((id - 1)) + LOG2(id)) + 1)|\n",
            "+--------------------------------------------+\n",
            "|                                        NULL|\n",
            "|                                         2.0|\n",
            "|                                         4.0|\n",
            "|                           5.584962500721156|\n",
            "|                                         7.0|\n",
            "|                           8.321928094887362|\n",
            "|                           9.584962500721156|\n",
            "|                          10.807354922057604|\n",
            "|                                        12.0|\n",
            "|                          13.169925001442312|\n",
            "+--------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Pandas Function API\n",
        "from typing import Iterator\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def pandas_plus_one(iterator: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\n",
        "    return map(lambda v: v + 1, iterator)\n",
        "\n",
        "\n",
        "# pandas_plus_one is just a regular Python function, and mapInPandas is\n",
        "# logically treated as _a separate SQL query plan_ instead of a SQL expression.\n",
        "# Therefore, direct interactions with other expressions are impossible.\n",
        "spark.range(10).mapInPandas(pandas_plus_one, schema=\"id long\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSWL1-aLpWbu",
        "outputId": "943cb014-497a-4da0-a645-d0d76c106e74"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  1|\n",
            "|  2|\n",
            "|  3|\n",
            "|  4|\n",
            "|  5|\n",
            "|  6|\n",
            "|  7|\n",
            "|  8|\n",
            "|  9|\n",
            "| 10|\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1LC0zkE4q44p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Plane Python UDF"
      ],
      "metadata": {
        "id": "gBEksxNBrtTt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "bVJXmfQNrZRY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# In Python\n",
        "from pyspark.sql.types import LongType\n",
        "\n",
        "# Create cubed function\n",
        "def cubed(s):\n",
        "  return s * s * s\n",
        "\n",
        "# Register UDF\n",
        "spark.udf.register(\"cubed_udf\", cubed, LongType())\n",
        "\n",
        "# Generate temporary view\n",
        "spark.range(1, 9).createOrReplaceTempView(\"udf_test\")\n",
        "\n",
        "\n",
        "spark.sql(\"SELECT id, cubed_udf(id) AS id_cubed_udf FROM udf_test\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FnIK0JK6pagv",
        "outputId": "03e0d26a-f6f1-424c-b57e-7308efddeba3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------+\n",
            "| id|id_cubed_udf|\n",
            "+---+------------+\n",
            "|  1|           1|\n",
            "|  2|           8|\n",
            "|  3|          27|\n",
            "|  4|          64|\n",
            "|  5|         125|\n",
            "|  6|         216|\n",
            "|  7|         343|\n",
            "|  8|         512|\n",
            "+---+------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In Python\n",
        "# Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Import various pyspark SQL functions including pandas_udf\n",
        "from pyspark.sql.functions import col, pandas_udf\n",
        "from pyspark.sql.types import LongType\n",
        "\n",
        "# Declare the cubed function\n",
        "def cubed(a: pd.Series) -> pd.Series:\n",
        "    return a * a * a\n",
        "\n",
        "# Create the pandas UDF for the cubed function\n",
        "cubed_udf_pandas = pandas_udf(cubed, returnType=LongType())\n",
        "\n",
        "\n",
        "# Create a Pandas Series\n",
        "x = pd.Series([1, 2, 3])\n",
        "\n",
        "# The function for a cubed_udf_pandas executed with local Pandas data\n",
        "print(cubed(x))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXtszZ7hq6A_",
        "outputId": "8449693e-430d-467e-dd5f-e1ec2ff7ea4b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0     1\n",
            "1     8\n",
            "2    27\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Spark DataFrame, 'spark' is an existing SparkSession\n",
        "df = spark.range(1, 4)\n",
        "\n",
        "# Execute function as a Spark vectorized UDF\n",
        "df.select(\"id\", cubed_udf_pandas(col(\"id\"))).show()"
      ],
      "metadata": {
        "id": "gZ8Llz6Lr4gO",
        "outputId": "183db401-7201-4f05-f958-83620f799c3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+---------+\n",
            "| id|cubed(id)|\n",
            "+---+---------+\n",
            "|  1|        1|\n",
            "|  2|        8|\n",
            "|  3|       27|\n",
            "+---+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### iterator"
      ],
      "metadata": {
        "id": "FEHWcSI6slZG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "-6HNmW83slP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from typing import Iterator\n",
        "from pyspark.sql.functions import col, pandas_udf, struct\n",
        "\n",
        "pdf = pd.DataFrame([1, 2, 3], columns=[\"x\"])\n",
        "df = spark.createDataFrame(pdf)\n",
        "\n",
        "# When the UDF is called with the column,\n",
        "# the input to the underlying function is an iterator of pd.Series.\n",
        "@pandas_udf(\"long\")\n",
        "def plus_one(batch_iter: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
        "    for x in batch_iter:\n",
        "        yield x + 1\n",
        "\n",
        "df.select(plus_one(col(\"x\"))).show()"
      ],
      "metadata": {
        "id": "4Js0HIS_sO-v",
        "outputId": "bc093587-492b-44cc-ff58-df1a0e9032e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+\n",
            "|plus_one(x)|\n",
            "+-----------+\n",
            "|          2|\n",
            "|          3|\n",
            "|          4|\n",
            "+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_bc = spark.sparkContext.broadcast(1)\n",
        "\n",
        "print(y_bc)\n",
        "\n",
        "@pandas_udf(\"long\")\n",
        "def plus_y(batch_iter: Iterator[pd.Series]) -> Iterator[pd.Series]:\n",
        "    y = y_bc.value  # initialize states\n",
        "    print(y)\n",
        "    try:\n",
        "        for x in batch_iter:\n",
        "            yield x + y\n",
        "    finally:\n",
        "        pass  # release resources here, if any\n",
        "\n",
        "df.select(plus_y(col(\"x\"))).show()"
      ],
      "metadata": {
        "id": "E7DwC1XxsxbN",
        "outputId": "1d5a0c2b-c47c-4756-86a1-0a556f8e0dcb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pyspark.broadcast.Broadcast object at 0x7c2d90a1c0d0>\n",
            "+---------+\n",
            "|plus_y(x)|\n",
            "+---------+\n",
            "|        2|\n",
            "|        3|\n",
            "|        4|\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "e7G-VylbsyIK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}