{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOhdZqi6MBrsrAV87ehCEzK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SanjayJanardhan-89/ApacheSparkHandsOn/blob/main/ArrayFunctions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "WuOZ0C0lhCL4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup Pyspark\n"
      ],
      "metadata": {
        "id": "vRYFGd5ehHN5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 566
        },
        "id": "QLc7s-Qo1irp",
        "outputId": "34e73ba1-ae4d-46bf-92f9-cc2d4b847cc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "30 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "tar: spark-3.2.1-bin-hadoop3.2.tgz: Cannot open: No such file or directory\n",
            "tar: Error is not recoverable: exiting now\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.5)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (0.10.9.7)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7dac7369b550>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://ffaee0ead349:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.5</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>OurSparkApp</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "!sudo apt update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "#Check this site for the latest download link https://www.apache.org/dyn/closer.lua/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "!pip install -q findspark\n",
        "!pip install pyspark\n",
        "!pip install py4j\n",
        "\n",
        "import os\n",
        "import sys\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "findspark.find()\n",
        "\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark= SparkSession \\\n",
        "       .builder \\\n",
        "       .appName(\"OurSparkApp\") \\\n",
        "       .getOrCreate()\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Play with Arrays\n"
      ],
      "metadata": {
        "id": "cAbqNvQRhMI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sql_array = spark.sql(\"SELECT array('KGF 1', 'KGF 2', 'Autograph', 'Kicha','Hucha') as movies\")\n",
        "df_sql_array.printSchema()\n",
        "df_sql_array.show(truncate=False)"
      ],
      "metadata": {
        "id": "NDhHyKMo5Q4K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cccb56d-3444-4bc8-e626-9d4119ac4432"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- movies: array (nullable = false)\n",
            " |    |-- element: string (containsNull = false)\n",
            "\n",
            "+---------------------------------------+\n",
            "|movies                                 |\n",
            "+---------------------------------------+\n",
            "|[KGF 1, KGF 2, Autograph, Kicha, Hucha]|\n",
            "+---------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType,StructField, StringType, ArrayType,MapType\n",
        "\n",
        "data = [\n",
        "          [\n",
        "            [\"KGF 1\", \"KGF 2\", \"Autograph\", \"Kicha\",\"Hucha\"],\n",
        "            # [\"Hello\", \"Hi\"]\n",
        "          ]\n",
        "        ]\n",
        "\n",
        "# Schema\n",
        "schema = StructType([\n",
        "             StructField('movies', ArrayType(StringType()), True),\n",
        "     ])\n",
        "\n",
        "# Create DataFrame\n",
        "df_array = spark.createDataFrame(data = data, schema = schema)\n",
        "df_array.printSchema()\n",
        "df_array.show(truncate=False) # shows all columns"
      ],
      "metadata": {
        "id": "KRnw_ETEci45",
        "outputId": "65004037-5205-43a5-ffe6-9b9df4b5a9b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- movies: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            "\n",
            "+---------------------------------------+\n",
            "|movies                                 |\n",
            "+---------------------------------------+\n",
            "|[KGF 1, KGF 2, Autograph, Kicha, Hucha]|\n",
            "+---------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Play with Map\n"
      ],
      "metadata": {
        "id": "r8_hBKIdhUev"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating Map type**"
      ],
      "metadata": {
        "id": "FtiiHCUffA9m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sql_map = spark.sql(\"SELECT map('Building','500 CR', 'Commercal',100) as income\")\n",
        "df_sql_map.printSchema()\n",
        "df_sql_map.show(truncate=False)"
      ],
      "metadata": {
        "id": "PpgJZQiUfJA3",
        "outputId": "aecd7037-6294-4dd5-da2e-c464636bb7fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- income: map (nullable = false)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = false)\n",
            "\n",
            "+--------------------------------------+\n",
            "|income                                |\n",
            "+--------------------------------------+\n",
            "|{Building -> 500 CR, Commercal -> 100}|\n",
            "+--------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "df = spark.createDataFrame([\n",
        "                              Row({\"Bank\":\"100 CR\", \"Business\":\"50 CR\", \"Land\":\"150 CR\"}),\n",
        "                              Row({\"Others\":\"300 CR\"}),\n",
        "                              Row({\"Building\":\"500 CR\", \"Commercal\":100}),\n",
        "                              Row({\"Building\":100 , \"Commercal\":100}),\n",
        "                            ]\n",
        "                            , [\"Assests\"])\n",
        "df.show(truncate=False)"
      ],
      "metadata": {
        "id": "q74em3H4heaZ",
        "outputId": "639281ec-1b9c-4b08-f678-2429b9480e9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------------------------------------------+\n",
            "|Assests                                            |\n",
            "+---------------------------------------------------+\n",
            "|{Bank -> 100 CR, Land -> 150 CR, Business -> 50 CR}|\n",
            "|{Others -> 300 CR}                                 |\n",
            "|{Building -> 500 CR, Commercal -> 100}             |\n",
            "|{Building -> 100, Commercal -> 100}                |\n",
            "+---------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType,StructField, StringType, ArrayType,MapType\n",
        "\n",
        "\n",
        "data = [\n",
        "\n",
        "          ({\"Bank\":\"100 CR\", \"Business\":\"50 CR\", \"Land\":\"150 CR\"}),\n",
        "          ({\"Others\":\"300 CR\"}),\n",
        "          # ({\"Building\":\"500 CR\", \"Commercal\":100}),\n",
        "          # ({\"Building\":\"500 CR\", \"Commercal\":100}),\n",
        "\n",
        "      ]\n",
        "\n",
        "\n",
        "# Schema\n",
        "schema = StructType([\n",
        "               StructField('properties', MapType(StringType(),StringType()), True)\n",
        "        ])\n",
        "\n",
        "# Create DataFrame\n",
        "df_map = spark.createDataFrame(data = data, schema = schema)\n",
        "df_map.printSchema()\n",
        "df_map.show(truncate=False) # shows all columns"
      ],
      "metadata": {
        "id": "vL_8teOyfpC4",
        "outputId": "44a2954e-8e88-4144-f857-4042f3e8ad92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- properties: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            "\n",
            "+----------+\n",
            "|properties|\n",
            "+----------+\n",
            "|NULL      |\n",
            "|NULL      |\n",
            "+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "|spark.sql(\"SELECT struct(1, 2, 3) as ex_struct\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77On1brROzSI",
        "outputId": "11a8b49c-d290-4289-9972-b8180310fcc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[ex_struct: struct<col1:int,col2:int,col3:int>]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Play with Struct\n"
      ],
      "metadata": {
        "id": "Ky55RcyOiHOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_struct = spark.sql(\"SELECT struct(1, 2, '3') as ex_struct\")\n",
        "df_struct.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACGJLu9-NjKw",
        "outputId": "266cc7c4-a284-47da-8a16-b17910862ffe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|ex_struct|\n",
            "+---------+\n",
            "|{1, 2, 3}|\n",
            "+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_struct.select(\"ex_struct.col3\", \"ex_struct\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B46JwCaePajw",
        "outputId": "82ce3755-9e13-404d-897c-a0c22551a2bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---------+\n",
            "|col3|ex_struct|\n",
            "+----+---------+\n",
            "|   3|{1, 2, 3}|\n",
            "+----+---------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_struct.select(\"ex_struct.*\").show(truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeMsLV5Qdbp9",
        "outputId": "4a03e2d9-f573-4e65-f05f-57e597d116d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+----+----+\n",
            "|col1|col2|col3|\n",
            "+----+----+----+\n",
            "|1   |2   |3   |\n",
            "+----+----+----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_map = spark.sql(\"SELECT map(1.0, '2', 3.0, '4') as ex_map\")\n",
        "df_map.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-Dl2Y1DOSd3",
        "outputId": "b1195e4e-834a-40f3-e646-3f8e06500f20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|              ex_map|\n",
            "+--------------------+\n",
            "|{1.0 -> 2, 3.0 -> 4}|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType,StructField, StringType, ArrayType,MapType\n",
        "\n",
        "# https://sparkbyexamples.com/pyspark/pyspark-maptype-dict-examples/\n",
        "\n",
        "spark = SparkSession.builder.appName('SparkByExamples.com').getOrCreate()\n",
        "\n",
        "# Data\n",
        "data = [\n",
        "        ((\"James\",None,\"Smith\"),\"OH\",\"M\"),\n",
        "        ((\"Anna\",\"Rose\",\"\"),\"NY\",\"F\"),\n",
        "        ((\"Julia\",\"\",\"Williams\"),\"OH\",\"F\"),\n",
        "        ((\"Maria\",\"Anne\",\"Jones\"),\"NY\",\"M\"),\n",
        "        ((\"Jen\",\"Mary\",\"Brown\"),\"NY\",\"M\"),\n",
        "        ((\"Mike\",\"Mary\",\"Williams\"),\"OH\",\"M\")\n",
        "        ]\n",
        "\n",
        "# Schema\n",
        "schema = StructType([\n",
        "    StructField('name', StructType([\n",
        "         StructField('firstname', StringType(), True),\n",
        "         StructField('middlename', StringType(), True),\n",
        "         StructField('lastname', StringType(), True)\n",
        "         ])),\n",
        "     StructField('state', StringType(), True),\n",
        "     StructField('gender', StringType(), True)\n",
        "     ])\n",
        "\n",
        "# Create DataFrame\n",
        "df2 = spark.createDataFrame(data = data, schema = schema)\n",
        "df2.printSchema()\n",
        "df2.show(truncate=False) # shows all columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jye_i4rTOfIk",
        "outputId": "bbe0fdec-6e59-4576-ce1f-863553ac0384"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: struct (nullable = true)\n",
            " |    |-- firstname: string (nullable = true)\n",
            " |    |-- middlename: string (nullable = true)\n",
            " |    |-- lastname: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            "\n",
            "+----------------------+-----+------+\n",
            "|name                  |state|gender|\n",
            "+----------------------+-----+------+\n",
            "|{James, NULL, Smith}  |OH   |M     |\n",
            "|{Anna, Rose, }        |NY   |F     |\n",
            "|{Julia, , Williams}   |OH   |F     |\n",
            "|{Maria, Anne, Jones}  |NY   |M     |\n",
            "|{Jen, Mary, Brown}    |NY   |M     |\n",
            "|{Mike, Mary, Williams}|OH   |M     |\n",
            "+----------------------+-----+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df2.select(\"name\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLx1sLW0ctrH",
        "outputId": "6fdd982d-97ec-4db7-c198-c55727ce70ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|                name|\n",
            "+--------------------+\n",
            "|{James, NULL, Smith}|\n",
            "|      {Anna, Rose, }|\n",
            "| {Julia, , Williams}|\n",
            "|{Maria, Anne, Jones}|\n",
            "|  {Jen, Mary, Brown}|\n",
            "|{Mike, Mary, Will...|\n",
            "+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType,StructField, StringType, ArrayType,MapType\n",
        "from pyspark.sql.functions import element_at\n",
        "\n",
        "data = [\n",
        "        ([(\"Yash\",\"K\",None),(\"Yash2\",\"K2\",None)],\"BL\",\"M\",[\"KGF 1\", \"KGF 2\"],{\"Bank\":\"100 CR\", \"Business\":\"50 CR\", \"Land\":\"150 CR\"}),\n",
        "        ([(\"Sudeep\",\"Kicha\",\"S\")],\"DL\",\"M\",[\"Autograph\", \"Kicha\",\"Hucha\"],{\"Others\":\"300 CR\"}),\n",
        "        ([(\"Puneeth\",None,\"Raj\")],\"MB\",\"M\",[], {\"Building\":\"500 CR\", \"Commercal\":100}),\n",
        "        ([(\"Darshan\",None,None)],\"MB\",\"M\",[], {\"Building\":\"500 CR\", \"Commercal\":100}),\n",
        "\n",
        "        ]\n",
        "\n",
        "# Schema\n",
        "schema = StructType([\n",
        "    StructField(    'name'\n",
        "                  , ArrayType(\n",
        "                        StructType([\n",
        "                            StructField('firstname', StringType(), True),\n",
        "                            StructField('middlename', StringType(), True),\n",
        "                            StructField('lastname', StringType(), True)\n",
        "                        ])\n",
        "                    ), True),\n",
        "     StructField('state', StringType(), True),\n",
        "     StructField('gender', StringType(), True),\n",
        "     StructField('movies', ArrayType(StringType()), True),\n",
        "     StructField('properties', MapType(StringType(),StringType()), True)\n",
        "     ])\n",
        "\n",
        "# Create DataFrame\n",
        "df2 = spark.createDataFrame(data = data, schema = schema)\n",
        "df2.printSchema()\n",
        "df2.show(truncate=False) # shows all columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QJwWwq4Xc-0Z",
        "outputId": "468e11f5-95f8-4a03-80a4-102964f61988"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: array (nullable = true)\n",
            " |    |-- element: struct (containsNull = true)\n",
            " |    |    |-- firstname: string (nullable = true)\n",
            " |    |    |-- middlename: string (nullable = true)\n",
            " |    |    |-- lastname: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- movies: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- properties: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            "\n",
            "+------------------------------------+-----+------+-------------------------+---------------------------------------------------+\n",
            "|name                                |state|gender|movies                   |properties                                         |\n",
            "+------------------------------------+-----+------+-------------------------+---------------------------------------------------+\n",
            "|[{Yash, K, NULL}, {Yash2, K2, NULL}]|BL   |M     |[KGF 1, KGF 2]           |{Bank -> 100 CR, Land -> 150 CR, Business -> 50 CR}|\n",
            "|[{Sudeep, Kicha, S}]                |DL   |M     |[Autograph, Kicha, Hucha]|{Others -> 300 CR}                                 |\n",
            "|[{Puneeth, NULL, Raj}]              |MB   |M     |[]                       |{Building -> 500 CR, Commercal -> 100}             |\n",
            "|[{Darshan, NULL, NULL}]             |MB   |M     |[]                       |{Building -> 500 CR, Commercal -> 100}             |\n",
            "+------------------------------------+-----+------+-------------------------+---------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " (df2\n",
        "  .select(\"properties\", \"name\", \"name.firstname\")\n",
        "  .withColumn(\"M-Building\", col(\"properties\").getItem(\"Building\"))\n",
        "  .withColumn(\"M-Commercal \", col(\"properties\").getItem(\"Commercal\"))\n",
        "  .withColumn(\"M-Others \", col(\"properties\").getItem(\"Others\"))\n",
        "  .withColumn(\"S-FName\", col(\"name.firstname\"))\n",
        "  ).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0-Z3EjP0njz",
        "outputId": "a6e04d9a-3bb8-48f6-9dcd-8a7d9bd219a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+---------+----------+------------+---------+-------+\n",
            "|          properties|                name|firstname|M-Building|M-Commercal |M-Others |S-FName|\n",
            "+--------------------+--------------------+---------+----------+------------+---------+-------+\n",
            "|{Bank -> 100 CR, ...|     {Yash, K, NULL}|     Yash|      NULL|        NULL|     NULL|   Yash|\n",
            "|  {Others -> 300 CR}|  {Sudeep, Kicha, S}|   Sudeep|      NULL|        NULL|   300 CR| Sudeep|\n",
            "|{Building -> 500 ...|{Puneeth, NULL, Raj}|  Puneeth|    500 CR|         100|     NULL|Puneeth|\n",
            "|{Building -> 500 ...|{Darshan, NULL, N...|  Darshan|    500 CR|         100|     NULL|Darshan|\n",
            "+--------------------+--------------------+---------+----------+------------+---------+-------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType,StructField, StringType, ArrayType,MapType\n",
        "from pyspark.sql.functions import element_at\n",
        "\n",
        "data = [\n",
        "        ((\"Yash\",\"K\",None),\"BL\",\"M\",[\"KGF 1\", \"KGF 2\"],{\"Bank\":\"100 CR\", \"Business\":\"50 CR\", \"Land\":\"150 CR\"}),\n",
        "        ((\"Sudeep\",\"Kicha\",\"S\"),\"DL\",\"M\",[\"Autograph\", \"Kicha\",\"Hucha\"],{\"Others\":\"300 CR\"}),\n",
        "        ((\"Puneeth\",None,\"Raj\"),\"MB\",\"M\",[], {\"Building\":\"500 CR\", \"Commercal\":100}),\n",
        "        ((\"Darshan\",None,None),\"MB\",\"M\",[], {\"Building\":\"500 CR\", \"Commercal\":100}),\n",
        "\n",
        "        ]\n",
        "\n",
        "# Schema\n",
        "schema = StructType([\n",
        "    StructField('name', StructType([\n",
        "         StructField('firstname', StringType(), True),\n",
        "         StructField('middlename', StringType(), True),\n",
        "         StructField('lastname', StringType(), True)\n",
        "         ])),\n",
        "     StructField('state', StringType(), True),\n",
        "     StructField('gender', StringType(), True),\n",
        "     StructField('movies', ArrayType(StringType()), True),\n",
        "     StructField('properties', MapType(StringType(),StringType()), True)\n",
        "     ])\n",
        "\n",
        "# Create DataFrame\n",
        "df2 = spark.createDataFrame(data = data, schema = schema)\n",
        "df2.printSchema()\n",
        "df2.show(truncate=False) # shows all columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QiMme632DET",
        "outputId": "22e1ec1c-594b-473b-b950-2f87e1b7e29c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- name: struct (nullable = true)\n",
            " |    |-- firstname: string (nullable = true)\n",
            " |    |-- middlename: string (nullable = true)\n",
            " |    |-- lastname: string (nullable = true)\n",
            " |-- state: string (nullable = true)\n",
            " |-- gender: string (nullable = true)\n",
            " |-- movies: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- properties: map (nullable = true)\n",
            " |    |-- key: string\n",
            " |    |-- value: string (valueContainsNull = true)\n",
            "\n",
            "+---------------------+-----+------+-------------------------+---------------------------------------------------+\n",
            "|name                 |state|gender|movies                   |properties                                         |\n",
            "+---------------------+-----+------+-------------------------+---------------------------------------------------+\n",
            "|{Yash, K, NULL}      |BL   |M     |[KGF 1, KGF 2]           |{Bank -> 100 CR, Land -> 150 CR, Business -> 50 CR}|\n",
            "|{Sudeep, Kicha, S}   |DL   |M     |[Autograph, Kicha, Hucha]|{Others -> 300 CR}                                 |\n",
            "|{Puneeth, NULL, Raj} |MB   |M     |[]                       |{Building -> 500 CR, Commercal -> 100}             |\n",
            "|{Darshan, NULL, NULL}|MB   |M     |[]                       |{Building -> 500 CR, Commercal -> 100}             |\n",
            "+---------------------+-----+------+-------------------------+---------------------------------------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
        "\n",
        "myManualSchema = StructType([\n",
        "  StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
        "  StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
        "  StructField(\"count\", LongType(), False, metadata={\"hello\":\"world\"})\n",
        "])\n",
        "df = spark.read.format(\"json\").schema(myManualSchema)\\\n",
        "  .load(\"sample_data/2015-summary.json\")\n",
        "\n",
        "df.show()"
      ],
      "metadata": {
        "id": "21WEGU2v8_tA",
        "outputId": "50a1e446-52a5-4bf8-c206-028cf5b7f366",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/content/sample_data/2015-summary.json.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-50b2766d5f6a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m ])\n\u001b[1;32m      8\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmyManualSchema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sample_data/2015-summary.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/content/sample_data/2015-summary.json."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.printSchema()"
      ],
      "metadata": {
        "id": "mcGnKSvKbQSR",
        "outputId": "42ba3405-038e-4bcd-fb0b-92de2fa18373",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
            " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
            " |-- count: long (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "spark.read.format(\"json\").load(\"sample_data/2015-summary.json\").schema"
      ],
      "metadata": {
        "id": "vC1GYIlfbT9m",
        "outputId": "0e7f09ed-fa95-454d-f2dc-168a6a8eaac2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructType([StructField('DEST_COUNTRY_NAME', StringType(), True), StructField('ORIGIN_COUNTRY_NAME', StringType(), True), StructField('count', LongType(), True)])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "Q2lTqwHdg9V4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "]\\"
      ],
      "metadata": {
        "id": "2DZCMcH5DkWZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
